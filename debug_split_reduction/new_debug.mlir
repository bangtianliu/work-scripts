// -----// IR Dump After AutoInputConversionPipelinePass (iree-auto-input-conversion) //----- //
module {
  func.func @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After IREEImportPublicPass (iree-import-public) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After ImportMLProgramPass (iree-import-ml-program) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After SanitizeModuleNamesPass (iree-sanitize-module-names) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After ConvertShardToFlowPass (iree-convert-shard-to-flow) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After DemoteF64ToF32Pass (iree-input-conversion-demote-f64-to-f32) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After ConvertStreamableOpsPass (iree-abi-convert-streamable-ops) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After WrapEntryPointsPass (iree-abi-wrap-entry-points) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = util.call @_test_simple_argmax_4x16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %2 = hal.tensor.export %1#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %3 = hal.tensor.export %1#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
  util.func private @_test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) attributes {hal.abi.convention = #hal.abi.convention<synchronous>} {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %0 = tensor.empty() : tensor<4x16xf16>
    %1 = tensor.empty() : tensor<4x16xi32>
    %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %5 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %5 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func private @_test_simple_argmax_4x16(%arg0: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) attributes {hal.abi.convention = #hal.abi.convention<synchronous>} {
  %cst = arith.constant 0xFC00 : f16
  %c0_i32 = arith.constant 0 : i32
  %0 = tensor.empty() : tensor<4x16xf16>
  %1 = tensor.empty() : tensor<4x16xi32>
  %2 = linalg.fill ins(%cst : f16) outs(%0 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %3 = linalg.fill ins(%c0_i32 : i32) outs(%1 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %4:2 = iree_linalg_ext.arg_compare dimension(2) ins(%arg0 : tensor<4x16x128256xf16>) outs(%2, %3 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %5 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %5 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  util.return %4#0, %4#1 : tensor<4x16xf16>, tensor<4x16xi32>
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = util.call @_test_simple_argmax_4x16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %2 = hal.tensor.export %1#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %3 = hal.tensor.export %1#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
module {
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
module attributes {hal.device.targets = [#hal.device.alias<"hip"> : !hal.device]} {
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #hal.device.alias<"hip"> : !hal.device
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #hal.device.alias<"hip"> : !hal.device
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AttrBasedPipelinePass (iree-preprocessing-attr-based-pipeline) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After WarnOnUninitializedValuesPass (iree-global-opt-warn-on-uninitialized-values) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After StripDebugOpsPass (iree-util-strip-debug-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After LinalgQuantizedConvToConvPass (iree-global-opt-quantized-conv-to-conv) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After LinalgQuantizedMatmulToMatmulPass (iree-global-opt-quantized-matmul-to-matmul) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After RemoveZeroExtentTensorsPass (iree-global-opt-remove-zero-extent-tensors) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After DetachElementwiseFromNamedOpsPass (iree-global-opt-detach-elementwise-from-named-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyDepthwiseConvPass (simplify-depthwise-conv) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After EraseUnusedLinalgOperandsPass (iree-global-opt-erase-unused-linalg-operands) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ExpandTensorShapesPass (iree-global-opt-expand-tensor-shapes) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = tensor.empty() : tensor<4x16xf16>
    %2 = tensor.empty() : tensor<4x16xi32>
    %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %8 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %8 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ConvertElementwiseToLinalgPass (convert-elementwise-to-linalg) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOpsPass (iree-global-opt-raise-special-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After DecomposeConcatPass (iree-global-opt-decompose-concat) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After GeneralizeLinalgNamedOpsPass (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = tensor.empty() : tensor<4x16xf16>
  %2 = tensor.empty() : tensor<4x16xi32>
  %3 = linalg.fill ins(%cst : f16) outs(%1 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %5:2 = iree_linalg_ext.arg_compare dimension(2) ins(%0 : tensor<4x16x128256xf16>) outs(%3, %4 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %8 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %8 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %6 = hal.tensor.export %5#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %5#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After InsertTensorBarriersPass (iree-dispatch-creation-insert-tensor-barriers) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldUnitExtentDimsPass (iree-dispatch-creation-fold-unit-extent-dims) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After DemoteContractionInputsToBF16Pass (iree-global-opt-demote-contraction-inputs-to-bf16) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After PropagateLinalgTransposePass (iree-global-opt-propagate-linalg-transpose) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ConvertStridedContractionToContractionPass (iree-global-opt-convert-strided-contraction-to-contraction) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After GeneralizeLinalgNamedOpsPass (iree-global-opt-generalize-linalg-named-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After GlobalLoopInvariantCodeMotionPass (iree-global-opt-loop-invariant-code-motion) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After HoistIntoGlobalsPass (iree-util-hoist-into-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After JitGlobalsPass (iree-consteval-jit-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After RaiseSpecialOpsPass (iree-global-opt-raise-special-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After TensorPadToTensorInsertSlicePass (iree-dispatch-creation-tensor-pad-to-tensor-insert-slice) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%arg1: f16, %arg2: f16):
      %11 = arith.cmpf ogt, %arg1, %arg2 : f16
      iree_linalg_ext.yield %11 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FusionPreprocessingPass (iree-dispatch-creation-fusion-preprocessing) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-dispatch-creation-elementwise-op-fusion) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After BubbleUpExpandShapesPass (iree-dispatch-creation-bubble-up-expand-shapes) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-dispatch-creation-elementwise-op-fusion) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SinkReshapesPass (iree-dispatch-creation-sink-reshapes) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FuseMultiUseElementwiseProducerPass (iree-dispatch-creation-fuse-multi-use-elementwise-producer) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SplitReductionPass (iree-dispatch-creation-split-reduction-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SetSplitReductionSizesPass (iree-dispatch-creation-set-split-reduction-sizes) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6:2 = iree_linalg_ext.arg_compare {iree_linalg_ext.split_reduction = [1336 : index]} dimension(2) ins(%1 : tensor<4x16x128256xf16>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%arg1: f16, %arg2: f16):
    %11 = arith.cmpf ogt, %arg1, %arg2 : f16
    iree_linalg_ext.yield %11 : i1
  } -> tensor<4x16xf16>, tensor<4x16xi32>
  %7 = iree_tensor_ext.compute_barrier.end %6#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %8 = iree_tensor_ext.compute_barrier.end %6#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %9 = hal.tensor.export %8 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %10 = hal.tensor.export %7 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FormSplitReductionDispatchesPass (iree-dispatch-creation-form-split-reduction-dispatches) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %15 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %16 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %17 = arith.muli %14, %c1336 : index
      %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %21 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      %19 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %20 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %18#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %18#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After TransposeGenericOpsPass (iree-dispatch-creation-transpose-generic-ops) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %15 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %16 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %17 = arith.muli %14, %c1336 : index
      %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %21 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      %19 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %20 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %18#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %18#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After PropagateEncodingsPass (iree-dispatch-creation-propagate-encodings) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %15 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %16 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %17 = arith.muli %14, %c1336 : index
      %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %21 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      %19 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      %20 = affine.apply affine_map<()[s0] -> (s0 floordiv 1336)>()[%arg1]
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %18#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %18#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After HoistIntoGlobalsPass (iree-util-hoist-into-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#map = affine_map<()[s0] -> (s0 floordiv 1336)>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c1336 = arith.constant 1336 : index
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2 = tensor.empty() : tensor<4x16xf16>
    %3 = tensor.empty() : tensor<4x16xi32>
    %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16x96xi32>
    %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %14 = affine.apply #map()[%arg1]
        %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
        %15 = affine.apply #map()[%arg1]
        %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %16 = affine.apply #map()[%arg1]
        %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %17 = arith.muli %14, %c1336 : index
        %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
        ^bb0(%arg4: f16, %arg5: f16):
          %21 = arith.cmpf ogt, %arg4, %arg5 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        %19 = affine.apply #map()[%arg1]
        %20 = affine.apply #map()[%arg1]
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %18#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %18#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
    }
    %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %13 = arith.cmpf ogt, %in, %init : f16
        %14 = arith.select %13, %in, %init : f16
        %15 = arith.select %13, %in_0, %init_1 : i32
        linalg.yield %14, %15 : f16, i32
      }
    %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = arith.cmpi slt, %arg1, %c0 : index
      %15 = arith.subi %c-1, %arg1 : index
      %16 = arith.select %14, %15, %arg1 : index
      %17 = arith.divsi %16, %c1336 : index
      %18 = arith.subi %c-1, %17 : index
      %19 = arith.select %14, %18, %17 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %20 = arith.cmpi slt, %arg1, %c0 : index
      %21 = arith.subi %c-1, %arg1 : index
      %22 = arith.select %20, %21, %arg1 : index
      %23 = arith.divsi %22, %c1336 : index
      %24 = arith.subi %c-1, %23 : index
      %25 = arith.select %20, %24, %23 : index
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %25] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %26 = arith.cmpi slt, %arg1, %c0 : index
      %27 = arith.subi %c-1, %arg1 : index
      %28 = arith.select %26, %27, %arg1 : index
      %29 = arith.divsi %28, %c1336 : index
      %30 = arith.subi %c-1, %29 : index
      %31 = arith.select %26, %30, %29 : index
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %31] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %32 = arith.muli %19, %c1336 : index
      %33:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%32 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %46 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %46 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      %34 = arith.cmpi slt, %arg1, %c0 : index
      %35 = arith.subi %c-1, %arg1 : index
      %36 = arith.select %34, %35, %arg1 : index
      %37 = arith.divsi %36, %c1336 : index
      %38 = arith.subi %c-1, %37 : index
      %39 = arith.select %34, %38, %37 : index
      %40 = arith.cmpi slt, %arg1, %c0 : index
      %41 = arith.subi %c-1, %arg1 : index
      %42 = arith.select %40, %41, %arg1 : index
      %43 = arith.divsi %42, %c1336 : index
      %44 = arith.subi %c-1, %43 : index
      %45 = arith.select %40, %44, %43 : index
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %33#0 into %arg2[0, 0, %39] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %33#1 into %arg3[0, 0, %45] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = arith.cmpi slt, %arg1, %c0 : index
      %15 = arith.subi %c-1, %arg1 : index
      %16 = arith.select %14, %15, %arg1 : index
      %17 = arith.divsi %16, %c1336 : index
      %18 = arith.subi %c-1, %17 : index
      %19 = arith.select %14, %18, %17 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %20 = arith.muli %19, %c1336 : index
      %21:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%20 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %22 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %22 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %21#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %21#1 into %arg3[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FormScalarDispatchesPass (iree-dispatch-creation-form-scalar-dispatches) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %13:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %14 = arith.cmpi slt, %arg1, %c0 : index
      %15 = arith.subi %c-1, %arg1 : index
      %16 = arith.select %14, %15, %arg1 : index
      %17 = arith.divsi %16, %c1336 : index
      %18 = arith.subi %c-1, %17 : index
      %19 = arith.select %14, %18, %17 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %20 = arith.muli %19, %c1336 : index
      %21:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%20 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %22 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %22 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %21#0 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %21#1 into %arg3[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %13#0, %13#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %13 = arith.cmpf ogt, %in, %init : f16
      %14 = arith.select %13, %in, %init : f16
      %15 = arith.select %13, %in_0, %init_1 : i32
      linalg.yield %14, %15 : f16, i32
    }
  %9 = iree_tensor_ext.compute_barrier.end %reduced#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %10 = iree_tensor_ext.compute_barrier.end %reduced#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %11 = hal.tensor.export %10 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %12 = hal.tensor.export %9 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %11, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FormDispatchRegionsPass (iree-dispatch-creation-form-dispatch-regions) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %9:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %14 = arith.cmpf ogt, %in, %init : f16
        %15 = arith.select %14, %in, %init : f16
        %16 = arith.select %14, %in_0, %init_1 : i32
        linalg.yield %15, %16 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %10 = iree_tensor_ext.compute_barrier.end %9#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %11 = iree_tensor_ext.compute_barrier.end %9#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %12 = hal.tensor.export %11 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %13 = hal.tensor.export %10 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %12, %13 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ElementwiseOpFusionPass (iree-dispatch-creation-elementwise-op-fusion) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %9:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %14 = arith.cmpf ogt, %in, %init : f16
        %15 = arith.select %14, %in, %init : f16
        %16 = arith.select %14, %in_0, %init_1 : i32
        linalg.yield %15, %16 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %10 = iree_tensor_ext.compute_barrier.end %9#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %11 = iree_tensor_ext.compute_barrier.end %9#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %12 = hal.tensor.export %11 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %13 = hal.tensor.export %10 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %12, %13 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FuseMultiUseElementwiseProducerPass (iree-dispatch-creation-fuse-multi-use-elementwise-producer) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c-1 = arith.constant -1 : index
  %c0 = arith.constant 0 : index
  %c1336 = arith.constant 1336 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2 = tensor.empty() : tensor<4x16xf16>
  %3 = tensor.empty() : tensor<4x16xi32>
  %4 = linalg.fill ins(%cst : f16) outs(%2 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %5 = linalg.fill ins(%c0_i32 : i32) outs(%3 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %9:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %reduced:2 = linalg.reduce ins(%8#0, %8#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%4, %5 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %14 = arith.cmpf ogt, %in, %init : f16
        %15 = arith.select %14, %in, %init : f16
        %16 = arith.select %14, %in_0, %init_1 : i32
        linalg.yield %15, %16 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %10 = iree_tensor_ext.compute_barrier.end %9#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %11 = iree_tensor_ext.compute_barrier.end %9#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %12 = hal.tensor.export %11 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %13 = hal.tensor.export %10 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %12, %13 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CloneProducersIntoDispatchRegionsPass (iree-dispatch-creation-clone-producers-into-dispatch-regions) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %8 = tensor.empty() : tensor<4x16x96xi32>
    %9 = tensor.empty() : tensor<4x16x96xf16>
    %10 = tensor.empty() : tensor<4x16xi32>
    %c0_i32 = arith.constant 0 : i32
    %11 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %8 = tensor.empty() : tensor<4x16xi32>
    %c0_i32 = arith.constant 0 : i32
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %12 = arith.cmpf ogt, %in, %init : f16
        %13 = arith.select %12, %in, %init : f16
        %14 = arith.select %12, %in_0, %init_1 : i32
        linalg.yield %13, %14 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CollapseDimensionsPass (iree-dispatch-creation-collapse-dimensions) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %8 = tensor.empty() : tensor<4x16x96xi32>
    %9 = tensor.empty() : tensor<4x16x96xf16>
    %10 = tensor.empty() : tensor<4x16xi32>
    %c0_i32 = arith.constant 0 : i32
    %11 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %8 = tensor.empty() : tensor<4x16xi32>
    %c0_i32 = arith.constant 0 : i32
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %12 = arith.cmpf ogt, %in, %init : f16
        %13 = arith.select %12, %in, %init : f16
        %14 = arith.select %12, %in_0, %init_1 : i32
        linalg.yield %13, %14 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After HoistUniformScalarComputePass (iree-dispatch-creation-hoist-uniform-scalar-compute) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %c1336 = arith.constant 1336 : index
  %c-1 = arith.constant -1 : index
  %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %c1336_0 = arith.constant 1336 : index
    %c-1_1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16x96xi32>
    %9 = tensor.empty() : tensor<4x16x96xf16>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1_1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336_0 : index
      %19 = arith.subi %c-1_1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_2 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_2 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_4 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_5 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_4 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_6 = tensor.extract_slice %broadcasted_5[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336_0 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_3, %extracted_slice_6 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16xi32>
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %12 = arith.cmpf ogt, %in, %init : f16
        %13 = arith.select %12, %in, %init : f16
        %14 = arith.select %12, %in_0, %init_1 : i32
        linalg.yield %13, %14 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FuseEncodingOpsIntoDispatchRegionsPass (iree-dispatch-creation-fuse-encoding-ops-into-dispatch-regions-pass) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16x96xi32>
    %9 = tensor.empty() : tensor<4x16x96xf16>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16xi32>
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %12 = arith.cmpf ogt, %in, %init : f16
        %13 = arith.select %12, %in, %init : f16
        %14 = arith.select %12, %in_0, %init_1 : i32
        linalg.yield %13, %14 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ConvertEncodingToFlowPass (iree-dispatch-creation-convert-encoding-to-flow) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
  %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16x96xi32>
    %9 = tensor.empty() : tensor<4x16x96xf16>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %15 = arith.cmpi slt, %arg1, %c0 : index
      %16 = arith.subi %c-1, %arg1 : index
      %17 = arith.select %15, %16, %arg1 : index
      %18 = arith.divsi %17, %c1336 : index
      %19 = arith.subi %c-1, %18 : index
      %20 = arith.select %15, %19, %18 : index
      %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %21 = arith.muli %20, %c1336 : index
      %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %23 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %23 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %c0_i32 = arith.constant 0 : i32
    %8 = tensor.empty() : tensor<4x16xi32>
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %12 = arith.cmpf ogt, %in, %init : f16
        %13 = arith.select %12, %in, %init : f16
        %14 = arith.select %12, %in_0, %init_1 : i32
        linalg.yield %13, %14 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
  %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
  %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After HoistIntoGlobalsPass (iree-util-hoist-into-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1 = iree_tensor_ext.compute_barrier.start %0 : tensor<4x16x128256xf16> -> tensor<4x16x128256xf16>
    %2:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %c1336 = arith.constant 1336 : index
      %c-1 = arith.constant -1 : index
      %c0 = arith.constant 0 : index
      %c0_i32 = arith.constant 0 : i32
      %8 = tensor.empty() : tensor<4x16x96xi32>
      %9 = tensor.empty() : tensor<4x16x96xf16>
      %10 = tensor.empty() : tensor<4x16xi32>
      %11 = tensor.empty() : tensor<4x16xf16>
      %cst = arith.constant 0xFC00 : f16
      %12 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %13 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %14:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %9, %arg3 = %8) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %15 = arith.cmpi slt, %arg1, %c0 : index
        %16 = arith.subi %c-1, %arg1 : index
        %17 = arith.select %15, %16, %arg1 : index
        %18 = arith.divsi %17, %c1336 : index
        %19 = arith.subi %c-1, %18 : index
        %20 = arith.select %15, %19, %18 : index
        %extracted_slice = tensor.extract_slice %1[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
        %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%12 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_3 = linalg.broadcast ins(%13 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %21 = arith.muli %20, %c1336 : index
        %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
        ^bb0(%arg4: f16, %arg5: f16):
          %23 = arith.cmpf ogt, %arg4, %arg5 : f16
          iree_linalg_ext.yield %23 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %22#0 into %arg2[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %22#1 into %arg3[0, 0, %20] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      flow.return %14#0, %14#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
    }
    %3:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %c0_i32 = arith.constant 0 : i32
      %8 = tensor.empty() : tensor<4x16xi32>
      %9 = tensor.empty() : tensor<4x16xf16>
      %cst = arith.constant 0xFC00 : f16
      %10 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%2#0, %2#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%11, %10 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %12 = arith.cmpf ogt, %in, %init : f16
          %13 = arith.select %12, %in, %init : f16
          %14 = arith.select %12, %in_0, %init_1 : i32
          linalg.yield %13, %14 : f16, i32
        }
      flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    %4 = iree_tensor_ext.compute_barrier.end %3#1 : tensor<4x16xi32> -> tensor<4x16xi32>
    %5 = iree_tensor_ext.compute_barrier.end %3#0 : tensor<4x16xf16> -> tensor<4x16xf16>
    %6 = hal.tensor.export %5 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %7 = hal.tensor.export %4 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %6, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After RemoveTensorBarriersPass (iree-dispatch-creation-remove-tensor-barriers) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.region -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg1) = (0) to (128256) step (1336) shared_outs(%arg2 = %6, %arg3 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg1, %c0 : index
      %13 = arith.subi %c-1, %arg1 : index
      %14 = arith.select %12, %13, %arg1 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %extracted_slice = tensor.extract_slice %0[0, 0, %arg1] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg3[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %18 = arith.muli %17, %c1336 : index
      %19:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%18 : index) {
      ^bb0(%arg4: f16, %arg5: f16):
        %20 = arith.cmpf ogt, %arg4, %arg5 : f16
        iree_linalg_ext.yield %20 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %19#0 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %19#1 into %arg3[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    flow.return %11#0, %11#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>
  }
  %2:2 = flow.dispatch.region -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %c0_i32 = arith.constant 0 : i32
    %5 = tensor.empty() : tensor<4x16xi32>
    %6 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %7 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %8 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%1#0, %1#1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%8, %7 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %9 = arith.cmpf ogt, %in, %init : f16
        %10 = arith.select %9, %in, %init : f16
        %11 = arith.select %9, %in_0, %init_1 : i32
        linalg.yield %10, %11 : f16, i32
      }
    flow.return %reduced#0, %reduced#1 : tensor<4x16xf16>, tensor<4x16xi32>
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ConvertDispatchRegionsToWorkgroupsPass (iree-dispatch-creation-convert-dispatch-regions-to-workgroups) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 128256], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x128256xf16>
    %c1336 = arith.constant 1336 : index
    %c-1 = arith.constant -1 : index
    %c0 = arith.constant 0 : index
    %c0_i32 = arith.constant 0 : i32
    %6 = tensor.empty() : tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16x96xf16>
    %8 = tensor.empty() : tensor<4x16xi32>
    %9 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %10 = linalg.fill ins(%cst : f16) outs(%9 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %11 = linalg.fill ins(%c0_i32 : i32) outs(%8 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %12:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %7, %arg6 = %6) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %13 = arith.cmpi slt, %arg4, %c0 : index
      %14 = arith.subi %c-1, %arg4 : index
      %15 = arith.select %13, %14, %arg4 : index
      %16 = arith.divsi %15, %c1336 : index
      %17 = arith.subi %c-1, %16 : index
      %18 = arith.select %13, %17, %16 : index
      %extracted_slice = tensor.extract_slice %5[0, 0, %arg4] [4, 16, 1336] [1, 1, 1] : tensor<4x16x128256xf16> to tensor<4x16x1336xf16>
      %extracted_slice_0 = tensor.extract_slice %arg5[0, 0, %18] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%10 : tensor<4x16xf16>) outs(%extracted_slice_0 : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_2 = tensor.extract_slice %arg6[0, 0, %18] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_3 = linalg.broadcast ins(%11 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %18, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%extracted_slice : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %18] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %18] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %12#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %12#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %c0_i32 = arith.constant 0 : i32
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %cst = arith.constant 0xFC00 : f16
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ConvertTensorToFlowPass (iree-dispatch-creation-convert-tensor-to-flow) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After MaterializeDefaultWorkgroupCountRegionPass (iree-dispatch-creation-materialize-default-workgroup-count-region) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After BitcastUnsupportedElementTypesPass (iree-dispatch-creation-bitcast-unsupported-element-types) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After VerifyInputLegalityPass (iree-verify-input-legality) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg4, %c0 : index
        %13 = arith.subi %c-1, %arg4 : index
        %14 = arith.select %12, %13, %arg4 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg7: f16, %arg8: f16):
          %21 = arith.cmpf ogt, %arg7, %arg8 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %11 = arith.cmpf ogt, %in, %init : f16
          %12 = arith.select %11, %in, %init : f16
          %13 = arith.select %11, %in_0, %init_1 : i32
          linalg.yield %12, %13 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg4, %c0 : index
        %13 = arith.subi %c-1, %arg4 : index
        %14 = arith.select %12, %13, %arg4 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg7: f16, %arg8: f16):
          %21 = arith.cmpf ogt, %arg7, %arg8 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %11 = arith.cmpf ogt, %in, %init : f16
          %12 = arith.select %11, %in, %init : f16
          %13 = arith.select %11, %in_0, %init_1 : i32
          linalg.yield %12, %13 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AttributeCallGraphPass (iree-util-attribute-call-graph) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg4, %c0 : index
        %13 = arith.subi %c-1, %arg4 : index
        %14 = arith.select %12, %13, %arg4 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg7: f16, %arg8: f16):
          %21 = arith.cmpf ogt, %arg7, %arg8 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %11 = arith.cmpf ogt, %in, %init : f16
          %12 = arith.select %11, %in, %init : f16
          %13 = arith.select %11, %in_0, %init_1 : i32
          linalg.yield %12, %13 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After InitializeEmptyTensorsPass (iree-flow-initialize-empty-tensors) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CaptureDynamicDimsPass (iree-flow-capture-dynamic-dims) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg4, %c0 : index
      %13 = arith.subi %c-1, %arg4 : index
      %14 = arith.select %12, %13, %arg4 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg7: f16, %arg8: f16):
        %21 = arith.cmpf ogt, %arg7, %arg8 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
    iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
      (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
    %cst = arith.constant 0xFC00 : f16
    %c0_i32 = arith.constant 0 : i32
    %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
    %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
      (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
        %11 = arith.cmpf ogt, %in, %init : f16
        %12 = arith.select %11, %in, %init : f16
        %13 = arith.select %11, %in_0, %init_1 : i32
        linalg.yield %12, %13 : f16, i32
      }
    iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
    iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
    flow.return
  } count() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OutlineDispatchExternsPass (iree-flow-outline-dispatch-externs) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch.workgroups(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg4) = (0) to (128256) step (1336) shared_outs(%arg5 = %6, %arg6 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg4, %c0 : index
        %13 = arith.subi %c-1, %arg4 : index
        %14 = arith.select %12, %13, %arg4 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, %arg4], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg7: f16, %arg8: f16):
          %21 = arith.cmpf ogt, %arg7, %arg8 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg5[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg6[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %11#0, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %11#1, %arg3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    %2:2 = flow.dispatch.workgroups(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>) =
        (%arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg4: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %5 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %6 = iree_tensor_ext.dispatch.tensor.load %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %11 = arith.cmpf ogt, %in, %init : f16
          %12 = arith.select %11, %in, %init : f16
          %13 = arith.select %11, %in_0, %init_1 : i32
          linalg.yield %12, %13 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      flow.return
    } count() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After OutlineDispatchRegionsPass (iree-flow-outline-dispatch-regions) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchesPass (iree-flow-annotate-dispatches) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After StripDebugOpsPass (iree-util-strip-debug-ops) //----- //
flow.executable private @test_simple_argmax_4x16_dispatch_1 {
  flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    flow.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %2 = tensor.empty() : tensor<4x16xi32>
      %3 = tensor.empty() : tensor<4x16xf16>
      %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %6 = arith.cmpf ogt, %in, %init : f16
          %7 = arith.select %6, %in, %init : f16
          %8 = arith.select %6, %in_0, %init_1 : i32
          linalg.yield %7, %8 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      return
    }
  }
}

// -----// IR Dump After StripDebugOpsPass (iree-util-strip-debug-ops) //----- //
flow.executable private @test_simple_argmax_4x16_dispatch_0 {
  flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    flow.return %result_x, %result_y, %result_z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
      %cst = arith.constant 0xFC00 : f16
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %0 = tensor.empty() : tensor<4x16x96xi32>
      %1 = tensor.empty() : tensor<4x16x96xf16>
      %2 = tensor.empty() : tensor<4x16xi32>
      %3 = tensor.empty() : tensor<4x16xf16>
      %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %7 = arith.cmpi slt, %arg3, %c0 : index
        %8 = arith.subi %c-1, %arg3 : index
        %9 = arith.select %7, %8, %arg3 : index
        %10 = arith.divsi %9, %c1336 : index
        %11 = arith.subi %c-1, %10 : index
        %12 = arith.select %7, %11, %10 : index
        %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %14 = arith.muli %12, %c1336 : index
        %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
        ^bb0(%arg6: f16, %arg7: f16):
          %16 = arith.cmpf ogt, %arg6, %arg7 : f16
          iree_linalg_ext.yield %16 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      return
    }
  }
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After DeduplicateExecutablesPass (iree-flow-deduplicate-executables) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After InjectTensorTracingPass (iree-flow-inject-tensor-tracing) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CleanupTensorShapesPass (iree-flow-cleanup-tensor-shapes) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OutlineConstantsPass (iree-flow-outline-constants) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CanonicalizePass (iree-flow-canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInputPass (iree-stream-verify-input) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
  %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
  %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
  %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After CloneToConsumersPass (iree-stream-clone-to-consumers) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  flow.executable private @test_simple_argmax_4x16_dispatch_0 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      flow.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %0 = tensor.empty() : tensor<4x16x96xi32>
        %1 = tensor.empty() : tensor<4x16x96xf16>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %5 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %6:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %1, %arg5 = %0) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %7 = arith.cmpi slt, %arg3, %c0 : index
          %8 = arith.subi %c-1, %arg3 : index
          %9 = arith.select %7, %8, %arg3 : index
          %10 = arith.divsi %9, %c1336 : index
          %11 = arith.subi %c-1, %10 : index
          %12 = arith.select %7, %11, %10 : index
          %13 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%4 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%5 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %14 = arith.muli %12, %c1336 : index
          %15:2 = iree_linalg_ext.arg_compare dimension(2) ins(%13 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%14 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %16 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %16 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %15#0 into %arg4[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %15#1 into %arg5[0, 0, %12] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %6#0, %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %6#1, %arg2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  flow.executable private @test_simple_argmax_4x16_dispatch_1 {
    flow.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      flow.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>, %arg1: !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>, %arg2: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>, %arg3: !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %0 = iree_tensor_ext.dispatch.tensor.load %arg0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %1 = iree_tensor_ext.dispatch.tensor.load %arg1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %2 = tensor.empty() : tensor<4x16xi32>
        %3 = tensor.empty() : tensor<4x16xf16>
        %4 = linalg.fill ins(%c0_i32 : i32) outs(%2 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %5 = linalg.fill ins(%cst : f16) outs(%3 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%0, %1 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%5, %4 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %6 = arith.cmpf ogt, %in, %init : f16
            %7 = arith.select %6, %in, %init : f16
            %8 = arith.select %6, %in_0, %init_1 : i32
            linalg.yield %7, %8 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %arg2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %arg3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %0 = hal.tensor.import %arg0 "input0" : !hal.buffer_view -> tensor<4x16x128256xf16>
    %1:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0) : (tensor<4x16x128256xf16>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
    %2:2 = flow.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0, %1#1) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16xf16>, tensor<4x16xi32>)
    %3 = hal.tensor.export %2#0 "output0" : tensor<4x16xf16> -> !hal.buffer_view
    %4 = hal.tensor.export %2#1 "output1" : tensor<4x16xi32> -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ConvertToStreamPass (iree-stream-conversion) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0_0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0_0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_3 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c128256 = arith.constant 128256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.transfer %8#0 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.transfer %8#1 : !stream.resource<*>{%7} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToTensorsPass (iree-stream-verify-lowering-to-tensors) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0_0 = arith.constant 0 : index
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0_0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_3 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    %c4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c128256 = arith.constant 128256 : index
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.transfer %1 : !stream.resource<external>{%0} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.transfer %8#0 : !stream.resource<*>{%6} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.transfer %8#1 : !stream.resource<*>{%7} from(#hal.device.affinity<@__device_0>) -> to(#hal.device.affinity<@__device_0>) !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
  %c1336 = arith.constant 1336 : index
  %c-1 = arith.constant -1 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg3, %c0 : index
    %11 = arith.subi %c-1, %arg3 : index
    %12 = arith.select %10, %11, %arg3 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg6: f16, %arg7: f16):
      %19 = arith.cmpf ogt, %arg6, %arg7 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %10 = arith.cmpf ogt, %in, %init : f16
      %11 = arith.select %10, %in, %init : f16
      %12 = arith.select %10, %in_0, %init_1 : i32
      linalg.yield %11, %12 : f16, i32
    }
  iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Inliner (inline) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After CombineInitializersPass (iree-util-combine-initializers) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After SpecializeEncodingsPass (iree-stream-specialize-encodings) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
    %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
    %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
    %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
    %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
    %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
    %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
    %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
    %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
    %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
    %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
    %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
    util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After EncodeDeviceTensorsPass (iree-stream-encode-device-tensors) //----- //
stream.executable private @test_simple_argmax_4x16_dispatch_1 {
  stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    stream.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0xFC00 : f16
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16xi32>
      %7 = tensor.empty() : tensor<4x16xf16>
      %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %10 = arith.cmpf ogt, %in, %init : f16
          %11 = arith.select %10, %in, %init : f16
          %12 = arith.select %10, %in_0, %init_1 : i32
          linalg.yield %11, %12 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      return
    }
  }
}

// -----// IR Dump After EncodeDeviceTensorsPass (iree-stream-encode-device-tensors) //----- //
stream.executable private @test_simple_argmax_4x16_dispatch_0 {
  stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    stream.return %result_x, %result_y, %result_z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
      %c1336 = arith.constant 1336 : index
      %c-1 = arith.constant -1 : index
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0xFC00 : f16
      %c0 = arith.constant 0 : index
      %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
      %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      %3 = tensor.empty() : tensor<4x16x96xi32>
      %4 = tensor.empty() : tensor<4x16x96xf16>
      %5 = tensor.empty() : tensor<4x16xi32>
      %6 = tensor.empty() : tensor<4x16xf16>
      %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %10 = arith.cmpi slt, %arg3, %c0 : index
        %11 = arith.subi %c-1, %arg3 : index
        %12 = arith.select %10, %11, %arg3 : index
        %13 = arith.divsi %12, %c1336 : index
        %14 = arith.subi %c-1, %13 : index
        %15 = arith.select %10, %14, %13 : index
        %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %17 = arith.muli %15, %c1336 : index
        %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
        ^bb0(%arg6: f16, %arg7: f16):
          %19 = arith.cmpf ogt, %arg6, %arg7 : f16
          iree_linalg_ext.yield %19 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      return
    }
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x128256xf16> : index
  %1 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%0}
  %2 = stream.async.clone on(#hal.device.affinity<@__device_0>) %1 : !stream.resource<external>{%0} -> !stream.resource<*>{%0}
  %3 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xf16> : index
  %4 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16x96xi32> : index
  %5:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%2) : (tensor<4x16x128256xf16> in !stream.resource<*>{%0}) -> (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4})
  %6 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xf16> : index
  %7 = stream.tensor.sizeof on(#hal.device.affinity<@__device_0>) tensor<4x16xi32> : index
  %8:2 = stream.tensor.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0, %5#1) : (tensor<4x16x96xf16> in !stream.resource<*>{%3}, tensor<4x16x96xi32> in !stream.resource<*>{%4}) -> (tensor<4x16xf16> in !stream.resource<*>{%6}, tensor<4x16xi32> in !stream.resource<*>{%7})
  %9 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#0 : !stream.resource<*>{%6} -> !stream.resource<external>{%6}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9 : tensor<4x16xf16> in !stream.resource<external>{%6} -> !hal.buffer_view
  %11 = stream.async.clone on(#hal.device.affinity<@__device_0>) %8#1 : !stream.resource<*>{%7} -> !stream.resource<external>{%7}
  %12 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %11 : tensor<4x16xi32> in !stream.resource<external>{%7} -> !hal.buffer_view
  util.return %10, %12 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After EncodeHostTensorsPass (iree-stream-encode-host-tensors) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After MaterializeEncodingsPass (iree-stream-materialize-encodings) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncResourcesPass (iree-stream-verify-lowering-to-async-resources) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeCopyOnWritePass (iree-stream-materialize-copy-on-write) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After EmplaceAllocationsPass (iree-stream-emplace-allocations) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<*>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<*>{%c16416768}) -> (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<*>{%c12288}, !stream.resource<*>{%c24576}) -> (!stream.resource<*>{%c128}, !stream.resource<*>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<*>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<*>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After RefineUsagePass (iree-stream-refine-usage) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<external>{%c16416768}
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<external>{%c128} -> !stream.resource<external>{%c128}
    %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<external>{%c256} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1 = stream.async.clone on(#hal.device.affinity<@__device_0>) %0 : !stream.resource<external>{%c16416768} -> !stream.resource<external>{%c16416768}
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %3:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%2#0[%c0 to %c12288 for %c12288], %2#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %4 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#0 : !stream.resource<external>{%c128} -> !stream.resource<external>{%c128}
  %5 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %4 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %6 = stream.async.clone on(#hal.device.affinity<@__device_0>) %3#1 : !stream.resource<external>{%c256} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %5, %7 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ElideAsyncCopiesPass (iree-stream-elide-async-copies) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
  %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyAsyncAccessRangesPass (iree-stream-verify-async-access-ranges) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%0[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %2:2 = stream.async.dispatch on(#hal.device.affinity<@__device_0>) @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%1#0[%c0 to %c12288 for %c12288], %1#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleExecutionPass (iree-stream-schedule-execution) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ScheduleConcurrencyPass (iree-stream-schedule-concurrency) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SyncInitializersPass (iree-stream-sync-initializers) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After PropagateTimepointsPass (iree-stream-propagate-timepoints) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.timepoint.immediate => !stream.timepoint
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %6:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0[%c0 to %c12288 for %c12288], %5#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %6#0, %6#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %2:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeBuiltinsPass (iree-stream-materialize-builtins) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %1 = stream.timepoint.immediate => !stream.timepoint
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %6:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%5#0[%c0 to %c12288 for %c12288], %5#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %6#0, %6#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %2:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %4 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %2#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %3, %4 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
    %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
    %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
    stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  } => !stream.timepoint
  %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToAsyncPass (iree-stream-verify-lowering-to-async) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %results:2, %result_timepoint = stream.async.execute on(#hal.device.affinity<@__device_0>) with(%0 as %arg1: !stream.resource<external>{%c16416768}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256}) {
      %4:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg1[%c0 to %c16416768 for %c16416768]) : (!stream.resource<external>{%c16416768}) -> (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576})
      %5:2 = stream.async.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%4#0[%c0 to %c12288 for %c12288], %4#1[%c0 to %c24576 for %c24576]) : (!stream.resource<transient>{%c12288}, !stream.resource<transient>{%c24576}) -> (!stream.resource<external>{%c128}, !stream.resource<external>{%c256})
      stream.yield %5#0, %5#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    } => !stream.timepoint
    %1:2 = stream.timepoint.await %result_timepoint => %results#0, %results#1 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %2 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %3 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %1#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %2, %3 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ScheduleAllocationPass (iree-stream-schedule-allocation) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %c0_0 = arith.constant 0 : index
    %1:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 0] = %c128,
      [0, 0] = %c256
    }) : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%1#0} => !stream.timepoint
    %2 = stream.resource.subview %result[%1#1] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c128}
    %3 = stream.resource.subview %result[%1#2] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c256}
    %4:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 1] = %c12288,
      [0, 1] = %c24576
    }) : index
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%4#0} => !stream.timepoint
    %5 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2) => !stream.timepoint
    %6 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%5) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %2 as %arg2: !stream.resource<external>{%c128}, %3 as %arg3: !stream.resource<external>{%c256}, %result_1 as %arg4: !stream.resource<transient>{%4#0}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        wo %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        ro %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0},
        wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c128},
        wo %arg3[%c0_0 for %c256] : !stream.resource<external>{%c256}
      }
    } => !stream.timepoint
    %7 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%6) => %result_1 : !stream.resource<transient>{%4#0} => !stream.timepoint
    %8 = stream.timepoint.join max(%7, %6) => !stream.timepoint
    %9:2 = stream.timepoint.await %8 => %2, %3 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %10, %11 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After EmplaceTransientsPass (iree-stream-emplace-transients) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %c0_0 = arith.constant 0 : index
    %1:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 0] = %c128,
      [0, 0] = %c256
    }) : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%1#0} => !stream.timepoint
    %2 = stream.resource.subview %result[%1#1] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c128}
    %3 = stream.resource.subview %result[%1#2] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c256}
    %4:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 1] = %c12288,
      [0, 1] = %c24576
    }) : index
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%4#0} => !stream.timepoint
    %5 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2) => !stream.timepoint
    %6 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%5) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %2 as %arg2: !stream.resource<external>{%c128}, %3 as %arg3: !stream.resource<external>{%c256}, %result_1 as %arg4: !stream.resource<transient>{%4#0}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        wo %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        ro %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0},
        wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c128},
        wo %arg3[%c0_0 for %c256] : !stream.resource<external>{%c256}
      }
    } => !stream.timepoint
    %7 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%6) => %result_1 : !stream.resource<transient>{%4#0} => !stream.timepoint
    %8 = stream.timepoint.join max(%7, %6) => !stream.timepoint
    %9:2 = stream.timepoint.await %8 => %2, %3 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %10, %11 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTransientSizeQueriesPass (iree-stream-materialize-transient-size-queries) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %c0_0 = arith.constant 0 : index
    %1:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 0] = %c128,
      [0, 0] = %c256
    }) : index
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%1#0} => !stream.timepoint
    %2 = stream.resource.subview %result[%1#1] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c128}
    %3 = stream.resource.subview %result[%1#2] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c256}
    %4:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
      [0, 1] = %c12288,
      [0, 1] = %c24576
    }) : index
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%4#0} => !stream.timepoint
    %5 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2) => !stream.timepoint
    %6 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%5) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %2 as %arg2: !stream.resource<external>{%c128}, %3 as %arg3: !stream.resource<external>{%c256}, %result_1 as %arg4: !stream.resource<transient>{%4#0}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        wo %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
        ro %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0},
        wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c128},
        wo %arg3[%c0_0 for %c256] : !stream.resource<external>{%c256}
      }
    } => !stream.timepoint
    %7 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%6) => %result_1 : !stream.resource<transient>{%4#0} => !stream.timepoint
    %8 = stream.timepoint.join max(%7, %6) => !stream.timepoint
    %9:2 = stream.timepoint.await %8 => %2, %3 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %10, %11 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After PackConstantsPass (iree-stream-pack-constants) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %c0_0 = arith.constant 0 : index
  %1:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
    [0, 0] = %c128,
    [0, 0] = %c256
  }) : index
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%1#0} => !stream.timepoint
  %2 = stream.resource.subview %result[%1#1] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c128}
  %3 = stream.resource.subview %result[%1#2] : !stream.resource<external>{%1#0} -> !stream.resource<external>{%c256}
  %4:3 = stream.resource.pack on(#hal.device.affinity<@__device_0>) slices({
    [0, 1] = %c12288,
    [0, 1] = %c24576
  }) : index
  %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%4#0} => !stream.timepoint
  %5 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2) => !stream.timepoint
  %6 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%5) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %2 as %arg2: !stream.resource<external>{%c128}, %3 as %arg3: !stream.resource<external>{%c256}, %result_1 as %arg4: !stream.resource<transient>{%4#0}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
      wo %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg4[%4#1 for %c12288] : !stream.resource<transient>{%4#0},
      ro %arg4[%4#2 for %c24576] : !stream.resource<transient>{%4#0},
      wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c128},
      wo %arg3[%c0_0 for %c256] : !stream.resource<external>{%c256}
    }
  } => !stream.timepoint
  %7 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%6) => %result_1 : !stream.resource<transient>{%4#0} => !stream.timepoint
  %8 = stream.timepoint.join max(%7, %6) => !stream.timepoint
  %9:2 = stream.timepoint.await %8 => %2, %3 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %11 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %9#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %10, %11 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After LayoutSlicesPass (iree-stream-layout-slices) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %c0_0 = arith.constant 0 : index
  %c0_1 = arith.constant 0 : index
  %c128_2 = arith.constant 128 : index
  %c128_3 = arith.constant 128 : index
  %c384 = arith.constant 384 : index
  %c384_4 = arith.constant 384 : index
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384_4} => !stream.timepoint
  %1 = stream.resource.subview %result[%c0_1] : !stream.resource<external>{%c384_4} -> !stream.resource<external>{%c128}
  %2 = stream.resource.subview %result[%c128_3] : !stream.resource<external>{%c384_4} -> !stream.resource<external>{%c256}
  %c0_5 = arith.constant 0 : index
  %c12288_6 = arith.constant 12288 : index
  %c12288_7 = arith.constant 12288 : index
  %c36864 = arith.constant 36864 : index
  %c36864_8 = arith.constant 36864 : index
  %result_9, %result_timepoint_10 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864_8} => !stream.timepoint
  %3 = stream.timepoint.join max(%result_timepoint, %result_timepoint_10) => !stream.timepoint
  %4 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%3) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %1 as %arg2: !stream.resource<external>{%c128}, %2 as %arg3: !stream.resource<external>{%c256}, %result_9 as %arg4: !stream.resource<transient>{%c36864_8}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg4[%c0_5 for %c12288] : !stream.resource<transient>{%c36864_8},
      wo %arg4[%c12288_7 for %c24576] : !stream.resource<transient>{%c36864_8}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg4[%c0_5 for %c12288] : !stream.resource<transient>{%c36864_8},
      ro %arg4[%c12288_7 for %c24576] : !stream.resource<transient>{%c36864_8},
      wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c128},
      wo %arg3[%c0_0 for %c256] : !stream.resource<external>{%c256}
    }
  } => !stream.timepoint
  %5 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%4) => %result_9 : !stream.resource<transient>{%c36864_8} => !stream.timepoint
  %6 = stream.timepoint.join max(%5, %4) => !stream.timepoint
  %7:2 = stream.timepoint.await %6 => %1, %2 : !stream.resource<external>{%c128}, !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7#0 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7#1 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c0 = arith.constant 0 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After PropagateSubrangesPass (iree-util-propagate-subranges) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c0_0 = arith.constant 0 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_1, %result_timepoint_2 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_2) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_1 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0_0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0_0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0_0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0_0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_1 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0_0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AutomaticReferenceCountingPass (iree-stream-automatic-reference-counting) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateConstantTransientSizePass (iree-stream-annotate-constant-transient-size) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyLoweringToCmdPass (iree-stream-verify-lowering-to-cmd) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After SCFToControlFlowPass (convert-scf-to-cf) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ReuseAllocationsPass (iree-stream-reuse-allocations) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
  %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
  %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.join max(%3, %2) => !stream.timepoint
    %5 = stream.timepoint.await %4 => %result : !stream.resource<external>{%c384}
    %6 = stream.resource.subview %5[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %7 = stream.resource.subview %5[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %8, %9 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepointsPass (iree-stream-elide-timepoints) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 0 : index, iree.fixedpoint.modified, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.immediate => !stream.timepoint
    %5 = stream.timepoint.join max(%3, %4) => !stream.timepoint
    %6 = stream.timepoint.await %5 => %result : !stream.resource<external>{%c384}
    %7 = stream.resource.subview %6[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %8 = stream.resource.subview %6[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %9 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %7 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %10 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %8 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %9, %10 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c24576 = arith.constant 24576 : index
  %c12288 = arith.constant 12288 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
      ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
      wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 1 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 1 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 1 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ElideTimepointsPass (iree-stream-elide-timepoints) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {iree.fixedpoint.iteration = 1 : index, stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FixedPointIteratorPass (iree-util-fixed-point-iterator) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: !stream.binding) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg2[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg3[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        wo %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c12288] : !stream.resource<transient>{%c36864},
        ro %arg3[%c12288 for %c24576] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c128] : !stream.resource<external>{%c384},
        wo %arg2[%c128 for %c256] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseDispatchBindingsPass (iree-stream-fuse-dispatch-bindings) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg6) = (0) to (128256) step (1336) shared_outs(%arg7 = %4, %arg8 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg6, %c0 : index
          %11 = arith.subi %c-1, %arg6 : index
          %12 = arith.select %10, %11, %arg6 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg6], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg7[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg8[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg9: f16, %arg10: f16):
            %19 = arith.cmpf ogt, %arg9, %arg10 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg7[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg8[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding, %arg1: !stream.binding, %arg2: !stream.binding, %arg3: index, %arg4: index, %arg5: index, %arg6: index) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%arg6] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %c0_2 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0, %c0, %c12288 : index, index, index) {
        ro %arg1[%c0_2 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0, %c12288, %c0, %c128 : index, index, index, index) {
        ro %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchArgumentsPass (iree-stream-annotate-dispatch-arguments) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.alignment = 4096 : index, stream.values = [12288 : index]}) {
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg6) = (0) to (128256) step (1336) shared_outs(%arg7 = %4, %arg8 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg6, %c0 : index
          %11 = arith.subi %c-1, %arg6 : index
          %12 = arith.select %10, %11, %arg6 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg6], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg7[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg8[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg9: f16, %arg10: f16):
            %19 = arith.cmpf ogt, %arg9, %arg10 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg7[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg8[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.alignment = 4096 : index, stream.values = [12288 : index]}, %arg5: index {stream.values = [0 : index]}, %arg6: index {stream.alignment = 128 : index, stream.values = [128 : index]}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %0 = stream.binding.subspan %arg0[%arg3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%arg4] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%arg5] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%arg6] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %c0_2 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0, %c0, %c12288 : index, index, index) {
        ro %arg1[%c0_2 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0, %c12288, %c0, %c128 : index, index, index, index) {
        ro %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AnnotateDispatchAssumptionsPass (iree-stream-annotate-dispatch-assumptions) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.values = [0 : index]}, %arg5: index {stream.alignment = 4096 : index, stream.values = [12288 : index]}) {
        %0:3 = util.assume.int 
            %arg3<umin = 0, umax = 0>, 
            %arg4<umin = 0, umax = 0>, 
            %arg5<umin = 12288, umax = 12288, udiv = 12288>
          : index, index, index
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %1 = stream.binding.subspan %arg0[%0#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %2 = stream.binding.subspan %arg1[%0#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %3 = stream.binding.subspan %arg2[%0#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %4 = tensor.empty() : tensor<4x16x96xi32>
        %5 = tensor.empty() : tensor<4x16x96xf16>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %9 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %10:2 = scf.forall (%arg6) = (0) to (128256) step (1336) shared_outs(%arg7 = %5, %arg8 = %4) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %11 = arith.cmpi slt, %arg6, %c0 : index
          %12 = arith.subi %c-1, %arg6 : index
          %13 = arith.select %11, %12, %arg6 : index
          %14 = arith.divsi %13, %c1336 : index
          %15 = arith.subi %c-1, %14 : index
          %16 = arith.select %11, %15, %14 : index
          %17 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, %arg6], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg7[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%8 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg8[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%9 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %18 = arith.muli %16, %c1336 : index
          %19:2 = iree_linalg_ext.arg_compare dimension(2) ins(%17 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%18 : index) {
          ^bb0(%arg9: f16, %arg10: f16):
            %20 = arith.cmpf ogt, %arg9, %arg10 : f16
            iree_linalg_ext.yield %20 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %19#0 into %arg7[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %19#1 into %arg8[0, 0, %16] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: index {stream.values = [0 : index]}, %arg4: index {stream.alignment = 4096 : index, stream.values = [12288 : index]}, %arg5: index {stream.values = [0 : index]}, %arg6: index {stream.alignment = 128 : index, stream.values = [128 : index]}) {
        %0:4 = util.assume.int 
            %arg3<umin = 0, umax = 0>, 
            %arg4<umin = 12288, umax = 12288, udiv = 12288>, 
            %arg5<umin = 0, umax = 0>, 
            %arg6<umin = 128, umax = 128, udiv = 128>
          : index, index, index, index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %1 = stream.binding.subspan %arg0[%0#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg0[%0#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %3 = stream.binding.subspan %arg1[%0#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %4 = stream.binding.subspan %arg2[%0#3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %6 = iree_tensor_ext.dispatch.tensor.load %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %7 = tensor.empty() : tensor<4x16xi32>
        %8 = tensor.empty() : tensor<4x16xf16>
        %9 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %10 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%5, %6 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%10, %9 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %11 = arith.cmpf ogt, %in, %init : f16
            %12 = arith.select %11, %in, %init : f16
            %13 = arith.select %11, %in_0, %init_1 : i32
            linalg.yield %12, %13 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %4, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %c0_2 = arith.constant 0 : index
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0, %c0, %c12288 : index, index, index) {
        ro %arg1[%c0_2 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0, %c12288, %c0, %c128 : index, index, index, index) {
        ro %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After PackDispatchOperandsPass (iree-stream-pack-dispatch-operands) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %c32_i64 = arith.constant 32 : i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %c32_i64_0 = arith.constant 32 : i64
        %7 = arith.shli %6, %c32_i64_0 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.values = [0 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %c32_i64_1 = arith.constant 32 : i64
        %12 = arith.shli %11, %c32_i64_1 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.alignment = 4096 : index, stream.values = [12288 : index]} : i64 to index
        %15:3 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 0, umax = 0>, 
            %14<umin = 12288, umax = 12288, udiv = 12288>
          : index, index, index
        %c1336 = arith.constant 1336 : index
        %c-1 = arith.constant -1 : index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %16 = stream.binding.subspan %arg0[%15#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %17 = stream.binding.subspan %arg1[%15#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %18 = stream.binding.subspan %arg2[%15#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %19 = tensor.empty() : tensor<4x16x96xi32>
        %20 = tensor.empty() : tensor<4x16x96xf16>
        %21 = tensor.empty() : tensor<4x16xi32>
        %22 = tensor.empty() : tensor<4x16xf16>
        %23 = linalg.fill ins(%cst : f16) outs(%22 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %24 = linalg.fill ins(%c0_i32 : i32) outs(%21 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %25:2 = scf.forall (%arg9) = (0) to (128256) step (1336) shared_outs(%arg10 = %20, %arg11 = %19) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %26 = arith.cmpi slt, %arg9, %c0 : index
          %27 = arith.subi %c-1, %arg9 : index
          %28 = arith.select %26, %27, %arg9 : index
          %29 = arith.divsi %28, %c1336 : index
          %30 = arith.subi %c-1, %29 : index
          %31 = arith.select %26, %30, %29 : index
          %32 = iree_tensor_ext.dispatch.tensor.load %16, offsets = [0, 0, %arg9], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg10[0, 0, %31] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%23 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_2 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_3 = tensor.extract_slice %arg11[0, 0, %31] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_4 = linalg.broadcast ins(%24 : tensor<4x16xi32>) outs(%extracted_slice_3 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_5 = tensor.extract_slice %broadcasted_4[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %33 = arith.muli %31, %c1336 : index
          %34:2 = iree_linalg_ext.arg_compare dimension(2) ins(%32 : tensor<4x16x1336xf16>) outs(%extracted_slice_2, %extracted_slice_5 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%33 : index) {
          ^bb0(%arg12: f16, %arg13: f16):
            %35 = arith.cmpf ogt, %arg12, %arg13 : f16
            iree_linalg_ext.yield %35 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %34#0 into %arg10[0, 0, %31] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %34#1 into %arg11[0, 0, %31] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %25#0, %17, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %25#1, %18, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32) {
        %0 = arith.extui %arg3 : i32 to i64
        %1 = arith.extui %arg4 : i32 to i64
        %c32_i64 = arith.constant 32 : i64
        %2 = arith.shli %1, %c32_i64 : i64
        %3 = arith.ori %0, %2 : i64
        %4 = arith.index_castui %3 {stream.values = [0 : index]} : i64 to index
        %5 = arith.extui %arg5 : i32 to i64
        %6 = arith.extui %arg6 : i32 to i64
        %c32_i64_0 = arith.constant 32 : i64
        %7 = arith.shli %6, %c32_i64_0 : i64
        %8 = arith.ori %5, %7 : i64
        %9 = arith.index_castui %8 {stream.alignment = 4096 : index, stream.values = [12288 : index]} : i64 to index
        %10 = arith.extui %arg7 : i32 to i64
        %11 = arith.extui %arg8 : i32 to i64
        %c32_i64_1 = arith.constant 32 : i64
        %12 = arith.shli %11, %c32_i64_1 : i64
        %13 = arith.ori %10, %12 : i64
        %14 = arith.index_castui %13 {stream.values = [0 : index]} : i64 to index
        %15 = arith.extui %arg9 : i32 to i64
        %16 = arith.extui %arg10 : i32 to i64
        %c32_i64_2 = arith.constant 32 : i64
        %17 = arith.shli %16, %c32_i64_2 : i64
        %18 = arith.ori %15, %17 : i64
        %19 = arith.index_castui %18 {stream.alignment = 128 : index, stream.values = [128 : index]} : i64 to index
        %20:4 = util.assume.int 
            %4<umin = 0, umax = 0>, 
            %9<umin = 12288, umax = 12288, udiv = 12288>, 
            %14<umin = 0, umax = 0>, 
            %19<umin = 128, umax = 128, udiv = 128>
          : index, index, index, index
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %21 = stream.binding.subspan %arg0[%20#0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %22 = stream.binding.subspan %arg0[%20#1] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %23 = stream.binding.subspan %arg1[%20#2] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %24 = stream.binding.subspan %arg2[%20#3] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %25 = iree_tensor_ext.dispatch.tensor.load %21, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %26 = iree_tensor_ext.dispatch.tensor.load %22, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %27 = tensor.empty() : tensor<4x16xi32>
        %28 = tensor.empty() : tensor<4x16xf16>
        %29 = linalg.fill ins(%c0_i32 : i32) outs(%27 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %30 = linalg.fill ins(%cst : f16) outs(%28 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%25, %26 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%30, %29 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_3: i32, %init: f16, %init_4: i32) {
            %31 = arith.cmpf ogt, %in, %init : f16
            %32 = arith.select %31, %in, %init : f16
            %33 = arith.select %31, %in_3, %init_4 : i32
            linalg.yield %32, %33 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %23, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %24, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c24576 = arith.constant 24576 : index
    %c12288 = arith.constant 12288 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %c0_2 = arith.constant 0 : index
    %c0_i64 = arith.constant 0 : i64
    %c0_i32 = arith.constant 0 : i32
    %c32_i64 = arith.constant 32 : i64
    %c0_i64_3 = arith.constant 0 : i64
    %c0_i32_4 = arith.constant 0 : i32
    %c0_i64_5 = arith.constant 0 : i64
    %c0_i32_6 = arith.constant 0 : i32
    %c32_i64_7 = arith.constant 32 : i64
    %c0_i64_8 = arith.constant 0 : i64
    %c0_i32_9 = arith.constant 0 : i32
    %c12288_i64 = arith.constant 12288 : i64
    %c12288_i32 = arith.constant 12288 : i32
    %c32_i64_10 = arith.constant 32 : i64
    %c0_i64_11 = arith.constant 0 : i64
    %c0_i32_12 = arith.constant 0 : i32
    %c0_i64_13 = arith.constant 0 : i64
    %c0_i32_14 = arith.constant 0 : i32
    %c32_i64_15 = arith.constant 32 : i64
    %c0_i64_16 = arith.constant 0 : i64
    %c0_i32_17 = arith.constant 0 : i32
    %c12288_i64_18 = arith.constant 12288 : i64
    %c12288_i32_19 = arith.constant 12288 : i32
    %c32_i64_20 = arith.constant 32 : i64
    %c0_i64_21 = arith.constant 0 : i64
    %c0_i32_22 = arith.constant 0 : i32
    %c0_i64_23 = arith.constant 0 : i64
    %c0_i32_24 = arith.constant 0 : i32
    %c32_i64_25 = arith.constant 32 : i64
    %c0_i64_26 = arith.constant 0 : i64
    %c0_i32_27 = arith.constant 0 : i32
    %c128_i64 = arith.constant 128 : i64
    %c128_i32 = arith.constant 128 : i32
    %c32_i64_28 = arith.constant 32 : i64
    %c0_i64_29 = arith.constant 0 : i64
    %c0_i32_30 = arith.constant 0 : i32
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32_4, %c0_i32_6, %c0_i32_9, %c12288_i32, %c0_i32_12 : i32, i32, i32, i32, i32, i32) {
        ro %arg1[%c0_2 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32_14, %c0_i32_17, %c12288_i32_19, %c0_i32_22, %c0_i32_24, %c0_i32_27, %c128_i32, %c0_i32_30 : i32, i32, i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0_2 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0_2 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128_i32 = arith.constant 128 : i32
  %c12288_i32 = arith.constant 12288 : i32
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128_i32 = arith.constant 128 : i32
  %c12288_i32 = arith.constant 12288 : i32
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128_i32 = arith.constant 128 : i32
  %c12288_i32 = arith.constant 12288 : i32
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128_i32 = arith.constant 128 : i32
  %c12288_i32 = arith.constant 12288 : i32
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c128_i32 = arith.constant 128 : i32
  %c12288_i32 = arith.constant 12288 : i32
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg9) = (0) to (128256) step (1336) shared_outs(%arg10 = %4, %arg11 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg9, %c0 : index
          %11 = arith.subi %c-1, %arg9 : index
          %12 = arith.select %10, %11, %arg9 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg9], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg12: f16, %arg13: f16):
            %19 = arith.cmpf ogt, %arg12, %arg13 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128_i32 = arith.constant 128 : i32
    %c12288_i32 = arith.constant 12288 : i32
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg9) = (0) to (128256) step (1336) shared_outs(%arg10 = %4, %arg11 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg9, %c0 : index
          %11 = arith.subi %c-1, %arg9 : index
          %12 = arith.select %10, %11, %arg9 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg9], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg12: f16, %arg13: f16):
            %19 = arith.cmpf ogt, %arg12, %arg13 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128_i32 = arith.constant 128 : i32
    %c12288_i32 = arith.constant 12288 : i32
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32) {
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg9) = (0) to (128256) step (1336) shared_outs(%arg10 = %4, %arg11 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg9, %c0 : index
          %11 = arith.subi %c-1, %arg9 : index
          %12 = arith.select %10, %11, %arg9 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg9], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg12: f16, %arg13: f16):
            %19 = arith.cmpf ogt, %arg12, %arg13 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg10[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg11[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}, %arg3: i32, %arg4: i32, %arg5: i32, %arg6: i32, %arg7: i32, %arg8: i32, %arg9: i32, %arg10: i32) {
        %cst = arith.constant 0xFC00 : f16
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128_i32 = arith.constant 128 : i32
    %c12288_i32 = arith.constant 12288 : i32
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%c0_i32, %c0_i32, %c0_i32, %c0_i32, %c12288_i32, %c0_i32 : i32, i32, i32, i32, i32, i32) {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%c0_i32, %c0_i32, %c12288_i32, %c0_i32, %c0_i32, %c0_i32, %c128_i32, %c0_i32 : i32, i32, i32, i32, i32, i32, i32, i32) {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FoldUniformOperandsPass (iree-stream-fold-uniform-operands) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c12288_i32 = arith.constant 12288 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c0_i32_0 = arith.constant 0 : i32
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32_0 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_1 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_2 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_3 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_2 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_4 = tensor.extract_slice %broadcasted_3[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_1, %extracted_slice_4 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c12288_i32 = arith.constant 12288 : i32
        %c128_i32 = arith.constant 128 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0_i32_0 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32_0 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_1: i32, %init: f16, %init_2: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_1, %init_2 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c128_i32 = arith.constant 128 : i32
    %c12288_i32 = arith.constant 12288 : i32
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After IPOPass (iree-util-ipo) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After SymbolDCE (symbol-dce) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyInitializationOrderPass (iree-util-verify-initialization-order) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AttributeCallGraphPass (iree-util-attribute-call-graph) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After AssignTargetDevicesPass (iree-hal-assign-target-devices) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeTargetDevicesPass (iree-hal-materialize-target-devices) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDevicePromisesPass (iree-hal-resolve-device-promises) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After ResolveDeviceAliasesPass (iree-hal-resolve-device-aliases) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After Canonicalizer (canonicalize) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After CSE (cse) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After SimplifyGlobalAccessesPass (iree-util-simplify-global-accesses) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After ApplyPatternsPass (iree-util-apply-patterns) //----- //
util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
  %c0 = arith.constant 0 : index
  %c36864 = arith.constant 36864 : index
  %c384 = arith.constant 384 : index
  %c256 = arith.constant 256 : index
  %c128 = arith.constant 128 : index
  %c16416768 = arith.constant 16416768 : index
  %c128256 = arith.constant 128256 : index
  %c16 = arith.constant 16 : index
  %c4 = arith.constant 4 : index
  %element_type_f16 = hal.element_type<f16> : i32
  %dense_row_major = hal.encoding_type<dense_row_major> : i32
  hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
  %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
  %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
  %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
  %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
  %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
      ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
    }
    stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
      ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
      wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
    }
  } => !stream.timepoint
  %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
  %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
  %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
  %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
  %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
  %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
  util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
}

// -----// IR Dump After FoldGlobalsPass (iree-util-fold-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After FuseGlobalsPass (iree-util-fuse-globals) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After VerifyDevicesPass (iree-hal-verify-devices) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  stream.executable private @test_simple_argmax_4x16_dispatch_0 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      stream.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        %2 = stream.binding.subspan %arg2[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        %3 = tensor.empty() : tensor<4x16x96xi32>
        %4 = tensor.empty() : tensor<4x16x96xf16>
        %5 = tensor.empty() : tensor<4x16xi32>
        %6 = tensor.empty() : tensor<4x16xf16>
        %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9:2 = scf.forall (%arg3) = (0) to (128256) step (1336) shared_outs(%arg4 = %4, %arg5 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %10 = arith.cmpi slt, %arg3, %c0 : index
          %11 = arith.subi %c-1, %arg3 : index
          %12 = arith.select %10, %11, %arg3 : index
          %13 = arith.divsi %12, %c1336 : index
          %14 = arith.subi %c-1, %13 : index
          %15 = arith.select %10, %14, %13 : index
          %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg3], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %17 = arith.muli %15, %c1336 : index
          %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
          ^bb0(%arg6: f16, %arg7: f16):
            %19 = arith.cmpf ogt, %arg6, %arg7 : f16
            iree_linalg_ext.yield %19 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %18#0 into %arg4[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %18#1 into %arg5[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
        iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
        return
      }
    }
  }
  stream.executable private @test_simple_argmax_4x16_dispatch_1 {
    stream.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 workgroups() -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      stream.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !stream.binding {stream.alignment = 64 : index}, %arg1: !stream.binding {stream.alignment = 64 : index}, %arg2: !stream.binding {stream.alignment = 64 : index}) {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = stream.binding.subspan %arg0[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
        %1 = stream.binding.subspan %arg0[%c12288] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
        %2 = stream.binding.subspan %arg1[%c0] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        %3 = stream.binding.subspan %arg2[%c128] : !stream.binding -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
        %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16xi32>
        %7 = tensor.empty() : tensor<4x16xf16>
        %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
          (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
            %10 = arith.cmpf ogt, %in, %init : f16
            %11 = arith.select %10, %in, %init : f16
            %12 = arith.select %10, %in_0, %init_1 : i32
            linalg.yield %11, %12 : f16, i32
          }
        iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
        iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
        return
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After MaterializeInterfacesPass (iree-hal-materialize-interfaces) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  hal.executable private @test_simple_argmax_4x16_dispatch_0 {
    hal.executable.variant public @rocm_hsaco_fb target(#executable_target_rocm_hsaco_fb) {
      hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
        hal.return %result_x, %result_y, %result_z : index, index, index
      }
      builtin.module {
        func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
          %c0_i32 = arith.constant 0 : i32
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0xFC00 : f16
          %c-1 = arith.constant -1 : index
          %c1336 = arith.constant 1336 : index
          %c12288 = arith.constant 12288 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c12288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
          %3 = tensor.empty() : tensor<4x16x96xi32>
          %4 = tensor.empty() : tensor<4x16x96xf16>
          %5 = tensor.empty() : tensor<4x16xi32>
          %6 = tensor.empty() : tensor<4x16xf16>
          %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
          %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
          %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
            %10 = arith.cmpi slt, %arg0, %c0 : index
            %11 = arith.subi %c-1, %arg0 : index
            %12 = arith.select %10, %11, %arg0 : index
            %13 = arith.divsi %12, %c1336 : index
            %14 = arith.subi %c-1, %13 : index
            %15 = arith.select %10, %14, %13 : index
            %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
            %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
            %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
            %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
            %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
            %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
            %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
            %17 = arith.muli %15, %c1336 : index
            %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
            ^bb0(%arg3: f16, %arg4: f16):
              %19 = arith.cmpf ogt, %arg3, %arg4 : f16
              iree_linalg_ext.yield %19 : i1
            } -> tensor<4x16xf16>, tensor<4x16xi32>
            scf.forall.in_parallel {
              tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
              tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
            }
          } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
          iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
          iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
          return
        }
      }
    }
  }
  hal.executable private @test_simple_argmax_4x16_dispatch_1 {
    hal.executable.variant public @rocm_hsaco_fb target(#executable_target_rocm_hsaco_fb) {
      hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
          %c0_i32 = arith.constant 0 : i32
          %cst = arith.constant 0xFC00 : f16
          %c0 = arith.constant 0 : index
          %c12288 = arith.constant 12288 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
          %3 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
          %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
          %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
          %6 = tensor.empty() : tensor<4x16xi32>
          %7 = tensor.empty() : tensor<4x16xf16>
          %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
          %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
          %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
            (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
              %10 = arith.cmpf ogt, %in, %init : f16
              %11 = arith.select %10, %in, %init : f16
              %12 = arith.select %10, %in_0, %init_1 : i32
              linalg.yield %11, %12 : f16, i32
            }
          iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
          iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
          return
        }
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@rocm_hsaco_fb::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@rocm_hsaco_fb::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After PruneExecutablesPass (iree-hal-prune-executables) //----- //
#executable_target_rocm_hsaco_fb = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
#pipeline_layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>
#device_target_hip = #hal.device.target<"hip", [#executable_target_rocm_hsaco_fb]> : !hal.device
module attributes {stream.affinity.default = #hal.device.affinity<@__device_0>} {
  util.global private @__device_0 = #device_target_hip
  hal.executable private @test_simple_argmax_4x16_dispatch_0 {
    hal.executable.variant public @rocm_hsaco_fb target(#executable_target_rocm_hsaco_fb) {
      hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
        hal.return %result_x, %result_y, %result_z : index, index, index
      }
      builtin.module {
        func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
          %c0_i32 = arith.constant 0 : i32
          %c0 = arith.constant 0 : index
          %cst = arith.constant 0xFC00 : f16
          %c-1 = arith.constant -1 : index
          %c1336 = arith.constant 1336 : index
          %c12288 = arith.constant 12288 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c12288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
          %3 = tensor.empty() : tensor<4x16x96xi32>
          %4 = tensor.empty() : tensor<4x16x96xf16>
          %5 = tensor.empty() : tensor<4x16xi32>
          %6 = tensor.empty() : tensor<4x16xf16>
          %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
          %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
          %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
            %10 = arith.cmpi slt, %arg0, %c0 : index
            %11 = arith.subi %c-1, %arg0 : index
            %12 = arith.select %10, %11, %arg0 : index
            %13 = arith.divsi %12, %c1336 : index
            %14 = arith.subi %c-1, %13 : index
            %15 = arith.select %10, %14, %13 : index
            %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
            %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
            %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
            %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
            %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
            %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
            %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
            %17 = arith.muli %15, %c1336 : index
            %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
            ^bb0(%arg3: f16, %arg4: f16):
              %19 = arith.cmpf ogt, %arg3, %arg4 : f16
              iree_linalg_ext.yield %19 : i1
            } -> tensor<4x16xf16>, tensor<4x16xi32>
            scf.forall.in_parallel {
              tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
              tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
            }
          } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
          iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
          iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
          return
        }
      }
    }
  }
  hal.executable private @test_simple_argmax_4x16_dispatch_1 {
    hal.executable.variant public @rocm_hsaco_fb target(#executable_target_rocm_hsaco_fb) {
      hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#pipeline_layout) count(%arg0: !hal.device) -> (index, index, index) {
        %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
        hal.return %x, %y, %z : index, index, index
      }
      builtin.module {
        func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
          %c0_i32 = arith.constant 0 : i32
          %cst = arith.constant 0xFC00 : f16
          %c0 = arith.constant 0 : index
          %c12288 = arith.constant 12288 : index
          %c128 = arith.constant 128 : index
          %0 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
          %1 = hal.interface.binding.subspan layout(#pipeline_layout) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
          %2 = hal.interface.binding.subspan layout(#pipeline_layout) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
          %3 = hal.interface.binding.subspan layout(#pipeline_layout) binding(2) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
          %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
          %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
          %6 = tensor.empty() : tensor<4x16xi32>
          %7 = tensor.empty() : tensor<4x16xf16>
          %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
          %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
          %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
            (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
              %10 = arith.cmpf ogt, %in, %init : f16
              %11 = arith.select %10, %in, %init : f16
              %12 = arith.select %10, %in_0, %init_1 : i32
              linalg.yield %11, %12 : f16, i32
            }
          iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
          iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
          return
        }
      }
    }
  }
  util.func public @test_simple_argmax_4x16(%arg0: !hal.buffer_view) -> (!hal.buffer_view, !hal.buffer_view) attributes {iree.abi.stub, iree.reflection = {iree.abi.declaration = "sync func @test_simple_argmax_4x16(%input0: tensor<4x16x128256xf16>) -> (%output0: tensor<4x16xf16>, %output1: tensor<4x16xi32>)"}} {
    %c0 = arith.constant 0 : index
    %c36864 = arith.constant 36864 : index
    %c384 = arith.constant 384 : index
    %c256 = arith.constant 256 : index
    %c128 = arith.constant 128 : index
    %c16416768 = arith.constant 16416768 : index
    %c128256 = arith.constant 128256 : index
    %c16 = arith.constant 16 : index
    %c4 = arith.constant 4 : index
    %element_type_f16 = hal.element_type<f16> : i32
    %dense_row_major = hal.encoding_type<dense_row_major> : i32
    hal.buffer_view.assert<%arg0 : !hal.buffer_view> message("input0") shape([%c4, %c16, %c128256]) type(%element_type_f16) encoding(%dense_row_major)
    %0 = stream.tensor.import on(#hal.device.affinity<@__device_0>) %arg0 : !hal.buffer_view -> tensor<4x16x128256xf16> in !stream.resource<external>{%c16416768}
    %result, %result_timepoint = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<external>{%c384} => !stream.timepoint
    %result_0, %result_timepoint_1 = stream.resource.alloca uninitialized on(#hal.device.affinity<@__device_0>) : !stream.resource<transient>{%c36864} => !stream.timepoint
    %1 = stream.timepoint.join max(%result_timepoint, %result_timepoint_1) => !stream.timepoint
    %2 = stream.cmd.execute on(#hal.device.affinity<@__device_0>) await(%1) => with(%0 as %arg1: !stream.resource<external>{%c16416768}, %result as %arg2: !stream.resource<external>{%c384}, %result_0 as %arg3: !stream.resource<transient>{%c36864}) {
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_0::@rocm_hsaco_fb::@test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 {
        ro %arg1[%c0 for %c16416768] : !stream.resource<external>{%c16416768},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864}
      }
      stream.cmd.dispatch @test_simple_argmax_4x16_dispatch_1::@rocm_hsaco_fb::@test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 {
        ro %arg3[%c0 for %c36864] : !stream.resource<transient>{%c36864},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384},
        wo %arg2[%c0 for %c384] : !stream.resource<external>{%c384}
      }
    } => !stream.timepoint
    %3 = stream.resource.dealloca on(#hal.device.affinity<@__device_0>) await(%2) => %result_0 : !stream.resource<transient>{%c36864} => !stream.timepoint
    %4 = stream.timepoint.await %3 => %result : !stream.resource<external>{%c384}
    %5 = stream.resource.subview %4[%c0] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c128}
    %6 = stream.resource.subview %4[%c128] : !stream.resource<external>{%c384} -> !stream.resource<external>{%c256}
    %7 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %5 : tensor<4x16xf16> in !stream.resource<external>{%c128} -> !hal.buffer_view
    %8 = stream.tensor.export on(#hal.device.affinity<@__device_0>) %6 : tensor<4x16xi32> in !stream.resource<external>{%c256} -> !hal.buffer_view
    util.return %7, %8 : !hal.buffer_view, !hal.buffer_view
  }
}


// -----// IR Dump After SpecializeExportsPass (iree-codegen-specialize-exports) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0xFC00 : f16
      %c0 = arith.constant 0 : index
      %c12288 = arith.constant 12288 : index
      %c128 = arith.constant 128 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
      %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16xi32>
      %7 = tensor.empty() : tensor<4x16xf16>
      %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
        (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
          %10 = arith.cmpf ogt, %in, %init : f16
          %11 = arith.select %10, %in, %init : f16
          %12 = arith.select %10, %in_0, %init_1 : i32
          linalg.yield %11, %12 : f16, i32
        }
      iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
      iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
      return
    }
  }
}

// -----// IR Dump After SpecializeExportsPass (iree-codegen-specialize-exports) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    hal.return %result_x, %result_y, %result_z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0xFC00 : f16
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %c12288 = arith.constant 12288 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      %3 = tensor.empty() : tensor<4x16x96xi32>
      %4 = tensor.empty() : tensor<4x16x96xf16>
      %5 = tensor.empty() : tensor<4x16xi32>
      %6 = tensor.empty() : tensor<4x16xf16>
      %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %10 = arith.cmpi slt, %arg0, %c0 : index
        %11 = arith.subi %c-1, %arg0 : index
        %12 = arith.select %10, %11, %arg0 : index
        %13 = arith.divsi %12, %c1336 : index
        %14 = arith.subi %c-1, %13 : index
        %15 = arith.select %10, %14, %13 : index
        %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %17 = arith.muli %15, %c1336 : index
        %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
        ^bb0(%arg3: f16, %arg4: f16):
          %19 = arith.cmpf ogt, %arg3, %arg4 : f16
          iree_linalg_ext.yield %19 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
      iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
      return
    }
  }
}

// -----// IR Dump After MaterializeDeviceEncodingPass (iree-codegen-materialize-device-encoding) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %reduced:2 = linalg.reduce ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) dimensions = [2] 
    (%in: f16, %in_0: i32, %init: f16, %init_1: i32) {
      %10 = arith.cmpf ogt, %in, %init : f16
      %11 = arith.select %10, %in, %init : f16
      %12 = arith.select %10, %in_0, %init_1 : i32
      linalg.yield %11, %12 : f16, i32
    }
  iree_tensor_ext.dispatch.tensor.store %reduced#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %reduced#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After MaterializeDeviceEncodingPass (iree-codegen-materialize-device-encoding) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After GPUGeneralizeNamedOpsPass (iree-codegen-gpu-generalize-named-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After GPUGeneralizeNamedOpsPass (iree-codegen-gpu-generalize-named-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After ROCDLConfigureBufferInstructionsPass (iree-rocdl-configure-buffer-instructions) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After ROCDLConfigureBufferInstructionsPass (iree-rocdl-configure-buffer-instructions) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After TypePropagationPass (iree-codegen-type-propagation) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After TypePropagationPass (iree-codegen-type-propagation) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOpsPass (iree-codegen-bubble-up-ordinal-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After BubbleUpOrdinalOpsPass (iree-codegen-bubble-up-ordinal-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatchesPass (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After BufferizeCopyOnlyDispatchesPass (iree-codegen-bufferize-copy-only-dispatches) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After DecomposeSoftmaxPass (iree-codegen-decompose-softmax) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %4 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>> -> tensor<4x16x96xf16>
  %5 = iree_tensor_ext.dispatch.tensor.load %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>> -> tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16xi32>
  %7 = tensor.empty() : tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%6 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9 = linalg.fill ins(%cst : f16) outs(%7 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%4, %5 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%9, %8 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %11 = arith.cmpf ogt, %in, %out : f16
    %12 = arith.select %11, %in, %out : f16
    %13 = arith.select %11, %in_0, %out_1 : i32
    linalg.yield %12, %13 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_tensor_ext.dispatch.tensor.store %10#0, %2, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  iree_tensor_ext.dispatch.tensor.store %10#1, %3, offsets = [0, 0], sizes = [4, 16], strides = [1, 1] : tensor<4x16xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  return
}

// -----// IR Dump After DecomposeSoftmaxPass (iree-codegen-decompose-softmax) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %3 = tensor.empty() : tensor<4x16x96xi32>
  %4 = tensor.empty() : tensor<4x16x96xf16>
  %5 = tensor.empty() : tensor<4x16xi32>
  %6 = tensor.empty() : tensor<4x16xf16>
  %7 = linalg.fill ins(%cst : f16) outs(%6 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %8 = linalg.fill ins(%c0_i32 : i32) outs(%5 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %9:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %4, %arg2 = %3) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %10 = arith.cmpi slt, %arg0, %c0 : index
    %11 = arith.subi %c-1, %arg0 : index
    %12 = arith.select %10, %11, %arg0 : index
    %13 = arith.divsi %12, %c1336 : index
    %14 = arith.subi %c-1, %13 : index
    %15 = arith.select %10, %14, %13 : index
    %16 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%7 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%8 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %17 = arith.muli %15, %c1336 : index
    %18:2 = iree_linalg_ext.arg_compare dimension(2) ins(%16 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%17 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %19 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %19 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %18#0 into %arg1[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %18#1 into %arg2[0, 0, %15] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_tensor_ext.dispatch.tensor.store %9#0, %1, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xf16> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  iree_tensor_ext.dispatch.tensor.store %9#1, %2, offsets = [0, 0, 0], sizes = [4, 16, 96], strides = [1, 1, 1] : tensor<4x16x96xi32> -> !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  return
}

// -----// IR Dump After BufferizeDispatchTensorLoadStorePass (iree-codegen-bufferize-dispatch-tensor-load-store) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xf16>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x96xi32>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %8 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xf16>>
  %9 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %10 = amdgpu.fat_raw_buffer_cast %9 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %11 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16xi32>>
  %12 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %13 = iree_codegen.load_from_buffer %4 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %14 = tensor.empty() : tensor<4x16xi32>
  %15 = tensor.empty() : tensor<4x16xf16>
  %16 = linalg.fill ins(%c0_i32 : i32) outs(%14 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %17 = linalg.fill ins(%cst : f16) outs(%15 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %18:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%12, %13 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%17, %16 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %19 = arith.cmpf ogt, %in, %out : f16
    %20 = arith.select %19, %in, %out : f16
    %21 = arith.select %19, %in_0, %out_1 : i32
    linalg.yield %20, %21 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %18#0, %7 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %18#1, %10 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After BufferizeDispatchTensorLoadStorePass (iree-codegen-bufferize-dispatch-tensor-load-store) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xf16>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<writeonly:tensor<4x16x96xi32>>
  %7 = tensor.empty() : tensor<4x16x96xi32>
  %8 = tensor.empty() : tensor<4x16x96xf16>
  %9 = tensor.empty() : tensor<4x16xi32>
  %10 = tensor.empty() : tensor<4x16xf16>
  %11 = linalg.fill ins(%cst : f16) outs(%10 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%9 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %8, %arg2 = %7) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %14 = arith.cmpi slt, %arg0, %c0 : index
    %15 = arith.subi %c-1, %arg0 : index
    %16 = arith.select %14, %15, %arg0 : index
    %17 = arith.divsi %16, %c1336 : index
    %18 = arith.subi %c-1, %17 : index
    %19 = arith.select %14, %18, %17 : index
    %20 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%11 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%12 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %21 = arith.muli %19, %c1336 : index
    %22:2 = iree_linalg_ext.arg_compare dimension(2) ins(%20 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%21 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %23 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %23 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %22#0 into %arg1[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %22#1 into %arg2[0, 0, %19] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %13#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %13#1, %5 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUCombineLayoutTransformationPass (iree-codegen-gpu-combine-layout-transformation) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUCombineLayoutTransformationPass (iree-codegen-gpu-combine-layout-transformation) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = tensor.empty() : tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16xi32>
  %8 = tensor.empty() : tensor<4x16xf16>
  %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %12 = arith.cmpi slt, %arg0, %c0 : index
    %13 = arith.subi %c-1, %arg0 : index
    %14 = arith.select %12, %13, %arg0 : index
    %15 = arith.divsi %14, %c1336 : index
    %16 = arith.subi %c-1, %15 : index
    %17 = arith.select %12, %16, %15 : index
    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %19 = arith.muli %17, %c1336 : index
    %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %21 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUGeneralizeNamedOpsPass (iree-codegen-gpu-generalize-named-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUGeneralizeNamedOpsPass (iree-codegen-gpu-generalize-named-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = tensor.empty() : tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16xi32>
  %8 = tensor.empty() : tensor<4x16xf16>
  %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %12 = arith.cmpi slt, %arg0, %c0 : index
    %13 = arith.subi %c-1, %arg0 : index
    %14 = arith.select %12, %13, %arg0 : index
    %15 = arith.divsi %14, %c1336 : index
    %16 = arith.subi %c-1, %15 : index
    %17 = arith.select %12, %16, %15 : index
    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %19 = arith.muli %17, %c1336 : index
    %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %21 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After BlockDynamicDimensionsPass (iree-codegen-block-dynamic-dimensions) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After BlockDynamicDimensionsPass (iree-codegen-block-dynamic-dimensions) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = tensor.empty() : tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16xi32>
  %8 = tensor.empty() : tensor<4x16xf16>
  %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %12 = arith.cmpi slt, %arg0, %c0 : index
    %13 = arith.subi %c-1, %arg0 : index
    %14 = arith.select %12, %13, %arg0 : index
    %15 = arith.divsi %14, %c1336 : index
    %16 = arith.subi %c-1, %15 : index
    %17 = arith.select %12, %16, %15 : index
    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %19 = arith.muli %17, %c1336 : index
    %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %21 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = tensor.empty() : tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16xi32>
  %8 = tensor.empty() : tensor<4x16xf16>
  %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %12 = arith.cmpi slt, %arg0, %c0 : index
    %13 = arith.subi %c-1, %arg0 : index
    %14 = arith.select %12, %13, %arg0 : index
    %15 = arith.divsi %14, %c1336 : index
    %16 = arith.subi %c-1, %15 : index
    %17 = arith.select %12, %16, %15 : index
    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %19 = arith.muli %17, %c1336 : index
    %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %21 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
  %c0_i32 = arith.constant 0 : i32
  %c0 = arith.constant 0 : index
  %cst = arith.constant 0xFC00 : f16
  %c-1 = arith.constant -1 : index
  %c1336 = arith.constant 1336 : index
  %c12288 = arith.constant 12288 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = tensor.empty() : tensor<4x16x96xi32>
  %6 = tensor.empty() : tensor<4x16x96xf16>
  %7 = tensor.empty() : tensor<4x16xi32>
  %8 = tensor.empty() : tensor<4x16xf16>
  %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
    %12 = arith.cmpi slt, %arg0, %c0 : index
    %13 = arith.subi %c-1, %arg0 : index
    %14 = arith.select %12, %13, %arg0 : index
    %15 = arith.divsi %14, %c1336 : index
    %16 = arith.subi %c-1, %15 : index
    %17 = arith.select %12, %16, %15 : index
    %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
    %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
    %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
    %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
    %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
    %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
    %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
    %19 = arith.muli %17, %c1336 : index
    %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
    ^bb0(%arg3: f16, %arg4: f16):
      %21 = arith.cmpf ogt, %arg3, %arg4 : f16
      iree_linalg_ext.yield %21 : i1
    } -> tensor<4x16xf16>, tensor<4x16xi32>
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
      tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
    }
  } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
  iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After MaterializeTuningSpecsPass (iree-codegen-materialize-tuning-specs) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %c0 = arith.constant 0 : index
    %c12288 = arith.constant 12288 : index
    %c128 = arith.constant 128 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
    %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
    %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %15 = arith.cmpf ogt, %in, %out : f16
      %16 = arith.select %15, %in, %out : f16
      %17 = arith.select %15, %in_0, %out_1 : i32
      linalg.yield %16, %17 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After MaterializeUserConfigsPass (iree-codegen-materialize-user-configs) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %c0 = arith.constant 0 : index
    %c12288 = arith.constant 12288 : index
    %c128 = arith.constant 128 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
    %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
    %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %15 = arith.cmpf ogt, %in, %out : f16
      %16 = arith.select %15, %in, %out : f16
      %17 = arith.select %15, %in_0, %out_1 : i32
      linalg.yield %16, %17 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After MaterializeTuningSpecsPass (iree-codegen-materialize-tuning-specs) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0xFC00 : f16
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %c12288 = arith.constant 12288 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg0, %c0 : index
      %13 = arith.subi %c-1, %arg0 : index
      %14 = arith.select %12, %13, %arg0 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg3: f16, %arg4: f16):
        %21 = arith.cmpf ogt, %arg3, %arg4 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After LLVMGPUSelectLoweringStrategyPass (iree-llvmgpu-select-lowering-strategy) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %c0 = arith.constant 0 : index
    %c12288 = arith.constant 12288 : index
    %c128 = arith.constant 128 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
    %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
    %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %15 = arith.cmpf ogt, %in, %out : f16
      %16 = arith.select %15, %in, %out : f16
      %17 = arith.select %15, %in_0, %out_1 : i32
      linalg.yield %16, %17 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After MaterializeUserConfigsPass (iree-codegen-materialize-user-configs) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0xFC00 : f16
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %c12288 = arith.constant 12288 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg0, %c0 : index
      %13 = arith.subi %c-1, %arg0 : index
      %14 = arith.select %12, %13, %arg0 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2] 
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2] 
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg3: f16, %arg4: f16):
        %21 = arith.cmpf ogt, %arg3, %arg4 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After LLVMGPUSelectLoweringStrategyPass (iree-llvmgpu-select-lowering-strategy) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0xFC00 : f16
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %c12288 = arith.constant 12288 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg0, %c0 : index
      %13 = arith.subi %c-1, %arg0 : index
      %14 = arith.select %12, %13, %arg0 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg3: f16, %arg4: f16):
        %21 = arith.cmpf ogt, %arg3, %arg4 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After ConfigureTargetExecutableVariantsPass (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0xFC00 : f16
      %c0 = arith.constant 0 : index
      %c12288 = arith.constant 12288 : index
      %c128 = arith.constant 128 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
      %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
      %10 = tensor.empty() : tensor<4x16xi32>
      %11 = tensor.empty() : tensor<4x16xf16>
      %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
        %15 = arith.cmpf ogt, %in, %out : f16
        %16 = arith.select %15, %in, %out : f16
        %17 = arith.select %15, %in_0, %out_1 : i32
        linalg.yield %16, %17 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      return
    }
  }
}

// -----// IR Dump After ConfigureExecutablesPass (iree-hal-configure-executables) //----- //
hal.executable private @test_simple_argmax_4x16_dispatch_1 {
  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
    hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      hal.return %x, %y, %z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
        %c0_i32 = arith.constant 0 : i32
        %cst = arith.constant 0xFC00 : f16
        %c0 = arith.constant 0 : index
        %c12288 = arith.constant 12288 : index
        %c128 = arith.constant 128 : index
        %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
        %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
        %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
        %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
        %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
        %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
        %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
        %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
        %10 = tensor.empty() : tensor<4x16xi32>
        %11 = tensor.empty() : tensor<4x16xf16>
        %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
          %15 = arith.cmpf ogt, %in, %out : f16
          %16 = arith.select %15, %in, %out : f16
          %17 = arith.select %15, %in_0, %out_1 : i32
          linalg.yield %16, %17 : f16, i32
        } -> (tensor<4x16xf16>, tensor<4x16xi32>)
        iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
        iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
        return
      }
    }
  }
}

// -----// IR Dump After ConfigureTargetExecutableVariantsPass (iree-hal-configure-target-executable-variants) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    hal.return %result_x, %result_y, %result_z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0xFC00 : f16
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %c12288 = arith.constant 12288 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg0, %c0 : index
        %13 = arith.subi %c-1, %arg0 : index
        %14 = arith.select %12, %13, %arg0 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg3: f16, %arg4: f16):
          %21 = arith.cmpf ogt, %arg3, %arg4 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      return
    }
  }
}

// -----// IR Dump After ConfigureExecutablesPass (iree-hal-configure-executables) //----- //
hal.executable private @test_simple_argmax_4x16_dispatch_0 {
  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
    hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
      %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
      %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
      hal.return %result_x, %result_y, %result_z : index, index, index
    }
    builtin.module {
      func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
        %c0_i32 = arith.constant 0 : i32
        %c0 = arith.constant 0 : index
        %cst = arith.constant 0xFC00 : f16
        %c-1 = arith.constant -1 : index
        %c1336 = arith.constant 1336 : index
        %c12288 = arith.constant 12288 : index
        %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
        %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
        %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
        %5 = tensor.empty() : tensor<4x16x96xi32>
        %6 = tensor.empty() : tensor<4x16x96xf16>
        %7 = tensor.empty() : tensor<4x16xi32>
        %8 = tensor.empty() : tensor<4x16xf16>
        %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
        %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
        %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
          %12 = arith.cmpi slt, %arg0, %c0 : index
          %13 = arith.subi %c-1, %arg0 : index
          %14 = arith.select %12, %13, %arg0 : index
          %15 = arith.divsi %14, %c1336 : index
          %16 = arith.subi %c-1, %15 : index
          %17 = arith.select %12, %16, %15 : index
          %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
          %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
          %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
          %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
          %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
          %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
          %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
          %19 = arith.muli %17, %c1336 : index
          %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
          ^bb0(%arg3: f16, %arg4: f16):
            %21 = arith.cmpf ogt, %arg3, %arg4 : f16
            iree_linalg_ext.yield %21 : i1
          } -> tensor<4x16xf16>, tensor<4x16xi32>
          scf.forall.in_parallel {
            tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
            tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
          }
        } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
        iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
        iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
        return
      }
    }
  }
}

// -----// IR Dump After HoistExecutableObjectsPass (iree-hal-hoist-executable-objects) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
      %c0_i32 = arith.constant 0 : i32
      %cst = arith.constant 0xFC00 : f16
      %c0 = arith.constant 0 : index
      %c12288 = arith.constant 12288 : index
      %c128 = arith.constant 128 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
      %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
      %10 = tensor.empty() : tensor<4x16xi32>
      %11 = tensor.empty() : tensor<4x16xf16>
      %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
        %15 = arith.cmpf ogt, %in, %out : f16
        %16 = arith.select %15, %in, %out : f16
        %17 = arith.select %15, %in_0, %out_1 : i32
        linalg.yield %16, %17 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      return
    }
  }
}

// -----// IR Dump After LowerExecutableUsingTransformDialectPass (iree-codegen-lower-executable-using-transform-dialect) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
    %c0_i32 = arith.constant 0 : i32
    %cst = arith.constant 0xFC00 : f16
    %c0 = arith.constant 0 : index
    %c12288 = arith.constant 12288 : index
    %c128 = arith.constant 128 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
    %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
    %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
    %10 = tensor.empty() : tensor<4x16xi32>
    %11 = tensor.empty() : tensor<4x16xf16>
    %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %15 = arith.cmpf ogt, %in, %out : f16
      %16 = arith.select %15, %in, %out : f16
      %17 = arith.select %15, %in_0, %out_1 : i32
      linalg.yield %16, %17 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After HoistExecutableObjectsPass (iree-hal-hoist-executable-objects) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    %result_x, %result_y, %result_z = iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier workgroups(%x, %y, %z) workload()
    hal.return %result_x, %result_y, %result_z : index, index, index
  }
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
      %c0_i32 = arith.constant 0 : i32
      %c0 = arith.constant 0 : index
      %cst = arith.constant 0xFC00 : f16
      %c-1 = arith.constant -1 : index
      %c1336 = arith.constant 1336 : index
      %c12288 = arith.constant 12288 : index
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
      %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %5 = tensor.empty() : tensor<4x16x96xi32>
      %6 = tensor.empty() : tensor<4x16x96xf16>
      %7 = tensor.empty() : tensor<4x16xi32>
      %8 = tensor.empty() : tensor<4x16xf16>
      %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
      %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
      %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
        %12 = arith.cmpi slt, %arg0, %c0 : index
        %13 = arith.subi %c-1, %arg0 : index
        %14 = arith.select %12, %13, %arg0 : index
        %15 = arith.divsi %14, %c1336 : index
        %16 = arith.subi %c-1, %15 : index
        %17 = arith.select %12, %16, %15 : index
        %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
        %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
        %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
        %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
        %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
        %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
        %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
        %19 = arith.muli %17, %c1336 : index
        %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
        ^bb0(%arg3: f16, %arg4: f16):
          %21 = arith.cmpf ogt, %arg3, %arg4 : f16
          iree_linalg_ext.yield %21 : i1
        } -> tensor<4x16xf16>, tensor<4x16xi32>
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
          tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
        }
      } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
      iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      return
    }
  }
}

// -----// IR Dump After LowerExecutableUsingTransformDialectPass (iree-codegen-lower-executable-using-transform-dialect) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} {
    %c0_i32 = arith.constant 0 : i32
    %c0 = arith.constant 0 : index
    %cst = arith.constant 0xFC00 : f16
    %c-1 = arith.constant -1 : index
    %c1336 = arith.constant 1336 : index
    %c12288 = arith.constant 12288 : index
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
    %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
    %2 = amdgpu.fat_raw_buffer_cast %1 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c12288) flags(Indirect) : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
    %4 = amdgpu.fat_raw_buffer_cast %3 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %5 = tensor.empty() : tensor<4x16x96xi32>
    %6 = tensor.empty() : tensor<4x16x96xf16>
    %7 = tensor.empty() : tensor<4x16xi32>
    %8 = tensor.empty() : tensor<4x16xf16>
    %9 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%cst : f16) outs(%8 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %10 = linalg.fill {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} ins(%c0_i32 : i32) outs(%7 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %11:2 = scf.forall (%arg0) = (0) to (128256) step (1336) shared_outs(%arg1 = %6, %arg2 = %5) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>) {
      %12 = arith.cmpi slt, %arg0, %c0 : index
      %13 = arith.subi %c-1, %arg0 : index
      %14 = arith.select %12, %13, %arg0 : index
      %15 = arith.divsi %14, %c1336 : index
      %16 = arith.subi %c-1, %15 : index
      %17 = arith.select %12, %16, %15 : index
      %18 = iree_tensor_ext.dispatch.tensor.load %0, offsets = [0, 0, %arg0], sizes = [4, 16, 1336], strides = [1, 1, 1] : !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>> -> tensor<4x16x1336xf16>
      %extracted_slice = tensor.extract_slice %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x1xf16>
      %broadcasted = linalg.broadcast ins(%9 : tensor<4x16xf16>) outs(%extracted_slice : tensor<4x16x1xf16>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
      %extracted_slice_0 = tensor.extract_slice %broadcasted[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xf16> to tensor<4x16xf16>
      %extracted_slice_1 = tensor.extract_slice %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x1xi32>
      %broadcasted_2 = linalg.broadcast ins(%10 : tensor<4x16xi32>) outs(%extracted_slice_1 : tensor<4x16x1xi32>) dimensions = [2]  {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>}
      %extracted_slice_3 = tensor.extract_slice %broadcasted_2[0, 0, 0] [4, 16, 1] [1, 1, 1] : tensor<4x16x1xi32> to tensor<4x16xi32>
      %19 = arith.muli %17, %c1336 : index
      %20:2 = iree_linalg_ext.arg_compare {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} dimension(2) ins(%18 : tensor<4x16x1336xf16>) outs(%extracted_slice_0, %extracted_slice_3 : tensor<4x16xf16>, tensor<4x16xi32>) index_base(%19 : index) {
      ^bb0(%arg3: f16, %arg4: f16):
        %21 = arith.cmpf ogt, %arg3, %arg4 : f16
        iree_linalg_ext.yield %21 : i1
      } -> tensor<4x16xf16>, tensor<4x16xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %20#0 into %arg1[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xf16> into tensor<4x16x96xf16>
        tensor.parallel_insert_slice %20#1 into %arg2[0, 0, %17] [4, 16, 1] [1, 1, 1] : tensor<4x16xi32> into tensor<4x16x96xi32>
      }
    } {mapping = [#iree_linalg_ext.split_reduction_mapping<0>]}
    iree_codegen.store_to_buffer %11#0, %2 : tensor<4x16x96xf16> into memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    iree_codegen.store_to_buffer %11#1, %4 : tensor<4x16x96xi32> into memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    return
  }
}

// -----// IR Dump After GPUPadConvsPass (iree-codegen-gpu-pad-convs) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConvertAccGEMMToGEMMPass (iree-convert-accgemm-to-gemm) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12 = linalg.fill ins(%c0_i32 : i32) outs(%10 : tensor<4x16xi32>) -> tensor<4x16xi32>
  %13 = linalg.fill ins(%cst : f16) outs(%11 : tensor<4x16xf16>) -> tensor<4x16xf16>
  %14:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %12 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
  ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
    %15 = arith.cmpf ogt, %in, %out : f16
    %16 = arith.select %15, %in, %out : f16
    %17 = arith.select %15, %in_0, %out_1 : i32
    linalg.yield %16, %17 : f16, i32
  } -> (tensor<4x16xf16>, tensor<4x16xi32>)
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After TileAndDistributeToWorkgroupsUsingForallOpPass Failed (iree-codegen-tile-and-distribute-to-workgroups-using-forall-op) //----- //
"func.func"() <{function_type = () -> (), sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 0xFC00 : f16}> : () -> f16
  %4 = "arith.constant"() <{value = -1 : index}> : () -> index
  %5 = "arith.constant"() <{value = 1336 : index}> : () -> index
  %6 = "arith.constant"() <{value = 12288 : index}> : () -> index
  %7 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> {iree_gpu.use_rocdl_buffer_instructions} : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %8 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %9 = "amdgpu.fat_raw_buffer_cast"(%8) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %10 = "hal.interface.binding.subspan"(%6) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %11 = "amdgpu.fat_raw_buffer_cast"(%10) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %12 = "tensor.empty"() : () -> tensor<4x16x96xi32>
  %13 = "tensor.empty"() : () -> tensor<4x16x96xf16>
  %14:2 = "scf.forall"(%13, %12) <{mapping = [#iree_linalg_ext.split_reduction_mapping<0>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0>, staticStep = array<i64: 1336>, staticUpperBound = array<i64: 128256>}> ({
  ^bb0(%arg0: index, %arg1: tensor<4x16x96xf16>, %arg2: tensor<4x16x96xi32>):
    %15 = "arith.cmpi"(%arg0, %2) <{predicate = 2 : i64}> : (index, index) -> i1
    %16 = "arith.subi"(%4, %arg0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %17 = "arith.select"(%15, %16, %arg0) : (i1, index, index) -> index
    %18 = "arith.divsi"(%17, %5) : (index, index) -> index
    %19 = "arith.subi"(%4, %18) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %20 = "arith.select"(%15, %19, %18) : (i1, index, index) -> index
    %21 = "iree_tensor_ext.dispatch.tensor.load"(%7, %arg0) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, index) -> tensor<4x16x1336xf16>
    %22 = "tensor.extract_slice"(%arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xf16>, index) -> tensor<4x16x1xf16>
    %23 = "tensor.extract_slice"(%arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xi32>, index) -> tensor<4x16x1xi32>
    %24 = "arith.muli"(%20, %5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %25:2 = "scf.forall"(%22, %23) <{mapping = [#iree_codegen.workgroup_mapping<y>, #iree_codegen.workgroup_mapping<x>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0, 0>, staticStep = array<i64: 1, 128>, staticUpperBound = array<i64: 4, 16>}> ({
    ^bb0(%arg3: index, %arg4: index, %arg5: tensor<4x16x1xf16>, %arg6: tensor<4x16x1xi32>):
      %26 = "tensor.extract_slice"(%21, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1336xf16>, index) -> tensor<1x16x1336xf16>
      %27 = "tensor.empty"() : () -> tensor<1x16xf16>
      %28 = "linalg.fill"(%3, %27) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg15: f16, %arg16: f16):
        "linalg.yield"(%arg15) : (f16) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (f16, tensor<1x16xf16>) -> tensor<1x16xf16>
      %29 = "tensor.extract_slice"(%arg5, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, index) -> tensor<1x16x1xf16>
      %30 = "linalg.broadcast"(%28, %29) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg13: f16, %arg14: f16):
        "linalg.yield"(%arg13) : (f16) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xf16>, tensor<1x16x1xf16>) -> tensor<1x16x1xf16>
      %31 = "tensor.extract_slice"(%30) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xf16>) -> tensor<16x1xf16>
      %32 = "tensor.empty"() : () -> tensor<1x16xi32>
      %33 = "linalg.fill"(%1, %32) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg11: i32, %arg12: i32):
        "linalg.yield"(%arg11) : (i32) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (i32, tensor<1x16xi32>) -> tensor<1x16xi32>
      %34 = "tensor.extract_slice"(%arg6, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, index) -> tensor<1x16x1xi32>
      %35 = "linalg.broadcast"(%33, %34) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg9: i32, %arg10: i32):
        "linalg.yield"(%arg9) : (i32) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xi32>, tensor<1x16x1xi32>) -> tensor<1x16x1xi32>
      %36 = "tensor.extract_slice"(%35) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xi32>) -> tensor<16x1xi32>
      %37:2 = "iree_linalg_ext.arg_compare"(%26, %31, %36, %24) <{dimension = 2 : i64, operandSegmentSizes = array<i32: 1, 2, 1>}> ({
      ^bb0(%arg7: f16, %arg8: f16):
        %40 = "arith.cmpf"(%arg7, %arg8) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f16, f16) -> i1
        "iree_linalg_ext.yield"(%40) : (i1) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16x1336xf16>, tensor<16x1xf16>, tensor<16x1xi32>, index) -> (tensor<16x1xf16>, tensor<16x1xi32>)
      %38 = "tensor.cast"(%37#0) : (tensor<16x1xf16>) -> tensor<1x?xf16>
      %39 = "tensor.cast"(%37#1) : (tensor<16x1xi32>) -> tensor<1x?xi32>
      "scf.forall.in_parallel"() ({
        "tensor.parallel_insert_slice"(%38, %arg5, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xf16>, tensor<4x16x1xf16>, index, index, index) -> ()
        "tensor.parallel_insert_slice"(%39, %arg6, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xi32>, tensor<4x16x1xi32>, index, index, index) -> ()
      }) : () -> ()
    }) : (tensor<4x16x1xf16>, tensor<4x16x1xi32>) -> (tensor<4x16x1xf16>, tensor<4x16x1xi32>)
    "scf.forall.in_parallel"() ({
      "tensor.parallel_insert_slice"(%25#0, %arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, tensor<4x16x96xf16>, index) -> ()
      "tensor.parallel_insert_slice"(%25#1, %arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, tensor<4x16x96xi32>, index) -> ()
    }) : () -> ()
  }) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  "iree_codegen.store_to_buffer"(%14#0, %9) : (tensor<4x16x96xf16>, memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>) -> ()
  "iree_codegen.store_to_buffer"(%14#1, %11) : (tensor<4x16x96xi32>, memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>) -> ()
  "func.return"() : () -> ()
}) {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} : () -> ()

// -----// IR Dump After TileAndDistributeToWorkgroupsUsingForallOpPass (iree-codegen-tile-and-distribute-to-workgroups-using-forall-op) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[%c0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[%c0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After LLVMGPULowerExecutableTargetPass Failed (iree-llvmgpu-lower-executable-target) //----- //
"func.func"() <{function_type = () -> (), sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
  %0 = "arith.constant"() <{value = 16 : index}> : () -> index
  %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 0xFC00 : f16}> : () -> f16
  %4 = "arith.constant"() <{value = -1 : index}> : () -> index
  %5 = "arith.constant"() <{value = 1336 : index}> : () -> index
  %6 = "arith.constant"() <{value = 12288 : index}> : () -> index
  %7 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> {iree_gpu.use_rocdl_buffer_instructions} : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
  %8 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %9 = "amdgpu.fat_raw_buffer_cast"(%8) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %10 = "hal.interface.binding.subspan"(%6) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %11 = "amdgpu.fat_raw_buffer_cast"(%10) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %12 = "tensor.empty"() : () -> tensor<4x16x96xi32>
  %13 = "tensor.empty"() : () -> tensor<4x16x96xf16>
  %14:2 = "scf.forall"(%13, %12) <{mapping = [#iree_linalg_ext.split_reduction_mapping<0>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0>, staticStep = array<i64: 1336>, staticUpperBound = array<i64: 128256>}> ({
  ^bb0(%arg0: index, %arg1: tensor<4x16x96xf16>, %arg2: tensor<4x16x96xi32>):
    %15 = "arith.cmpi"(%arg0, %2) <{predicate = 2 : i64}> : (index, index) -> i1
    %16 = "arith.subi"(%4, %arg0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %17 = "arith.select"(%15, %16, %arg0) : (i1, index, index) -> index
    %18 = "arith.divsi"(%17, %5) : (index, index) -> index
    %19 = "arith.subi"(%4, %18) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %20 = "arith.select"(%15, %19, %18) : (i1, index, index) -> index
    %21 = "iree_tensor_ext.dispatch.tensor.load"(%7, %arg0) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, index) -> tensor<4x16x1336xf16>
    %22 = "tensor.extract_slice"(%arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xf16>, index) -> tensor<4x16x1xf16>
    %23 = "tensor.extract_slice"(%arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xi32>, index) -> tensor<4x16x1xi32>
    %24 = "arith.muli"(%20, %5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
    %25:2 = "scf.forall"(%22, %23) <{mapping = [#iree_codegen.workgroup_mapping<y>, #iree_codegen.workgroup_mapping<x>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0, 0>, staticStep = array<i64: 1, 128>, staticUpperBound = array<i64: 4, 16>}> ({
    ^bb0(%arg3: index, %arg4: index, %arg5: tensor<4x16x1xf16>, %arg6: tensor<4x16x1xi32>):
      %26 = "tensor.extract_slice"(%21, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1336xf16>, index) -> tensor<1x16x1336xf16>
      %27 = "tensor.empty"() : () -> tensor<1x16xf16>
      %28 = "linalg.fill"(%3, %27) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg15: f16, %arg16: f16):
        "linalg.yield"(%arg15) : (f16) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (f16, tensor<1x16xf16>) -> tensor<1x16xf16>
      %29 = "tensor.extract_slice"(%arg5, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, index) -> tensor<1x16x1xf16>
      %30 = "linalg.broadcast"(%28, %29) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg13: f16, %arg14: f16):
        "linalg.yield"(%arg13) : (f16) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xf16>, tensor<1x16x1xf16>) -> tensor<1x16x1xf16>
      %31 = "tensor.extract_slice"(%30) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xf16>) -> tensor<16x1xf16>
      %32 = "tensor.empty"() : () -> tensor<1x16xi32>
      %33 = "linalg.fill"(%1, %32) <{operandSegmentSizes = array<i32: 1, 1>}> ({
      ^bb0(%arg11: i32, %arg12: i32):
        "linalg.yield"(%arg11) : (i32) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (i32, tensor<1x16xi32>) -> tensor<1x16xi32>
      %34 = "tensor.extract_slice"(%arg6, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, index) -> tensor<1x16x1xi32>
      %35 = "linalg.broadcast"(%33, %34) <{dimensions = array<i64: 2>}> ({
      ^bb0(%arg9: i32, %arg10: i32):
        "linalg.yield"(%arg9) : (i32) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xi32>, tensor<1x16x1xi32>) -> tensor<1x16x1xi32>
      %36 = "tensor.extract_slice"(%35) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xi32>) -> tensor<16x1xi32>
      %37:2 = "iree_linalg_ext.arg_compare"(%26, %31, %36, %24) <{dimension = 2 : i64, operandSegmentSizes = array<i32: 1, 2, 1>}> ({
      ^bb0(%arg7: f16, %arg8: f16):
        %40 = "arith.cmpf"(%arg7, %arg8) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f16, f16) -> i1
        "iree_linalg_ext.yield"(%40) : (i1) -> ()
      }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16x1336xf16>, tensor<16x1xf16>, tensor<16x1xi32>, index) -> (tensor<16x1xf16>, tensor<16x1xi32>)
      %38 = "tensor.cast"(%37#0) : (tensor<16x1xf16>) -> tensor<1x?xf16>
      %39 = "tensor.cast"(%37#1) : (tensor<16x1xi32>) -> tensor<1x?xi32>
      "scf.forall.in_parallel"() ({
        "tensor.parallel_insert_slice"(%38, %arg5, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xf16>, tensor<4x16x1xf16>, index, index, index) -> ()
        "tensor.parallel_insert_slice"(%39, %arg6, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xi32>, tensor<4x16x1xi32>, index, index, index) -> ()
      }) : () -> ()
    }) : (tensor<4x16x1xf16>, tensor<4x16x1xi32>) -> (tensor<4x16x1xf16>, tensor<4x16x1xi32>)
    "scf.forall.in_parallel"() ({
      "tensor.parallel_insert_slice"(%25#0, %arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, tensor<4x16x96xf16>, index) -> ()
      "tensor.parallel_insert_slice"(%25#1, %arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, tensor<4x16x96xi32>, index) -> ()
    }) : () -> ()
  }) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
  "iree_codegen.store_to_buffer"(%14#0, %9) : (tensor<4x16x96xf16>, memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>) -> ()
  "iree_codegen.store_to_buffer"(%14#1, %11) : (tensor<4x16x96xi32>, memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>) -> ()
  "func.return"() : () -> ()
}) {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} : () -> ()

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After TranslateTargetExecutableVariantsPass Failed (iree-hal-translate-target-executable-variants) //----- //
"hal.executable.variant"() <{sym_name = "rocm_hsaco_fb", target = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>}> ({
  "hal.executable.export"() <{layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, ordinal = 0 : index, sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
  ^bb0(%arg17: !hal.device):
    %41:3 = "iree_tensor_ext.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
    %42:3 = "iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier"(%41#0, %41#1, %41#2) : (index, index, index) -> (index, index, index)
    "hal.return"(%42#0, %42#1, %42#2) : (index, index, index) -> ()
  }, {
  }) : () -> ()
  "builtin.module"() ({
    "func.func"() <{function_type = () -> (), sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
      %0 = "arith.constant"() <{value = 16 : index}> : () -> index
      %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
      %2 = "arith.constant"() <{value = 0 : index}> : () -> index
      %3 = "arith.constant"() <{value = 0xFC00 : f16}> : () -> f16
      %4 = "arith.constant"() <{value = -1 : index}> : () -> index
      %5 = "arith.constant"() <{value = 1336 : index}> : () -> index
      %6 = "arith.constant"() <{value = 12288 : index}> : () -> index
      %7 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> {iree_gpu.use_rocdl_buffer_instructions} : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
      %8 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %9 = "amdgpu.fat_raw_buffer_cast"(%8) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %10 = "hal.interface.binding.subspan"(%6) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %11 = "amdgpu.fat_raw_buffer_cast"(%10) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = "tensor.empty"() : () -> tensor<4x16x96xi32>
      %13 = "tensor.empty"() : () -> tensor<4x16x96xf16>
      %14:2 = "scf.forall"(%13, %12) <{mapping = [#iree_linalg_ext.split_reduction_mapping<0>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0>, staticStep = array<i64: 1336>, staticUpperBound = array<i64: 128256>}> ({
      ^bb0(%arg0: index, %arg1: tensor<4x16x96xf16>, %arg2: tensor<4x16x96xi32>):
        %15 = "arith.cmpi"(%arg0, %2) <{predicate = 2 : i64}> : (index, index) -> i1
        %16 = "arith.subi"(%4, %arg0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %17 = "arith.select"(%15, %16, %arg0) : (i1, index, index) -> index
        %18 = "arith.divsi"(%17, %5) : (index, index) -> index
        %19 = "arith.subi"(%4, %18) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %20 = "arith.select"(%15, %19, %18) : (i1, index, index) -> index
        %21 = "iree_tensor_ext.dispatch.tensor.load"(%7, %arg0) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, index) -> tensor<4x16x1336xf16>
        %22 = "tensor.extract_slice"(%arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xf16>, index) -> tensor<4x16x1xf16>
        %23 = "tensor.extract_slice"(%arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xi32>, index) -> tensor<4x16x1xi32>
        %24 = "arith.muli"(%20, %5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
        %25:2 = "scf.forall"(%22, %23) <{mapping = [#iree_codegen.workgroup_mapping<y>, #iree_codegen.workgroup_mapping<x>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0, 0>, staticStep = array<i64: 1, 128>, staticUpperBound = array<i64: 4, 16>}> ({
        ^bb0(%arg3: index, %arg4: index, %arg5: tensor<4x16x1xf16>, %arg6: tensor<4x16x1xi32>):
          %26 = "tensor.extract_slice"(%21, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1336xf16>, index) -> tensor<1x16x1336xf16>
          %27 = "tensor.empty"() : () -> tensor<1x16xf16>
          %28 = "linalg.fill"(%3, %27) <{operandSegmentSizes = array<i32: 1, 1>}> ({
          ^bb0(%arg15: f16, %arg16: f16):
            "linalg.yield"(%arg15) : (f16) -> ()
          }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (f16, tensor<1x16xf16>) -> tensor<1x16xf16>
          %29 = "tensor.extract_slice"(%arg5, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, index) -> tensor<1x16x1xf16>
          %30 = "linalg.broadcast"(%28, %29) <{dimensions = array<i64: 2>}> ({
          ^bb0(%arg13: f16, %arg14: f16):
            "linalg.yield"(%arg13) : (f16) -> ()
          }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xf16>, tensor<1x16x1xf16>) -> tensor<1x16x1xf16>
          %31 = "tensor.extract_slice"(%30) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xf16>) -> tensor<16x1xf16>
          %32 = "tensor.empty"() : () -> tensor<1x16xi32>
          %33 = "linalg.fill"(%1, %32) <{operandSegmentSizes = array<i32: 1, 1>}> ({
          ^bb0(%arg11: i32, %arg12: i32):
            "linalg.yield"(%arg11) : (i32) -> ()
          }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (i32, tensor<1x16xi32>) -> tensor<1x16xi32>
          %34 = "tensor.extract_slice"(%arg6, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, index) -> tensor<1x16x1xi32>
          %35 = "linalg.broadcast"(%33, %34) <{dimensions = array<i64: 2>}> ({
          ^bb0(%arg9: i32, %arg10: i32):
            "linalg.yield"(%arg9) : (i32) -> ()
          }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xi32>, tensor<1x16x1xi32>) -> tensor<1x16x1xi32>
          %36 = "tensor.extract_slice"(%35) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xi32>) -> tensor<16x1xi32>
          %37:2 = "iree_linalg_ext.arg_compare"(%26, %31, %36, %24) <{dimension = 2 : i64, operandSegmentSizes = array<i32: 1, 2, 1>}> ({
          ^bb0(%arg7: f16, %arg8: f16):
            %40 = "arith.cmpf"(%arg7, %arg8) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f16, f16) -> i1
            "iree_linalg_ext.yield"(%40) : (i1) -> ()
          }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16x1336xf16>, tensor<16x1xf16>, tensor<16x1xi32>, index) -> (tensor<16x1xf16>, tensor<16x1xi32>)
          %38 = "tensor.cast"(%37#0) : (tensor<16x1xf16>) -> tensor<1x?xf16>
          %39 = "tensor.cast"(%37#1) : (tensor<16x1xi32>) -> tensor<1x?xi32>
          "scf.forall.in_parallel"() ({
            "tensor.parallel_insert_slice"(%38, %arg5, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xf16>, tensor<4x16x1xf16>, index, index, index) -> ()
            "tensor.parallel_insert_slice"(%39, %arg6, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xi32>, tensor<4x16x1xi32>, index, index, index) -> ()
          }) : () -> ()
        }) : (tensor<4x16x1xf16>, tensor<4x16x1xi32>) -> (tensor<4x16x1xf16>, tensor<4x16x1xi32>)
        "scf.forall.in_parallel"() ({
          "tensor.parallel_insert_slice"(%25#0, %arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, tensor<4x16x96xf16>, index) -> ()
          "tensor.parallel_insert_slice"(%25#1, %arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, tensor<4x16x96xi32>, index) -> ()
        }) : () -> ()
      }) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
      "iree_codegen.store_to_buffer"(%14#0, %9) : (tensor<4x16x96xf16>, memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>) -> ()
      "iree_codegen.store_to_buffer"(%14#1, %11) : (tensor<4x16x96xi32>, memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>) -> ()
      "func.return"() : () -> ()
    }) {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} : () -> ()
  }) : () -> ()
  "hal.executable.variant_end"() : () -> ()
}) : () -> ()

// -----// IR Dump After TranslateAllExecutablesPass Failed (iree-hal-translate-all-executables) //----- //
"hal.executable"() <{sym_name = "test_simple_argmax_4x16_dispatch_0", sym_visibility = "private"}> ({
  "hal.executable.variant"() <{sym_name = "rocm_hsaco_fb", target = #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>}> ({
    "hal.executable.export"() <{layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, ordinal = 0 : index, sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
    ^bb0(%arg17: !hal.device):
      %41:3 = "iree_tensor_ext.dispatch.workgroup_count_from_slice"() : () -> (index, index, index)
      %42:3 = "iree_tensor_ext.dispatch.workgroup_count_split_reduction_modifier"(%41#0, %41#1, %41#2) : (index, index, index) -> (index, index, index)
      "hal.return"(%42#0, %42#1, %42#2) : (index, index, index) -> ()
    }, {
    }) : () -> ()
    "builtin.module"() ({
      "func.func"() <{function_type = () -> (), sym_name = "test_simple_argmax_4x16_dispatch_0_arg_compare_4x16x1336xf16"}> ({
        %0 = "arith.constant"() <{value = 16 : index}> : () -> index
        %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32
        %2 = "arith.constant"() <{value = 0 : index}> : () -> index
        %3 = "arith.constant"() <{value = 0xFC00 : f16}> : () -> f16
        %4 = "arith.constant"() <{value = -1 : index}> : () -> index
        %5 = "arith.constant"() <{value = 1336 : index}> : () -> index
        %6 = "arith.constant"() <{value = 12288 : index}> : () -> index
        %7 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 0 : index, descriptor_flags = 3 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> {iree_gpu.use_rocdl_buffer_instructions} : (index) -> !iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>
        %8 = "hal.interface.binding.subspan"(%2) <{alignment = 64 : index, binding = 1 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
        %9 = "amdgpu.fat_raw_buffer_cast"(%8) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
        %10 = "hal.interface.binding.subspan"(%6) <{alignment = 64 : index, binding = 2 : index, descriptor_flags = 2 : i32, layout = #hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>, operandSegmentSizes = array<i32: 1, 0>}> : (index) -> memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
        %11 = "amdgpu.fat_raw_buffer_cast"(%10) <{boundsCheck = true, operandSegmentSizes = array<i32: 1, 0, 0>, resetOffset}> : (memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>) -> memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
        %12 = "tensor.empty"() : () -> tensor<4x16x96xi32>
        %13 = "tensor.empty"() : () -> tensor<4x16x96xf16>
        %14:2 = "scf.forall"(%13, %12) <{mapping = [#iree_linalg_ext.split_reduction_mapping<0>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0>, staticStep = array<i64: 1336>, staticUpperBound = array<i64: 128256>}> ({
        ^bb0(%arg0: index, %arg1: tensor<4x16x96xf16>, %arg2: tensor<4x16x96xi32>):
          %15 = "arith.cmpi"(%arg0, %2) <{predicate = 2 : i64}> : (index, index) -> i1
          %16 = "arith.subi"(%4, %arg0) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %17 = "arith.select"(%15, %16, %arg0) : (i1, index, index) -> index
          %18 = "arith.divsi"(%17, %5) : (index, index) -> index
          %19 = "arith.subi"(%4, %18) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %20 = "arith.select"(%15, %19, %18) : (i1, index, index) -> index
          %21 = "iree_tensor_ext.dispatch.tensor.load"(%7, %arg0) <{operandSegmentSizes = array<i32: 1, 0, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (!iree_tensor_ext.dispatch.tensor<readonly:tensor<4x16x128256xf16>>, index) -> tensor<4x16x1336xf16>
          %22 = "tensor.extract_slice"(%arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xf16>, index) -> tensor<4x16x1xf16>
          %23 = "tensor.extract_slice"(%arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x96xi32>, index) -> tensor<4x16x1xi32>
          %24 = "arith.muli"(%20, %5) <{overflowFlags = #arith.overflow<none>}> : (index, index) -> index
          %25:2 = "scf.forall"(%22, %23) <{mapping = [#iree_codegen.workgroup_mapping<y>, #iree_codegen.workgroup_mapping<x>], operandSegmentSizes = array<i32: 0, 0, 0, 2>, staticLowerBound = array<i64: 0, 0>, staticStep = array<i64: 1, 128>, staticUpperBound = array<i64: 4, 16>}> ({
          ^bb0(%arg3: index, %arg4: index, %arg5: tensor<4x16x1xf16>, %arg6: tensor<4x16x1xi32>):
            %26 = "tensor.extract_slice"(%21, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1336>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1336xf16>, index) -> tensor<1x16x1336xf16>
            %27 = "tensor.empty"() : () -> tensor<1x16xf16>
            %28 = "linalg.fill"(%3, %27) <{operandSegmentSizes = array<i32: 1, 1>}> ({
            ^bb0(%arg15: f16, %arg16: f16):
              "linalg.yield"(%arg15) : (f16) -> ()
            }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (f16, tensor<1x16xf16>) -> tensor<1x16xf16>
            %29 = "tensor.extract_slice"(%arg5, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, index) -> tensor<1x16x1xf16>
            %30 = "linalg.broadcast"(%28, %29) <{dimensions = array<i64: 2>}> ({
            ^bb0(%arg13: f16, %arg14: f16):
              "linalg.yield"(%arg13) : (f16) -> ()
            }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xf16>, tensor<1x16x1xf16>) -> tensor<1x16x1xf16>
            %31 = "tensor.extract_slice"(%30) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xf16>) -> tensor<16x1xf16>
            %32 = "tensor.empty"() : () -> tensor<1x16xi32>
            %33 = "linalg.fill"(%1, %32) <{operandSegmentSizes = array<i32: 1, 1>}> ({
            ^bb0(%arg11: i32, %arg12: i32):
              "linalg.yield"(%arg11) : (i32) -> ()
            }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (i32, tensor<1x16xi32>) -> tensor<1x16xi32>
            %34 = "tensor.extract_slice"(%arg6, %arg3) <{operandSegmentSizes = array<i32: 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, index) -> tensor<1x16x1xi32>
            %35 = "linalg.broadcast"(%33, %34) <{dimensions = array<i64: 2>}> ({
            ^bb0(%arg9: i32, %arg10: i32):
              "linalg.yield"(%arg9) : (i32) -> ()
            }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16xi32>, tensor<1x16x1xi32>) -> tensor<1x16x1xi32>
            %36 = "tensor.extract_slice"(%35) <{operandSegmentSizes = array<i32: 1, 0, 0, 0>, static_offsets = array<i64: 0, 0, 0>, static_sizes = array<i64: 1, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x16x1xi32>) -> tensor<16x1xi32>
            %37:2 = "iree_linalg_ext.arg_compare"(%26, %31, %36, %24) <{dimension = 2 : i64, operandSegmentSizes = array<i32: 1, 2, 1>}> ({
            ^bb0(%arg7: f16, %arg8: f16):
              %40 = "arith.cmpf"(%arg7, %arg8) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f16, f16) -> i1
              "iree_linalg_ext.yield"(%40) : (i1) -> ()
            }) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16x1336xf16>, tensor<16x1xf16>, tensor<16x1xi32>, index) -> (tensor<16x1xf16>, tensor<16x1xi32>)
            %38 = "tensor.cast"(%37#0) : (tensor<16x1xf16>) -> tensor<1x?xf16>
            %39 = "tensor.cast"(%37#1) : (tensor<16x1xi32>) -> tensor<1x?xi32>
            "scf.forall.in_parallel"() ({
              "tensor.parallel_insert_slice"(%38, %arg5, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xf16>, tensor<4x16x1xf16>, index, index, index) -> ()
              "tensor.parallel_insert_slice"(%39, %arg6, %arg3, %2, %0) <{operandSegmentSizes = array<i32: 1, 1, 2, 1, 0>, static_offsets = array<i64: -9223372036854775808, -9223372036854775808, 0>, static_sizes = array<i64: 1, -9223372036854775808, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<1x?xi32>, tensor<4x16x1xi32>, index, index, index) -> ()
            }) : () -> ()
          }) : (tensor<4x16x1xf16>, tensor<4x16x1xi32>) -> (tensor<4x16x1xf16>, tensor<4x16x1xi32>)
          "scf.forall.in_parallel"() ({
            "tensor.parallel_insert_slice"(%25#0, %arg1, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xf16>, tensor<4x16x96xf16>, index) -> ()
            "tensor.parallel_insert_slice"(%25#1, %arg2, %20) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: 0, 0, -9223372036854775808>, static_sizes = array<i64: 4, 16, 1>, static_strides = array<i64: 1, 1, 1>}> : (tensor<4x16x1xi32>, tensor<4x16x96xi32>, index) -> ()
          }) : () -> ()
        }) : (tensor<4x16x96xf16>, tensor<4x16x96xi32>) -> (tensor<4x16x96xf16>, tensor<4x16x96xi32>)
        "iree_codegen.store_to_buffer"(%14#0, %9) : (tensor<4x16x96xf16>, memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>) -> ()
        "iree_codegen.store_to_buffer"(%14#1, %11) : (tensor<4x16x96xi32>, memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>) -> ()
        "func.return"() : () -> ()
      }) {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUDistribute workgroup_size = [128, 1, 1] subgroup_size = 64>} : () -> ()
    }) : () -> ()
    "hal.executable.variant_end"() : () -> ()
  }) : () -> ()
  "hal.executable_end"() : () -> ()
}) : () -> ()

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After LowerTensorUKernelsPass (iree-codegen-lower-tensor-ukernels) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After LoweringConfigInterpreterPass (iree-codegen-lowering-config-interpreter) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUPadOperandsPass (iree-codegen-gpu-pad-operands) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUPromoteMatmulOperandsPass (iree-codegen-gpu-promote-matmul-operands) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUTileAndConvertConvToMatmulPass (iree-codegen-gpu-tile-and-convert-conv-to-matmul) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUPackToIntrinsicsPass (iree-codegen-gpu-pack-to-intrinsics) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After DecomposeBoundaryPackUnPackOpsPass (iree-codegen-decompose-boundary-pack-unpack-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ExpandUndistributedInnerTilesPass (iree-gpu-expand-undistributed-inner-tiles) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After PropagateReshapesByExpansionPass (iree-codegen-propagate-reshapes-by-expansion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%8, %9 : tensor<4x16x96xf16>, tensor<4x16x96xi32>) outs(%13, %14 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
    ^bb0(%in: f16, %in_0: i32, %out: f16, %out_1: i32):
      %16 = arith.cmpf ogt, %in, %out : f16
      %17 = arith.select %16, %in, %out : f16
      %18 = arith.select %16, %in_0, %out_1 : i32
      linalg.yield %17, %18 : f16, i32
    } -> (tensor<4x16xf16>, tensor<4x16xi32>)
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUApplyTilingLevelPass (iree-codegen-gpu-apply-tiling-level) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUConvertToCoalescedDMAPass (iree-codegen-gpu-convert-to-coalesced-dma) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After DecomposePackUnPackOpsPass (iree-codegen-decompose-pack-unpack-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ExpandUndistributedInnerTilesPass (iree-gpu-expand-undistributed-inner-tiles) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After PropagateReshapesByExpansionPass (iree-codegen-propagate-reshapes-by-expansion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %8[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<4x16x4xf16>
      %extracted_slice_0 = tensor.extract_slice %9[0, 0, %arg3] [4, 16, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<4x16x4xi32>
      %16:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<4x16x4xf16>, tensor<4x16x4xi32>) outs(%arg4, %arg5 : tensor<4x16xf16>, tensor<4x16xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_1: i32, %out: f16, %out_2: i32):
        %17 = arith.cmpf ogt, %in, %out : f16
        %18 = arith.select %17, %in, %out : f16
        %19 = arith.select %17, %in_1, %out_2 : i32
        linalg.yield %18, %19 : f16, i32
      } -> (tensor<4x16xf16>, tensor<4x16xi32>)
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUApplyTilingLevelPass (iree-codegen-gpu-apply-tiling-level) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUApplyTilingLevelPass (iree-codegen-gpu-apply-tiling-level) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After DistributeInnerTiledToLanesPass (iree-gpu-distribute-inner-tiled-to-lanes) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After LowerBitcodeUKernelsPass (iree-codegen-lower-bitcode-ukernels) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) = (0) to (4) step (4) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After NormalizeLoopBoundsPass (iree-codegen-normalize-loop-bounds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = affine.apply affine_map<(d0) -> (d0 * 4)>(%arg0)
    %14 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %15 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %16:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %14, %arg5 = %15) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %17:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %18:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %19 = arith.cmpf ogt, %in, %out : f16
          %20 = arith.select %19, %in, %out : f16
          %21 = arith.select %19, %in_3, %out_4 : i32
          linalg.yield %20, %21 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %18#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %18#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %17#0, %17#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %16#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %16#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ConfigTrackingCanonicalizerPass (iree-codegen-config-tracking-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUBubbleResourceCastsPass (iree-codegen-gpu-bubble-resource-casts) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_0 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_2 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After OptimizeTensorInsertExtractSlicesPass (iree-codegen-optimize-tensor-insert-extract-slices) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUFuseAndHoistParallelLoopsPass (iree-codegen-gpu-fuse-and-hoist-parallel-loops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CombineLayoutTransformationPass (iree-codegen-combine-layout-transformation) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = linalg.fill ins(%cst : f16) outs(%arg1 : tensor<4x16xf16>) -> tensor<4x16xf16>
    %14 = linalg.fill ins(%c0_i32 : i32) outs(%arg2 : tensor<4x16xi32>) -> tensor<4x16xi32>
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUGreedilyDistributeToThreadsPass (iree-codegen-gpu-greedily-distribute-to-threads) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After TileLargeTensorsPass (iree-codegen-tile-large-tensors) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %c0_3 = arith.constant 0 : index
        %c0_4 = arith.constant 0 : index
        %c1 = arith.constant 1 : index
        %c1_5 = arith.constant 1 : index
        %c1_6 = arith.constant 1 : index
        %c1_7 = arith.constant 1 : index
        %17:2 = scf.for %arg10 = %c0_3 to %c1 step %c1_6 iter_args(%arg11 = %extracted_slice_1, %arg12 = %extracted_slice_0) -> (tensor<1x1xf16>, tensor<1x1xi32>) {
          %18:2 = scf.for %arg13 = %c0_4 to %c1_5 step %c1_7 iter_args(%arg14 = %arg11, %arg15 = %arg12) -> (tensor<1x1xf16>, tensor<1x1xi32>) {
            %extracted_slice_8 = tensor.extract_slice %extracted_slice[%arg10, %arg13, 0] [1, 1, 4] [1, 1, 1] : tensor<1x1x4xf16> to tensor<1x1x4xf16>
            %extracted_slice_9 = tensor.extract_slice %extracted_slice_2[%arg10, %arg13, 0] [1, 1, 4] [1, 1, 1] : tensor<1x1x4xi32> to tensor<1x1x4xi32>
            %extracted_slice_10 = tensor.extract_slice %arg14[%arg10, %arg13] [1, 1] [1, 1] : tensor<1x1xf16> to tensor<1x1xf16>
            %extracted_slice_11 = tensor.extract_slice %arg15[%arg10, %arg13] [1, 1] [1, 1] : tensor<1x1xi32> to tensor<1x1xi32>
            %19:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice_8, %extracted_slice_9 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_10, %extracted_slice_11 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
            ^bb0(%in: f16, %in_13: i32, %out: f16, %out_14: i32):
              %20 = arith.cmpf ogt, %in, %out : f16
              %21 = arith.select %20, %in, %out : f16
              %22 = arith.select %20, %in_13, %out_14 : i32
              linalg.yield %21, %22 : f16, i32
            } -> (tensor<1x1xf16>, tensor<1x1xi32>)
            %inserted_slice = tensor.insert_slice %19#0 into %arg14[%arg10, %arg13] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<1x1xf16>
            %inserted_slice_12 = tensor.insert_slice %19#1 into %arg15[%arg10, %arg13] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<1x1xi32>
            scf.yield %inserted_slice, %inserted_slice_12 : tensor<1x1xf16>, tensor<1x1xi32>
          }
          scf.yield %18#0, %18#1 : tensor<1x1xf16>, tensor<1x1xi32>
        }
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After PropagateDispatchSizeBoundsPass (iree-codegen-propagate-dispatch-size-bounds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CombineBarrierRegionsPass (iree-gpu-combine-barrier-regions) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After VectorizeIREELinalgExtOpsPass (iree-linalg-ext-vectorize-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After VectorizeIREEGPUOpsPass (iree-gpu-vectorize-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After DecomposeConvolutionToLowerDimOpsPass (iree-codegen-decompose-convolution-to-lower-dim-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After DecomposeIm2colPass (iree-linalg-ext-decompose-im2col) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After VectorizeIREEVectorExtOpsPass (iree-vector-ext-vectorize-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant 0xFC00 : f16
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = linalg.fill ins(%cst : f16) outs(%extracted_slice : tensor<1x1xf16>) -> tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = linalg.fill ins(%c0_i32 : i32) outs(%extracted_slice : tensor<1x1xi32>) -> tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_0 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_1 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_2 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_2 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_1, %extracted_slice_0 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_3: i32, %out: f16, %out_4: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_3, %out_4 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GenericVectorizationPass (iree-codegen-generic-vectorization) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_3 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_1 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_3 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_1 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_3 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_1 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After OptimizeTensorInsertExtractSlicesPass (iree-codegen-optimize-tensor-insert-extract-slices) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After CleanupBufferAllocViewPass (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUCombineValueBarriersPass (iree-codegen-gpu-combine-value-barriers) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = tensor.empty() : tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After EliminateEmptyTensorsPass (iree-eliminate-empty-tensors) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = tensor.empty() : tensor<4x16xi32>
  %12 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %13 = tensor.empty() : tensor<4x16xf16>
  %14:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %12, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %15 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %18 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %18 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %16 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %18 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %18 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %17:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %15, %arg5 = %16) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %18:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %19:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %20 = arith.cmpf ogt, %in, %out : f16
          %21 = arith.select %20, %in, %out : f16
          %22 = arith.select %20, %in_4, %out_5 : i32
          linalg.yield %21, %22 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %19#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %19#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %18#0, %18#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %17#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %17#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %14#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %14#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After EmptyTensorToAllocTensorPass (empty-tensor-to-alloc-tensor) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUInferMemorySpacePass (iree-codegen-gpu-infer-memory-space) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ROCDLConfigureBufferInstructionsPass (iree-rocdl-configure-buffer-instructions) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUBubbleResourceCastsPass (iree-codegen-gpu-bubble-resource-casts) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPUAllocPrivateMemoryForDPSOpsPass (iree-codegen-gpu-alloc-private-memory-for-dps-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = iree_codegen.load_from_buffer %1 : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xf16>
  %9 = iree_codegen.load_from_buffer %3 : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16x96xi32>
  %10 = iree_codegen.load_from_buffer %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xi32>
  %11 = iree_codegen.load_from_buffer %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> -> tensor<4x16xf16>
  %12:2 = scf.forall (%arg0) in (1) shared_outs(%arg1 = %11, %arg2 = %10) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
    %13 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg1) -> (tensor<4x16xf16>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
      %16 = vector.transfer_write %cst_0, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, tensor<1x1xf16>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %14 = scf.forall (%arg3, %arg4) in (4, 16) shared_outs(%arg5 = %arg2) -> (tensor<4x16xi32>) {
      %extracted_slice = tensor.extract_slice %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
      %16 = vector.transfer_write %cst, %extracted_slice[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, tensor<1x1xi32>
      scf.forall.in_parallel {
        tensor.parallel_insert_slice %16 into %arg5[%arg3, %arg4] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
      }
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %15:2 = scf.for %arg3 = %c0 to %c96 step %c4 iter_args(%arg4 = %13, %arg5 = %14) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
      %16:2 = scf.forall (%arg6, %arg7) in (4, 16) shared_outs(%arg8 = %arg4, %arg9 = %arg5) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
        %extracted_slice = tensor.extract_slice %8[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xf16> to tensor<1x1x4xf16>
        %extracted_slice_1 = tensor.extract_slice %9[%arg6, %arg7, %arg3] [1, 1, 4] [1, 1, 1] : tensor<4x16x96xi32> to tensor<1x1x4xi32>
        %extracted_slice_2 = tensor.extract_slice %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xf16> to tensor<1x1xf16>
        %extracted_slice_3 = tensor.extract_slice %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<4x16xi32> to tensor<1x1xi32>
        %17:2 = linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%extracted_slice, %extracted_slice_1 : tensor<1x1x4xf16>, tensor<1x1x4xi32>) outs(%extracted_slice_2, %extracted_slice_3 : tensor<1x1xf16>, tensor<1x1xi32>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_4: i32, %out: f16, %out_5: i32):
          %18 = arith.cmpf ogt, %in, %out : f16
          %19 = arith.select %18, %in, %out : f16
          %20 = arith.select %18, %in_4, %out_5 : i32
          linalg.yield %19, %20 : f16, i32
        } -> (tensor<1x1xf16>, tensor<1x1xi32>)
        scf.forall.in_parallel {
          tensor.parallel_insert_slice %17#0 into %arg8[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xf16> into tensor<4x16xf16>
          tensor.parallel_insert_slice %17#1 into %arg9[%arg6, %arg7] [1, 1] [1, 1] : tensor<1x1xi32> into tensor<4x16xi32>
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %16#0, %16#1 : tensor<4x16xf16>, tensor<4x16xi32>
    }
    scf.forall.in_parallel {
      tensor.parallel_insert_slice %15#0 into %arg1[0, 0] [4, 16] [1, 1] : tensor<4x16xf16> into tensor<4x16xf16>
      tensor.parallel_insert_slice %15#1 into %arg2[0, 0] [4, 16] [1, 1] : tensor<4x16xi32> into tensor<4x16xi32>
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  iree_codegen.store_to_buffer %12#0, %5 : tensor<4x16xf16> into memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  iree_codegen.store_to_buffer %12#1, %7 : tensor<4x16xi32> into memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After IREEComprehensiveBufferizePass (iree-codegen-iree-comprehensive-bufferize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_2 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview_2[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_3 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_2, %subview_3 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_2 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview_2[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_3 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_2, %subview_3 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %8:2 = scf.for %arg1 = %c0 to %c96 step %c4 iter_args(%arg2 = %5, %arg3 = %7) -> (memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>) {
      scf.forall (%arg4, %arg5) in (4, 16) {
        %subview_2 = memref.subview %1[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_3 = memref.subview %3[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_2, %subview_3 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_4, %subview_5 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_8: i32, %out: f16, %out_9: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_8, %out_9 : i32
          linalg.yield %10, %11 : f16, i32
        }
        %subview_6 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_4, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_7 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_5, %subview_7 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %arg2, %arg3 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
    %subview = memref.subview %5[0, 0] [4, 16] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#0, %subview : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_1 = memref.subview %7[0, 0] [4, 16] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#1, %subview_1 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  memref.copy %5, %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.copy %7, %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After GPULowerToGlobalLoadsPass (iree-codegen-gpu-lower-to-global-loads) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %0 resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %2 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %4 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %6 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_2 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview_2[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_3 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_2, %subview_3 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_2 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview_2[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_3 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_2, %subview_3 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %8:2 = scf.for %arg1 = %c0 to %c96 step %c4 iter_args(%arg2 = %5, %arg3 = %7) -> (memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>) {
      scf.forall (%arg4, %arg5) in (4, 16) {
        %subview_2 = memref.subview %1[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_3 = memref.subview %3[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_2, %subview_3 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_4, %subview_5 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_8: i32, %out: f16, %out_9: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_8, %out_9 : i32
          linalg.yield %10, %11 : f16, i32
        }
        %subview_6 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_4, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_7 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_5, %subview_7 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %arg2, %arg3 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
    %subview = memref.subview %5[0, 0] [4, 16] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#0, %subview : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_1 = memref.subview %7[0, 0] [4, 16] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#1, %subview_1 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  memref.copy %5, %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.copy %7, %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After IREEInjectAssumeAlignmentPass (iree-codegen-inject-assume-alignment) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_5 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview_5[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_5 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview_5[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_5, %subview_6 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %8:2 = scf.for %arg1 = %c0 to %c96 step %c4 iter_args(%arg2 = %5, %arg3 = %7) -> (memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>) {
      scf.forall (%arg4, %arg5) in (4, 16) {
        %subview_5 = memref.subview %1[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %3[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_7 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_8 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_7, %subview_8 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_11: i32, %out: f16, %out_12: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_11, %out_12 : i32
          linalg.yield %10, %11 : f16, i32
        }
        %subview_9 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_7, %subview_9 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_10 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_8, %subview_10 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %arg2, %arg3 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
    %subview = memref.subview %5[0, 0] [4, 16] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#0, %subview : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[0, 0] [4, 16] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#1, %subview_4 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  memref.copy %5, %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.copy %7, %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After ResolveShapedTypeResultDimsPass (resolve-shaped-type-result-dims) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_5 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview_5[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview_5 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview_5[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview_5, %subview_6 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    %8:2 = scf.for %arg1 = %c0 to %c96 step %c4 iter_args(%arg2 = %5, %arg3 = %7) -> (memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>) {
      scf.forall (%arg4, %arg5) in (4, 16) {
        %subview_5 = memref.subview %1[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %3[%arg4, %arg5, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_7 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_8 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_7, %subview_8 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_11: i32, %out: f16, %out_12: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_11, %out_12 : i32
          linalg.yield %10, %11 : f16, i32
        }
        %subview_9 = memref.subview %arg2[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_7, %subview_9 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_10 = memref.subview %arg3[%arg4, %arg5] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_8, %subview_10 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
      scf.yield %arg2, %arg3 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
    %subview = memref.subview %5[0, 0] [4, 16] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#0, %subview : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[0, 0] [4, 16] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
    memref.copy %8#1, %subview_4 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, strided<[16, 1]>, #amdgpu.address_space<fat_raw_buffer>>
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  memref.copy %5, %5 : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.copy %7, %7 : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  return
}

// -----// IR Dump After IREECodegenCanonicalizerPass (iree-codegen-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_4 = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_4 = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview, %subview_4 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_9: i32, %out: f16, %out_10: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_9, %out_10 : i32
          linalg.yield %9, %10 : f16, i32
        }
        %subview_7 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_5, %subview_7 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_8 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_6, %subview_8 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview, %subview : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.copy %subview, %subview : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
        memref.copy %subview_5, %subview_5 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.copy %subview_6, %subview_6 : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After IREECodegenCanonicalizerPass (iree-codegen-canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CleanupBufferAllocViewPass (iree-codegen-cleanup-buffer-alloc-view) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After ROCDLBufferInstructionsOptimizationPass (iree-rocdl-buffer-instructions-optimization) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After AMDGPULowerCoalescedDMAToGatherLDSPass (iree-codegen-amdgpu-lower-coalesced-dma-to-gather-lds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After DecomposeMapScatterPass (iree-linalg-ext-decompose-map-scatter) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After GPUDistributeCopyUsingForallPass (iree-codegen-gpu-distribute-copy-using-forall) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After NormalizeLoopBoundsPass (iree-codegen-normalize-loop-bounds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After GPUVerifyDistributionPass (iree-codegen-gpu-verify-distribution) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %5[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.forall (%arg1, %arg2) in (4, 16) {
      %subview = memref.subview %7[%arg1, %arg2] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    scf.for %arg1 = %c0 to %c96 step %c4 {
      scf.forall (%arg2, %arg3) in (4, 16) {
        %subview = memref.subview %1[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %3[%arg2, %arg3, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %5[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %7[%arg2, %arg3] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %8 = arith.cmpf ogt, %in, %out : f16
          %9 = arith.select %8, %in, %out : f16
          %10 = arith.select %8, %in_7, %out_8 : i32
          linalg.yield %9, %10 : f16, i32
        }
      } {mapping = [#gpu.thread<linear_dim_1>, #gpu.thread<linear_dim_0>]}
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After GPUDistributeForallPass (iree-codegen-gpu-distribute-forall) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %thread_id_z = gpu.thread_id  z
  %thread_id_y = gpu.thread_id  y
  %thread_id_x = gpu.thread_id  x
  %0 = affine.linearize_index disjoint [%thread_id_z, %thread_id_y, %thread_id_x] by (1, 1, 64) : index
  %cst = arith.constant dense<0> : vector<1x1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1x1xf16>
  %c4 = arith.constant 4 : index
  %c96 = arith.constant 96 : index
  %c0 = arith.constant 0 : index
  %c12288 = arith.constant 12288 : index
  %c128 = arith.constant 128 : index
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %1, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %3, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %5, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %6 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %7 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %7, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %8 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    %c4_4 = arith.constant 4 : index
    %c16 = arith.constant 16 : index
    %c0_5 = arith.constant 0 : index
    %c64 = arith.constant 64 : index
    %c64_6 = arith.constant 64 : index
    gpu.barrier
    scf.for %arg1 = %c0_5 to %c64 step %c64_6 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    %c4_7 = arith.constant 4 : index
    %c16_8 = arith.constant 16 : index
    %c0_9 = arith.constant 0 : index
    %c64_10 = arith.constant 64 : index
    %c64_11 = arith.constant 64 : index
    gpu.barrier
    scf.for %arg1 = %c0_9 to %c64_10 step %c64_11 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      %c4_12 = arith.constant 4 : index
      %c16_13 = arith.constant 16 : index
      %c0_14 = arith.constant 0 : index
      %c64_15 = arith.constant 64 : index
      %c64_16 = arith.constant 64 : index
      gpu.barrier
      scf.for %arg2 = %c0_14 to %c64_15 step %c64_16 {
        %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%0]
        %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
        %subview = memref.subview %2[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_17 = memref.subview %4[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_18 = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_19 = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_17 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_18, %subview_19 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_20: i32, %out: f16, %out_21: i32):
          %11 = arith.cmpf ogt, %in, %out : f16
          %12 = arith.select %11, %in, %out : f16
          %13 = arith.select %11, %in_20, %out_21 : i32
          linalg.yield %12, %13 : f16, i32
        }
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After VectorizeMemrefCopyPass (iree-codegen-vectorize-memref-copy) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_z = gpu.thread_id  z
  %thread_id_y = gpu.thread_id  y
  %thread_id_x = gpu.thread_id  x
  %0 = affine.linearize_index disjoint [%thread_id_z, %thread_id_y, %thread_id_x] by (1, 1, 64) : index
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %1, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %3, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %5, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %6 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %7 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %7, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %8 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    scf.for %arg1 = %c0 to %c64 step %c64 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    gpu.barrier
    scf.for %arg1 = %c0 to %c64 step %c64 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      scf.for %arg2 = %c0 to %c64 step %c64 {
        %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%0]
        %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
        %subview = memref.subview %2[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %4[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %11 = arith.cmpf ogt, %in, %out : f16
          %12 = arith.select %11, %in, %out : f16
          %13 = arith.select %11, %in_7, %out_8 : i32
          linalg.yield %12, %13 : f16, i32
        }
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After UnrollToIntrinsicsPass (iree-gpu-unroll-to-intrinsics) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c64 = arith.constant 64 : index
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_z = gpu.thread_id  z
  %thread_id_y = gpu.thread_id  y
  %thread_id_x = gpu.thread_id  x
  %0 = affine.linearize_index disjoint [%thread_id_z, %thread_id_y, %thread_id_x] by (1, 1, 64) : index
  %1 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %1, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %2 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %3 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %3, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %4 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %5 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %5, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %6 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %7 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %7, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %8 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    scf.for %arg1 = %c0 to %c64 step %c64 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    gpu.barrier
    scf.for %arg1 = %c0 to %c64 step %c64 {
      %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg1)[%0]
      %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
      %subview = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      vector.transfer_write %cst_0, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      scf.for %arg2 = %c0 to %c64 step %c64 {
        %9 = affine.apply affine_map<(d0)[s0] -> (d0 + s0)>(%arg2)[%0]
        %10:2 = affine.delinearize_index %9 into (4, 16) : index, index
        %subview = memref.subview %2[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_4 = memref.subview %4[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_5 = memref.subview %6[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %8[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview, %subview_4 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_5, %subview_6 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %11 = arith.cmpf ogt, %in, %out : f16
          %12 = arith.select %11, %in, %out : f16
          %13 = arith.select %11, %in_7, %out_8 : i32
          linalg.yield %12, %13 : f16, i32
        }
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %9:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview_4 = memref.subview %7[%9#0, %9#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %10:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
      %subview_5 = memref.subview %1[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%10#0, %10#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_7 = memref.subview %5[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_8 = memref.subview %7[%10#0, %10#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview_7, %subview_8 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_9: i32, %out: f16, %out_10: i32):
        %11 = arith.cmpf ogt, %in, %out : f16
        %12 = arith.select %11, %in, %out : f16
        %13 = arith.select %11, %in_9, %out_10 : i32
        linalg.yield %12, %13 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After PropagateDispatchSizeBoundsPass (iree-codegen-propagate-dispatch-size-bounds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After LowerIREEGPUOpsPass (iree-gpu-lower-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After UnrollAnnotatedLoopsPass (iree-codegen-unroll-annotated-loops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After GPUReduceBankConflictsPass (iree-codegen-gpu-reduce-bank-conflicts) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocationsPass (iree-codegen-hoist-statically-bound-allocations) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %subview[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xf16>, memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %subview_4[%c0, %c0] {in_bounds = [true, true]} : vector<1x1xi32>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After FoldMemRefAliasOpsPass (fold-memref-alias-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %5[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %7[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %5[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %7[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %cst = arith.constant dense<0xFC00> : vector<1x1xf16>
  %cst_0 = arith.constant dense<0> : vector<1x1xi32>
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst, %5[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    vector.transfer_write %cst_0, %7[%8#0, %8#1] {in_bounds = [true, true]} : vector<1x1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After OptimizeVectorTransferPass (iree-codegen-optimize-vector-transfer) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocationsPass (iree-codegen-hoist-statically-bound-allocations) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After LLVMGPULowerExecutableTargetPass (iree-llvmgpu-lower-executable-target) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After VerifyWorkgroupDistributionPass (iree-codegen-verify-workgroup-distribution) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() attributes {translation_info = #iree_codegen.translation_info<pipeline = LLVMGPUTileAndFuse workgroup_size = [64, 1, 1] subgroup_size = 64>} {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  scf.forall (%arg0) in (1) {
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg1 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg1] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) attrs =  {lowering_config = #iree_gpu.lowering_config<{reduction = [0, 0, 4], thread = [1, 1, 0], workgroup = [4, 16, 0]}>} {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
  } {mapping = [#iree_codegen.workgroup_mapping<x>]}
  return
}

// -----// IR Dump After ReconcileTranslationInfoPass (iree-codegen-reconcile-translation-info) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %x, %y, %z = iree_tensor_ext.dispatch.workgroup_count_from_slice()
    hal.return %x, %y, %z : index, index, index
  } attributes {subgroup_size = 64 : index, workgroup_size = [64 : index, 1 : index, 1 : index]}
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
      %cst = arith.constant dense<0> : vector<1xi32>
      %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
      %c128 = arith.constant 128 : index
      %c12288 = arith.constant 12288 : index
      %c0 = arith.constant 0 : index
      %c96 = arith.constant 96 : index
      %c4 = arith.constant 4 : index
      %thread_id_x = gpu.thread_id  x upper_bound 64
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
      %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      gpu.barrier
      vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      gpu.barrier
      vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      gpu.barrier
      scf.for %arg0 = %c0 to %c96 step %c4 {
        gpu.barrier
        %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_7, %out_8 : i32
          linalg.yield %10, %11 : f16, i32
        }
        gpu.barrier
      }
      iree_codegen.workgroup_count_hint(1)
      return
    }
  }
}

// -----// IR Dump After ResolveWorkgroupCountHintsPass (iree-codegen-resolve-workgroup-count-hints) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %c1 = arith.constant 1 : index
    %c1_0 = arith.constant 1 : index
    %c1_1 = arith.constant 1 : index
    hal.return %c1, %c1_0, %c1_1 : index, index, index
  } attributes {subgroup_size = 64 : index, workgroup_size = [64 : index, 1 : index, 1 : index]}
  builtin.module {
    func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
      %cst = arith.constant dense<0> : vector<1xi32>
      %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
      %c128 = arith.constant 128 : index
      %c12288 = arith.constant 12288 : index
      %c0 = arith.constant 0 : index
      %c96 = arith.constant 96 : index
      %c4 = arith.constant 4 : index
      %thread_id_x = gpu.thread_id  x upper_bound 64
      %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>>
      %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #hal.descriptor_type<storage_buffer>>
      %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #hal.descriptor_type<storage_buffer>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>>
      %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #hal.descriptor_type<storage_buffer>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
      %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %workgroup_id_x = hal.interface.workgroup.id[0] : index
      %workgroup_count_x = hal.interface.workgroup.count[0] : index
      gpu.barrier
      vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      gpu.barrier
      vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      gpu.barrier
      scf.for %arg0 = %c0 to %c96 step %c4 {
        gpu.barrier
        %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
        ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
          %9 = arith.cmpf ogt, %in, %out : f16
          %10 = arith.select %9, %in, %out : f16
          %11 = arith.select %9, %in_7, %out_8 : i32
          linalg.yield %10, %11 : f16, i32
        }
        gpu.barrier
      }
      return
    }
  }
}

// -----// IR Dump After ConvertHALDescriptorTypeToGPUAddressSpacePass (iree-codegen-convert-hal-descriptor-type-to-gpu-address-space) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %workgroup_id_x = hal.interface.workgroup.id[0] : index
    %workgroup_count_x = hal.interface.workgroup.count[0] : index
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg0 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
    return
  }
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg0 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
    return
  }
}

// -----// IR Dump After CSE (cse) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg0 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
    return
  }
}

// -----// IR Dump After LowerUKernelOpsToCallsPass (iree-codegen-lower-ukernel-ops-to-calls) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg0 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
      ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
        %9 = arith.cmpf ogt, %in, %out : f16
        %10 = arith.select %9, %in, %out : f16
        %11 = arith.select %9, %in_7, %out_8 : i32
        linalg.yield %10, %11 : f16, i32
      }
      gpu.barrier
    }
    return
  }
}

// -----// IR Dump After LinalgExtToLoopsPass (iree-linalg-ext-to-loops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
    ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
      %9 = arith.cmpf ogt, %in, %out : f16
      %10 = arith.select %9, %in, %out : f16
      %11 = arith.select %9, %in_7, %out_8 : i32
      linalg.yield %10, %11 : f16, i32
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After MemrefCopyToLinalgPass (iree-codegen-memrefcopy-to-linalg) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    linalg.generic {indexing_maps = [affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1, d2)>, affine_map<(d0, d1, d2) -> (d0, d1)>, affine_map<(d0, d1, d2) -> (d0, d1)>], iterator_types = ["parallel", "parallel", "reduction"]} ins(%subview_5, %subview_6 : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) outs(%subview, %subview_4 : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>, memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>) {
    ^bb0(%in: f16, %in_7: i32, %out: f16, %out_8: i32):
      %9 = arith.cmpf ogt, %in, %out : f16
      %10 = arith.select %9, %in, %out : f16
      %11 = arith.select %9, %in_7, %out_8 : i32
      linalg.yield %10, %11 : f16, i32
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After ConvertLinalgToLoopsPass (convert-linalg-to-loops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c1 step %c1 {
      scf.for %arg2 = %c0 to %c1 step %c1 {
        scf.for %arg3 = %c0 to %c4 step %c1 {
          %9 = memref.load %subview_5[%arg1, %arg2, %arg3] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
          %10 = memref.load %subview_6[%arg1, %arg2, %arg3] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
          %11 = memref.load %subview[%arg1, %arg2] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
          %12 = memref.load %subview_4[%arg1, %arg2] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
          %13 = arith.cmpf ogt, %9, %11 : f16
          %14 = arith.select %13, %9, %11 : f16
          %15 = arith.select %13, %10, %12 : i32
          memref.store %14, %subview[%arg1, %arg2] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
          memref.store %15, %subview_4[%arg1, %arg2] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        }
      }
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After PadDynamicAllocPass (iree-codegen-pad-dynamic-alloc) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocationsPass (iree-codegen-hoist-statically-bound-allocations) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After IREEBufferizeConstantsPass (iree-codegen-iree-bufferize-constants) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c1 = arith.constant 1 : index
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %c96 = arith.constant 96 : index
    %c4 = arith.constant 4 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
    %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    gpu.barrier
    scf.for %arg0 = %c0 to %c96 step %c4 {
      gpu.barrier
      %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      scf.for %arg1 = %c0 to %c4 step %c1 {
        %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        %13 = arith.cmpf ogt, %9, %11 : f16
        %14 = arith.select %13, %9, %11 : f16
        %15 = arith.select %13, %10, %12 : i32
        memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
        memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      }
      gpu.barrier
    }
    return
  }
}

// -----// IR Dump After FoldTensorExtractOpPass (iree-codegen-fold-tensor-extract-op) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After ExpandGPUOpsPass (iree-codegen-expand-gpu-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
    gpu.barrier
  }
  return
}

// -----// IR Dump After GpuEliminateBarriers (gpu-eliminate-barriers) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = memref.load %subview_5[%c0, %c0, %arg1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_6[%c0, %c0, %arg1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After ExtractAddressComputationGPUPass (extract-address-computation-gpu) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  vector.transfer_write %cst_0, %5[%8#0, %8#1] {in_bounds = [true]} : vector<1xf16>, memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  vector.transfer_write %cst, %7[%8#0, %8#1] {in_bounds = [true]} : vector<1xi32>, memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %subview_7 = memref.subview %subview_5[0, 0, %arg1] [1, 1, 1] [1, 1, 1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x1xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %9 = memref.load %subview_7[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_8 = memref.subview %subview_6[0, 0, %arg1] [1, 1, 1] [1, 1, 1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x1xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_8[%c0, %c0, %c0] : memref<1x1x1xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After VectorTransferLoweringPass (iree-codegen-vector-transfer-lowering) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  %subview = memref.subview %5[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  %subview_4 = memref.subview %7[%8#0, %8#1] [1, 1] [1, 1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    %subview_5 = memref.subview %1[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    %subview_6 = memref.subview %3[%8#0, %8#1, %arg0] [1, 1, 4] [1, 1, 1] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %subview_7 = memref.subview %subview_5[0, 0, %arg1] [1, 1, 1] [1, 1, 1] : memref<1x1x4xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x1xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %9 = memref.load %subview_7[%c0, %c0, %c0] : memref<1x1x1xf16, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %subview_8 = memref.subview %subview_6[0, 0, %arg1] [1, 1, 1] [1, 1, 1] : memref<1x1x4xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>> to memref<1x1x1xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %10 = memref.load %subview_8[%c0, %c0, %c0] : memref<1x1x1xi32, strided<[1536, 96, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      %13 = arith.cmpf ogt, %9, %11 : f16
      %14 = arith.select %13, %9, %11 : f16
      %15 = arith.select %13, %10, %12 : i32
      memref.store %14, %subview[%c0, %c0] : memref<1x1xf16, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %15, %subview_4[%c0, %c0] : memref<1x1xi32, strided<[16, 1], offset: ?>, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After FoldMemRefAliasOpsPass (fold-memref-alias-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %12 = memref.load %3[%8#0, %8#1, %11] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %14 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %15 = arith.cmpf ogt, %10, %13 : f16
      %16 = arith.select %15, %10, %13 : f16
      %17 = arith.select %15, %12, %14 : i32
      memref.store %16, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %17, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After PropagateConstantOffsetsPass (iree-codegen-propagate-constant-offsets) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %12 = memref.load %3[%8#0, %8#1, %11] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %14 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %15 = arith.cmpf ogt, %10, %13 : f16
      %16 = arith.select %15, %10, %13 : f16
      %17 = arith.select %15, %12, %14 : i32
      memref.store %16, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %17, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After ResolveSwizzleHintsPass (iree-codegen-resolve-swizzle-hints) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After IREEExpandStridedMetadataPass (iree-codegen-expand-strided-metadata) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After PropagateDispatchSizeBoundsPass (iree-codegen-propagate-dispatch-size-bounds) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After DecomposeAffineOpsPass (iree-codegen-decompose-affine-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8:2 = affine.delinearize_index %thread_id_x into (4, 16) : index, index
  vector.store %cst_0, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %9 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %10 = memref.load %1[%8#0, %8#1, %9] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %11 = memref.load %3[%8#0, %8#1, %9] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %12 = memref.load %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %13 = memref.load %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %14 = arith.cmpf ogt, %10, %12 : f16
      %15 = arith.select %14, %10, %12 : f16
      %16 = arith.select %14, %11, %13 : i32
      memref.store %15, %5[%8#0, %8#1] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %16, %7[%8#0, %8#1] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c16 = arith.constant 16 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.floordivsi %thread_id_x, %c16 : index
  %9 = arith.remsi %thread_id_x, %c16 : index
  %10 = arith.cmpi slt, %9, %c0 : index
  %11 = arith.addi %9, %c16 overflow<nsw> : index
  %12 = arith.select %10, %11, %9 : index
  vector.store %cst_0, %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %13 = affine.apply affine_map<()[s0, s1] -> (s0 + s1)>()[%arg0, %arg1]
      %14 = memref.load %1[%8, %12, %13] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %15 = memref.load %3[%8, %12, %13] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %18 = arith.cmpf ogt, %14, %16 : f16
      %19 = arith.select %18, %14, %16 : f16
      %20 = arith.select %18, %15, %17 : i32
      memref.store %19, %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %20, %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After LowerAffinePass (lower-affine) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c16 = arith.constant 16 : index
  %c1 = arith.constant 1 : index
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %c96 = arith.constant 96 : index
  %c4 = arith.constant 4 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.floordivsi %thread_id_x, %c16 : index
  %9 = arith.remsi %thread_id_x, %c16 : index
  %10 = arith.cmpi slt, %9, %c0 : index
  %11 = arith.addi %9, %c16 overflow<nsw> : index
  %12 = arith.select %10, %11, %9 : index
  vector.store %cst_0, %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0 to %c96 step %c4 {
    gpu.barrier
    scf.for %arg1 = %c0 to %c4 step %c1 {
      %13 = arith.addi %arg0, %arg1 : index
      %14 = memref.load %1[%8, %12, %13] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %15 = memref.load %3[%8, %12, %13] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %18 = arith.cmpf ogt, %14, %16 : f16
      %19 = arith.select %18, %14, %16 : f16
      %20 = arith.select %18, %15, %17 : i32
      memref.store %19, %5[%8, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %20, %7[%8, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After OptimizeIntArithmeticPass (iree-util-optimize-int-arithmetic) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.index_castui %thread_id_x : index to i32
  %12 = arith.remui %11, %c16_i32 : i32
  %13 = arith.index_castui %12 : i32 to index
  vector.store %cst_0, %5[%10, %13] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %13] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %14 = arith.addi %arg0, %arg1 : i32
      %15 = arith.index_castui %14 : i32 to index
      %16 = memref.load %1[%10, %13, %15] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %3[%10, %13, %15] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %5[%10, %13] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %19 = memref.load %7[%10, %13] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %20 = arith.cmpf ogt, %16, %18 : f16
      %21 = arith.select %20, %16, %18 : f16
      %22 = arith.select %20, %17, %19 : i32
      memref.store %21, %5[%10, %13] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %22, %7[%10, %13] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After IREELoopInvariantCodeMotionPass (iree-loop-invariant-code-motion) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After LLVMGPUVectorLoweringPass (iree-llvmgpu-vector-lowering) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After AmdgpuMaskedloadToLoadPass (amdgpu-maskedload-to-load) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After AmdgpuFoldMemRefOpsPass (amdgpu-fold-memrefs-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After GPUCheckResourceUsagePass (iree-codegen-gpu-check-resource-usage) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After HoistStaticallyBoundAllocationsPass (iree-codegen-hoist-statically-bound-allocations) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After ConvertComplexToStandardPass (convert-complex-to-standard) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After MathTransformPass (iree-codegen-math-transform) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  scf.for %arg0 = %c0_i32 to %c96_i32 step %c4_i32  : i32 {
    gpu.barrier
    scf.for %arg1 = %c0_i32 to %c4_i32 step %c1_i32  : i32 {
      %13 = arith.addi %arg0, %arg1 : i32
      %14 = arith.index_castui %13 : i32 to index
      %15 = memref.load %1[%10, %12, %14] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
      %16 = memref.load %3[%10, %12, %14] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
      %17 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      %18 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
      %19 = arith.cmpf ogt, %15, %17 : f16
      %20 = arith.select %19, %15, %17 : f16
      %21 = arith.select %19, %16, %18 : i32
      memref.store %20, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
      memref.store %21, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    }
  }
  return
}

// -----// IR Dump After SCFToControlFlowPass (convert-scf-to-cf) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After Canonicalizer (canonicalize) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After CSE (cse) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After FoldMemRefAliasOpsPass (fold-memref-alias-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After IREEExpandStridedMetadataPass (iree-codegen-expand-strided-metadata) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After AMDGPUEmulateNarrowTypePass (iree-amdgpu-emulate-narrow-type) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After AffineExpandIndexOps (affine-expand-index-ops) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After LowerAffinePass (lower-affine) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After ConvertUnsupportedFloatArithPass (iree-convert-unsupported-float-arith) //----- //
func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
  %c1_i32 = arith.constant 1 : i32
  %c4_i32 = arith.constant 4 : i32
  %c96_i32 = arith.constant 96 : i32
  %c0_i32 = arith.constant 0 : i32
  %c16_i32 = arith.constant 16 : i32
  %cst = arith.constant dense<0> : vector<1xi32>
  %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
  %c128 = arith.constant 128 : index
  %c12288 = arith.constant 12288 : index
  %c0 = arith.constant 0 : index
  %thread_id_x = gpu.thread_id  x upper_bound 64
  %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
  %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
  %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
  %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
  %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
  %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
  %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %8 = arith.index_castui %thread_id_x : index to i32
  %9 = arith.divui %8, %c16_i32 : i32
  %10 = arith.index_castui %9 : i32 to index
  %11 = arith.remui %8, %c16_i32 : i32
  %12 = arith.index_castui %11 : i32 to index
  vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
  gpu.barrier
  vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
  gpu.barrier
  cf.br ^bb1(%c0_i32 : i32)
^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
  %14 = arith.cmpi slt, %13, %c96_i32 : i32
  cf.cond_br %14, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  gpu.barrier
  cf.br ^bb3(%c0_i32 : i32)
^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
  %16 = arith.cmpi slt, %15, %c4_i32 : i32
  cf.cond_br %16, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %17 = arith.addi %13, %15 : i32
  %18 = arith.index_castui %17 : i32 to index
  %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
  %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
  %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %23 = arith.cmpf ogt, %19, %21 : f16
  %24 = arith.select %23, %19, %21 : f16
  %25 = arith.select %23, %20, %22 : i32
  memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
  memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
  %26 = arith.addi %15, %c1_i32 : i32
  cf.br ^bb3(%26 : i32)
^bb5:  // pred: ^bb3
  %27 = arith.addi %13, %c4_i32 : i32
  cf.br ^bb1(%27 : i32)
^bb6:  // pred: ^bb1
  return
}

// -----// IR Dump After StripDebugInfo (strip-debuginfo) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %c96_i32 = arith.constant 96 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = arith.index_castui %thread_id_x : index to i32
    %9 = arith.divui %8, %c16_i32 : i32
    %10 = arith.index_castui %9 : i32 to index
    %11 = arith.remui %8, %c16_i32 : i32
    %12 = arith.index_castui %11 : i32 to index
    vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
    gpu.barrier
    vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
    gpu.barrier
    cf.br ^bb1(%c0_i32 : i32)
  ^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
    %14 = arith.cmpi slt, %13, %c96_i32 : i32
    cf.cond_br %14, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    gpu.barrier
    cf.br ^bb3(%c0_i32 : i32)
  ^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
    %16 = arith.cmpi slt, %15, %c4_i32 : i32
    cf.cond_br %16, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %17 = arith.addi %13, %15 : i32
    %18 = arith.index_castui %17 : i32 to index
    %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %23 = arith.cmpf ogt, %19, %21 : f16
    %24 = arith.select %23, %19, %21 : f16
    %25 = arith.select %23, %20, %22 : i32
    memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %26 = arith.addi %15, %c1_i32 : i32
    cf.br ^bb3(%26 : i32)
  ^bb5:  // pred: ^bb3
    %27 = arith.addi %13, %c4_i32 : i32
    cf.br ^bb1(%27 : i32)
  ^bb6:  // pred: ^bb1
    return
  }
}

// -----// IR Dump After LLVMGPUCastAddressSpaceFunctionPass (iree-llvmgpu-cast-address-space-function) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %c96_i32 = arith.constant 96 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = arith.index_castui %thread_id_x : index to i32
    %9 = arith.divui %8, %c16_i32 : i32
    %10 = arith.index_castui %9 : i32 to index
    %11 = arith.remui %8, %c16_i32 : i32
    %12 = arith.index_castui %11 : i32 to index
    vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
    gpu.barrier
    vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
    gpu.barrier
    cf.br ^bb1(%c0_i32 : i32)
  ^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
    %14 = arith.cmpi slt, %13, %c96_i32 : i32
    cf.cond_br %14, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    gpu.barrier
    cf.br ^bb3(%c0_i32 : i32)
  ^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
    %16 = arith.cmpi slt, %15, %c4_i32 : i32
    cf.cond_br %16, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %17 = arith.addi %13, %15 : i32
    %18 = arith.index_castui %17 : i32 to index
    %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %23 = arith.cmpf ogt, %19, %21 : f16
    %24 = arith.select %23, %19, %21 : f16
    %25 = arith.select %23, %20, %22 : i32
    memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %26 = arith.addi %15, %c1_i32 : i32
    cf.br ^bb3(%26 : i32)
  ^bb5:  // pred: ^bb3
    %27 = arith.addi %13, %c4_i32 : i32
    cf.br ^bb1(%27 : i32)
  ^bb6:  // pred: ^bb1
    return
  }
}

// -----// IR Dump After DropCompilerHintsPass (iree-util-drop-compiler-hints) //----- //
module {
  func.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32() {
    %c1_i32 = arith.constant 1 : i32
    %c4_i32 = arith.constant 4 : i32
    %c96_i32 = arith.constant 96 : i32
    %c0_i32 = arith.constant 0 : i32
    %c16_i32 = arith.constant 16 : i32
    %cst = arith.constant dense<0> : vector<1xi32>
    %cst_0 = arith.constant dense<0xFC00> : vector<1xf16>
    %c128 = arith.constant 128 : index
    %c12288 = arith.constant 12288 : index
    %c0 = arith.constant 0 : index
    %thread_id_x = gpu.thread_id  x upper_bound 64
    %0 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c0) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xf16, #gpu.address_space<global>>
    %assume_align = memref.assume_alignment %0, 64 : memref<4x16x96xf16, #gpu.address_space<global>>
    %1 = amdgpu.fat_raw_buffer_cast %assume_align resetOffset : memref<4x16x96xf16, #gpu.address_space<global>> to memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %2 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(0) alignment(64) offset(%c12288) flags("ReadOnly|Indirect") {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_1 = memref.assume_alignment %2, 64 : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>>
    %3 = amdgpu.fat_raw_buffer_cast %assume_align_1 resetOffset : memref<4x16x96xi32, strided<[1536, 96, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %4 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(1) alignment(64) offset(%c0) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xf16, #gpu.address_space<global>>
    %assume_align_2 = memref.assume_alignment %4, 64 : memref<4x16xf16, #gpu.address_space<global>>
    %5 = amdgpu.fat_raw_buffer_cast %assume_align_2 resetOffset : memref<4x16xf16, #gpu.address_space<global>> to memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %6 = hal.interface.binding.subspan layout(<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) binding(2) alignment(64) offset(%c128) flags(Indirect) {iree_gpu.use_rocdl_buffer_instructions} : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %assume_align_3 = memref.assume_alignment %6, 64 : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>>
    %7 = amdgpu.fat_raw_buffer_cast %assume_align_3 resetOffset : memref<4x16xi32, strided<[16, 1], offset: ?>, #gpu.address_space<global>> to memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %8 = arith.index_castui %thread_id_x : index to i32
    %9 = arith.divui %8, %c16_i32 : i32
    %10 = arith.index_castui %9 : i32 to index
    %11 = arith.remui %8, %c16_i32 : i32
    %12 = arith.index_castui %11 : i32 to index
    vector.store %cst_0, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>, vector<1xf16>
    gpu.barrier
    vector.store %cst, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>, vector<1xi32>
    gpu.barrier
    cf.br ^bb1(%c0_i32 : i32)
  ^bb1(%13: i32):  // 2 preds: ^bb0, ^bb5
    %14 = arith.cmpi slt, %13, %c96_i32 : i32
    cf.cond_br %14, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    gpu.barrier
    cf.br ^bb3(%c0_i32 : i32)
  ^bb3(%15: i32):  // 2 preds: ^bb2, ^bb4
    %16 = arith.cmpi slt, %15, %c4_i32 : i32
    cf.cond_br %16, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %17 = arith.addi %13, %15 : i32
    %18 = arith.index_castui %17 : i32 to index
    %19 = memref.load %1[%10, %12, %18] : memref<4x16x96xf16, #amdgpu.address_space<fat_raw_buffer>>
    %20 = memref.load %3[%10, %12, %18] : memref<4x16x96xi32, #amdgpu.address_space<fat_raw_buffer>>
    %21 = memref.load %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    %22 = memref.load %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %23 = arith.cmpf ogt, %19, %21 : f16
    %24 = arith.select %23, %19, %21 : f16
    %25 = arith.select %23, %20, %22 : i32
    memref.store %24, %5[%10, %12] : memref<4x16xf16, #amdgpu.address_space<fat_raw_buffer>>
    memref.store %25, %7[%10, %12] : memref<4x16xi32, #amdgpu.address_space<fat_raw_buffer>>
    %26 = arith.addi %15, %c1_i32 : i32
    cf.br ^bb3(%26 : i32)
  ^bb5:  // pred: ^bb3
    %27 = arith.addi %13, %c4_i32 : i32
    cf.br ^bb1(%27 : i32)
  ^bb6:  // pred: ^bb1
    return
  }
}

// -----// IR Dump After ConvertToROCDLPass (iree-convert-to-rocdl) //----- //
module {
  llvm.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.nonnull, llvm.noundef, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.nonnull, llvm.noundef}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.noalias, llvm.nonnull, llvm.noundef}) {
    %0 = llvm.mlir.constant(1 : i32) : i32
    %1 = llvm.mlir.constant(4 : i32) : i32
    %2 = llvm.mlir.constant(96 : i32) : i32
    %3 = llvm.mlir.constant(0 : i32) : i32
    %4 = llvm.mlir.constant(16 : i32) : i32
    %5 = llvm.mlir.constant(dense<0> : vector<1xi32>) : vector<1xi32>
    %6 = llvm.mlir.constant(dense<0xFC00> : vector<1xf16>) : vector<1xf16>
    %7 = llvm.mlir.constant(128 : index) : i64
    %8 = llvm.mlir.constant(12288 : index) : i64
    %9 = llvm.mlir.constant(0 : index) : i64
    %10 = rocdl.workitem.id.x range <i32, 0, 64> : i32
    %11 = llvm.sext %10 : i32 to i64
    %12 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
    %13 = llvm.insertvalue %arg0, %12[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %14 = llvm.insertvalue %arg0, %13[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %15 = llvm.mlir.constant(0 : index) : i64
    %16 = llvm.insertvalue %15, %14[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %17 = llvm.mlir.constant(4 : index) : i64
    %18 = llvm.insertvalue %17, %16[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %19 = llvm.mlir.constant(1536 : index) : i64
    %20 = llvm.insertvalue %19, %18[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %21 = llvm.mlir.constant(16 : index) : i64
    %22 = llvm.insertvalue %21, %20[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %23 = llvm.mlir.constant(96 : index) : i64
    %24 = llvm.insertvalue %23, %22[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %25 = llvm.mlir.constant(96 : index) : i64
    %26 = llvm.insertvalue %25, %24[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %27 = llvm.mlir.constant(1 : index) : i64
    %28 = llvm.insertvalue %27, %26[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %29 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %30 = llvm.mlir.constant(true) : i1
    %31 = llvm.mlir.constant(64 : index) : i64
    llvm.intr.assume %30 ["align"(%29, %31 : !llvm.ptr<1>, i64)] : i1
    %32 = llvm.mlir.constant(12288 : i64) : i64
    %33 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %34 = llvm.mlir.constant(0 : index) : i64
    %35 = llvm.extractvalue %28[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %36 = llvm.extractvalue %28[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %37 = llvm.mlir.constant(0 : i16) : i16
    %38 = llvm.mlir.constant(159744 : i32) : i32
    %39 = rocdl.make.buffer.rsrc %33, %37, %32, %38 : <1> to <7>
    %40 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
    %41 = llvm.insertvalue %39, %40[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %42 = llvm.insertvalue %39, %41[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %43 = llvm.insertvalue %34, %42[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %44 = llvm.insertvalue %35, %43[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %45 = llvm.insertvalue %36, %44[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %46 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
    %47 = llvm.insertvalue %arg0, %46[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %48 = llvm.insertvalue %arg0, %47[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %49 = llvm.mlir.constant(32 : i64) : i64
    %50 = llvm.mlir.constant(8 : i64) : i64
    %51 = llvm.mul %8, %50 : i64
    %52 = llvm.udiv %51, %49 : i64
    %53 = llvm.insertvalue %52, %48[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %54 = llvm.mlir.constant(4 : index) : i64
    %55 = llvm.insertvalue %54, %53[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %56 = llvm.mlir.constant(16 : index) : i64
    %57 = llvm.insertvalue %56, %55[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %58 = llvm.mlir.constant(96 : index) : i64
    %59 = llvm.insertvalue %58, %57[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %60 = llvm.mlir.constant(1 : index) : i64
    %61 = llvm.insertvalue %60, %59[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %62 = llvm.mlir.constant(96 : index) : i64
    %63 = llvm.insertvalue %62, %61[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %64 = llvm.mlir.constant(1536 : index) : i64
    %65 = llvm.insertvalue %64, %63[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %66 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %67 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %68 = llvm.getelementptr %66[%67] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
    %69 = llvm.mlir.constant(true) : i1
    %70 = llvm.mlir.constant(64 : index) : i64
    llvm.intr.assume %69 ["align"(%68, %70 : !llvm.ptr<1>, i64)] : i1
    %71 = llvm.mlir.constant(24576 : i64) : i64
    %72 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %73 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %74 = llvm.getelementptr %72[%73] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
    %75 = llvm.mlir.constant(0 : index) : i64
    %76 = llvm.extractvalue %65[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %77 = llvm.extractvalue %65[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
    %78 = llvm.mlir.constant(0 : i16) : i16
    %79 = llvm.mlir.constant(159744 : i32) : i32
    %80 = rocdl.make.buffer.rsrc %74, %78, %71, %79 : <1> to <7>
    %81 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
    %82 = llvm.insertvalue %80, %81[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %83 = llvm.insertvalue %80, %82[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %84 = llvm.insertvalue %75, %83[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %85 = llvm.insertvalue %76, %84[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %86 = llvm.insertvalue %77, %85[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %87 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
    %88 = llvm.insertvalue %arg1, %87[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %89 = llvm.insertvalue %arg1, %88[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %90 = llvm.mlir.constant(0 : index) : i64
    %91 = llvm.insertvalue %90, %89[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %92 = llvm.mlir.constant(4 : index) : i64
    %93 = llvm.insertvalue %92, %91[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %94 = llvm.mlir.constant(16 : index) : i64
    %95 = llvm.insertvalue %94, %93[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %96 = llvm.mlir.constant(16 : index) : i64
    %97 = llvm.insertvalue %96, %95[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %98 = llvm.mlir.constant(1 : index) : i64
    %99 = llvm.insertvalue %98, %97[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %100 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %101 = llvm.mlir.constant(true) : i1
    %102 = llvm.mlir.constant(64 : index) : i64
    llvm.intr.assume %101 ["align"(%100, %102 : !llvm.ptr<1>, i64)] : i1
    %103 = llvm.mlir.constant(128 : i64) : i64
    %104 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %105 = llvm.mlir.constant(0 : index) : i64
    %106 = llvm.extractvalue %99[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %107 = llvm.extractvalue %99[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %108 = llvm.mlir.constant(0 : i16) : i16
    %109 = llvm.mlir.constant(159744 : i32) : i32
    %110 = rocdl.make.buffer.rsrc %104, %108, %103, %109 : <1> to <7>
    %111 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
    %112 = llvm.insertvalue %110, %111[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %113 = llvm.insertvalue %110, %112[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %114 = llvm.insertvalue %105, %113[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %115 = llvm.insertvalue %106, %114[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %116 = llvm.insertvalue %107, %115[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %117 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
    %118 = llvm.insertvalue %arg2, %117[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %119 = llvm.insertvalue %arg2, %118[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %120 = llvm.mlir.constant(32 : i64) : i64
    %121 = llvm.mlir.constant(8 : i64) : i64
    %122 = llvm.mul %7, %121 : i64
    %123 = llvm.udiv %122, %120 : i64
    %124 = llvm.insertvalue %123, %119[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %125 = llvm.mlir.constant(4 : index) : i64
    %126 = llvm.insertvalue %125, %124[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %127 = llvm.mlir.constant(16 : index) : i64
    %128 = llvm.insertvalue %127, %126[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %129 = llvm.mlir.constant(1 : index) : i64
    %130 = llvm.insertvalue %129, %128[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %131 = llvm.mlir.constant(16 : index) : i64
    %132 = llvm.insertvalue %131, %130[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %133 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %134 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %135 = llvm.getelementptr %133[%134] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
    %136 = llvm.mlir.constant(true) : i1
    %137 = llvm.mlir.constant(64 : index) : i64
    llvm.intr.assume %136 ["align"(%135, %137 : !llvm.ptr<1>, i64)] : i1
    %138 = llvm.mlir.constant(256 : i64) : i64
    %139 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %141 = llvm.getelementptr %139[%140] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
    %142 = llvm.mlir.constant(0 : index) : i64
    %143 = llvm.extractvalue %132[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %144 = llvm.extractvalue %132[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
    %145 = llvm.mlir.constant(0 : i16) : i16
    %146 = llvm.mlir.constant(159744 : i32) : i32
    %147 = rocdl.make.buffer.rsrc %141, %145, %138, %146 : <1> to <7>
    %148 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
    %149 = llvm.insertvalue %147, %148[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %150 = llvm.insertvalue %147, %149[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %151 = llvm.insertvalue %142, %150[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %152 = llvm.insertvalue %143, %151[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %153 = llvm.insertvalue %144, %152[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %154 = llvm.trunc %11 : i64 to i32
    %155 = llvm.udiv %154, %4 : i32
    %156 = llvm.zext %155 : i32 to i64
    %157 = llvm.urem %154, %4 : i32
    %158 = llvm.zext %157 : i32 to i64
    %159 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %160 = llvm.mlir.constant(16 : index) : i64
    %161 = llvm.mul %156, %160 : i64
    %162 = llvm.add %161, %158 : i64
    %163 = llvm.getelementptr %159[%162] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
    llvm.store %6, %163 {alignment = 2 : i64} : vector<1xf16>, !llvm.ptr<7>
    llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    rocdl.s.barrier
    llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    %164 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %165 = llvm.mlir.constant(16 : index) : i64
    %166 = llvm.mul %156, %165 : i64
    %167 = llvm.add %166, %158 : i64
    %168 = llvm.getelementptr %164[%167] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
    llvm.store %5, %168 {alignment = 4 : i64} : vector<1xi32>, !llvm.ptr<7>
    llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    rocdl.s.barrier
    llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    llvm.br ^bb1(%3 : i32)
  ^bb1(%169: i32):  // 2 preds: ^bb0, ^bb5
    %170 = llvm.icmp "slt" %169, %2 : i32
    llvm.cond_br %170, ^bb2, ^bb6
  ^bb2:  // pred: ^bb1
    llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    rocdl.s.barrier
    llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
    llvm.br ^bb3(%3 : i32)
  ^bb3(%171: i32):  // 2 preds: ^bb2, ^bb4
    %172 = llvm.icmp "slt" %171, %1 : i32
    llvm.cond_br %172, ^bb4, ^bb5
  ^bb4:  // pred: ^bb3
    %173 = llvm.add %169, %171 : i32
    %174 = llvm.zext %173 : i32 to i64
    %175 = llvm.extractvalue %45[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %176 = llvm.mlir.constant(1536 : index) : i64
    %177 = llvm.mul %156, %176 overflow<nsw, nuw> : i64
    %178 = llvm.mlir.constant(96 : index) : i64
    %179 = llvm.mul %158, %178 overflow<nsw, nuw> : i64
    %180 = llvm.add %177, %179 overflow<nsw, nuw> : i64
    %181 = llvm.add %180, %174 overflow<nsw, nuw> : i64
    %182 = llvm.getelementptr inbounds|nuw %175[%181] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
    %183 = llvm.load %182 : !llvm.ptr<7> -> f16
    %184 = llvm.extractvalue %86[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
    %185 = llvm.mlir.constant(1536 : index) : i64
    %186 = llvm.mul %156, %185 overflow<nsw, nuw> : i64
    %187 = llvm.mlir.constant(96 : index) : i64
    %188 = llvm.mul %158, %187 overflow<nsw, nuw> : i64
    %189 = llvm.add %186, %188 overflow<nsw, nuw> : i64
    %190 = llvm.add %189, %174 overflow<nsw, nuw> : i64
    %191 = llvm.getelementptr inbounds|nuw %184[%190] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
    %192 = llvm.load %191 : !llvm.ptr<7> -> i32
    %193 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %194 = llvm.mlir.constant(16 : index) : i64
    %195 = llvm.mul %156, %194 overflow<nsw, nuw> : i64
    %196 = llvm.add %195, %158 overflow<nsw, nuw> : i64
    %197 = llvm.getelementptr inbounds|nuw %193[%196] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
    %198 = llvm.load %197 : !llvm.ptr<7> -> f16
    %199 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %200 = llvm.mlir.constant(16 : index) : i64
    %201 = llvm.mul %156, %200 overflow<nsw, nuw> : i64
    %202 = llvm.add %201, %158 overflow<nsw, nuw> : i64
    %203 = llvm.getelementptr inbounds|nuw %199[%202] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
    %204 = llvm.load %203 : !llvm.ptr<7> -> i32
    %205 = llvm.fcmp "ogt" %183, %198 : f16
    %206 = llvm.select %205, %183, %198 : i1, f16
    %207 = llvm.select %205, %192, %204 : i1, i32
    %208 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %209 = llvm.mlir.constant(16 : index) : i64
    %210 = llvm.mul %156, %209 overflow<nsw, nuw> : i64
    %211 = llvm.add %210, %158 overflow<nsw, nuw> : i64
    %212 = llvm.getelementptr inbounds|nuw %208[%211] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
    llvm.store %206, %212 : f16, !llvm.ptr<7>
    %213 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
    %214 = llvm.mlir.constant(16 : index) : i64
    %215 = llvm.mul %156, %214 overflow<nsw, nuw> : i64
    %216 = llvm.add %215, %158 overflow<nsw, nuw> : i64
    %217 = llvm.getelementptr inbounds|nuw %213[%216] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
    llvm.store %207, %217 : i32, !llvm.ptr<7>
    %218 = llvm.add %171, %0 : i32
    llvm.br ^bb3(%218 : i32)
  ^bb5:  // pred: ^bb3
    %219 = llvm.add %169, %1 : i32
    llvm.br ^bb1(%219 : i32)
  ^bb6:  // pred: ^bb1
    llvm.return
  }
}

// -----// IR Dump After ROCDLAnnotateKernelForTranslationPass (iree-rocdl-annotate-kernel-for-translation) //----- //
llvm.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}) attributes {rocdl.flat_work_group_size = "64,64", rocdl.kernel, rocdl.reqd_work_group_size = array<i32: 64, 1, 1>} {
  %0 = llvm.mlir.constant(1 : i32) : i32
  %1 = llvm.mlir.constant(4 : i32) : i32
  %2 = llvm.mlir.constant(96 : i32) : i32
  %3 = llvm.mlir.constant(0 : i32) : i32
  %4 = llvm.mlir.constant(16 : i32) : i32
  %5 = llvm.mlir.constant(dense<0> : vector<1xi32>) : vector<1xi32>
  %6 = llvm.mlir.constant(dense<0xFC00> : vector<1xf16>) : vector<1xf16>
  %7 = llvm.mlir.constant(128 : index) : i64
  %8 = llvm.mlir.constant(12288 : index) : i64
  %9 = llvm.mlir.constant(0 : index) : i64
  %10 = rocdl.workitem.id.x range <i32, 0, 64> : i32
  %11 = llvm.sext %10 : i32 to i64
  %12 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
  %13 = llvm.insertvalue %arg0, %12[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %14 = llvm.insertvalue %arg0, %13[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %15 = llvm.mlir.constant(0 : index) : i64
  %16 = llvm.insertvalue %15, %14[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %17 = llvm.mlir.constant(4 : index) : i64
  %18 = llvm.insertvalue %17, %16[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %19 = llvm.mlir.constant(1536 : index) : i64
  %20 = llvm.insertvalue %19, %18[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %21 = llvm.mlir.constant(16 : index) : i64
  %22 = llvm.insertvalue %21, %20[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %23 = llvm.mlir.constant(96 : index) : i64
  %24 = llvm.insertvalue %23, %22[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %25 = llvm.mlir.constant(96 : index) : i64
  %26 = llvm.insertvalue %25, %24[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %27 = llvm.mlir.constant(1 : index) : i64
  %28 = llvm.insertvalue %27, %26[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %29 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %30 = llvm.mlir.constant(true) : i1
  %31 = llvm.mlir.constant(64 : index) : i64
  llvm.intr.assume %30 ["align"(%29, %31 : !llvm.ptr<1>, i64)] : i1
  %32 = llvm.mlir.constant(12288 : i64) : i64
  %33 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %34 = llvm.mlir.constant(0 : index) : i64
  %35 = llvm.extractvalue %28[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %36 = llvm.extractvalue %28[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %37 = llvm.mlir.constant(0 : i16) : i16
  %38 = llvm.mlir.constant(159744 : i32) : i32
  %39 = rocdl.make.buffer.rsrc %33, %37, %32, %38 : <1> to <7>
  %40 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
  %41 = llvm.insertvalue %39, %40[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %42 = llvm.insertvalue %39, %41[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %43 = llvm.insertvalue %34, %42[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %44 = llvm.insertvalue %35, %43[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %45 = llvm.insertvalue %36, %44[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %46 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
  %47 = llvm.insertvalue %arg0, %46[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %48 = llvm.insertvalue %arg0, %47[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %49 = llvm.mlir.constant(32 : i64) : i64
  %50 = llvm.mlir.constant(8 : i64) : i64
  %51 = llvm.mul %8, %50 : i64
  %52 = llvm.udiv %51, %49 : i64
  %53 = llvm.insertvalue %52, %48[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %54 = llvm.mlir.constant(4 : index) : i64
  %55 = llvm.insertvalue %54, %53[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %56 = llvm.mlir.constant(16 : index) : i64
  %57 = llvm.insertvalue %56, %55[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %58 = llvm.mlir.constant(96 : index) : i64
  %59 = llvm.insertvalue %58, %57[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %60 = llvm.mlir.constant(1 : index) : i64
  %61 = llvm.insertvalue %60, %59[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %62 = llvm.mlir.constant(96 : index) : i64
  %63 = llvm.insertvalue %62, %61[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %64 = llvm.mlir.constant(1536 : index) : i64
  %65 = llvm.insertvalue %64, %63[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %66 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %67 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %68 = llvm.getelementptr %66[%67] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
  %69 = llvm.mlir.constant(true) : i1
  %70 = llvm.mlir.constant(64 : index) : i64
  llvm.intr.assume %69 ["align"(%68, %70 : !llvm.ptr<1>, i64)] : i1
  %71 = llvm.mlir.constant(24576 : i64) : i64
  %72 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %73 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %74 = llvm.getelementptr %72[%73] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
  %75 = llvm.mlir.constant(0 : index) : i64
  %76 = llvm.extractvalue %65[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %77 = llvm.extractvalue %65[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
  %78 = llvm.mlir.constant(0 : i16) : i16
  %79 = llvm.mlir.constant(159744 : i32) : i32
  %80 = rocdl.make.buffer.rsrc %74, %78, %71, %79 : <1> to <7>
  %81 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
  %82 = llvm.insertvalue %80, %81[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %83 = llvm.insertvalue %80, %82[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %84 = llvm.insertvalue %75, %83[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %85 = llvm.insertvalue %76, %84[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %86 = llvm.insertvalue %77, %85[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %87 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
  %88 = llvm.insertvalue %arg1, %87[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %89 = llvm.insertvalue %arg1, %88[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %90 = llvm.mlir.constant(0 : index) : i64
  %91 = llvm.insertvalue %90, %89[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %92 = llvm.mlir.constant(4 : index) : i64
  %93 = llvm.insertvalue %92, %91[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %94 = llvm.mlir.constant(16 : index) : i64
  %95 = llvm.insertvalue %94, %93[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %96 = llvm.mlir.constant(16 : index) : i64
  %97 = llvm.insertvalue %96, %95[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %98 = llvm.mlir.constant(1 : index) : i64
  %99 = llvm.insertvalue %98, %97[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %100 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %101 = llvm.mlir.constant(true) : i1
  %102 = llvm.mlir.constant(64 : index) : i64
  llvm.intr.assume %101 ["align"(%100, %102 : !llvm.ptr<1>, i64)] : i1
  %103 = llvm.mlir.constant(128 : i64) : i64
  %104 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %105 = llvm.mlir.constant(0 : index) : i64
  %106 = llvm.extractvalue %99[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %107 = llvm.extractvalue %99[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %108 = llvm.mlir.constant(0 : i16) : i16
  %109 = llvm.mlir.constant(159744 : i32) : i32
  %110 = rocdl.make.buffer.rsrc %104, %108, %103, %109 : <1> to <7>
  %111 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
  %112 = llvm.insertvalue %110, %111[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %113 = llvm.insertvalue %110, %112[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %114 = llvm.insertvalue %105, %113[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %115 = llvm.insertvalue %106, %114[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %116 = llvm.insertvalue %107, %115[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %117 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
  %118 = llvm.insertvalue %arg2, %117[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %119 = llvm.insertvalue %arg2, %118[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %120 = llvm.mlir.constant(32 : i64) : i64
  %121 = llvm.mlir.constant(8 : i64) : i64
  %122 = llvm.mul %7, %121 : i64
  %123 = llvm.udiv %122, %120 : i64
  %124 = llvm.insertvalue %123, %119[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %125 = llvm.mlir.constant(4 : index) : i64
  %126 = llvm.insertvalue %125, %124[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %127 = llvm.mlir.constant(16 : index) : i64
  %128 = llvm.insertvalue %127, %126[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %129 = llvm.mlir.constant(1 : index) : i64
  %130 = llvm.insertvalue %129, %128[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %131 = llvm.mlir.constant(16 : index) : i64
  %132 = llvm.insertvalue %131, %130[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %133 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %134 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %135 = llvm.getelementptr %133[%134] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
  %136 = llvm.mlir.constant(true) : i1
  %137 = llvm.mlir.constant(64 : index) : i64
  llvm.intr.assume %136 ["align"(%135, %137 : !llvm.ptr<1>, i64)] : i1
  %138 = llvm.mlir.constant(256 : i64) : i64
  %139 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %141 = llvm.getelementptr %139[%140] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
  %142 = llvm.mlir.constant(0 : index) : i64
  %143 = llvm.extractvalue %132[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %144 = llvm.extractvalue %132[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
  %145 = llvm.mlir.constant(0 : i16) : i16
  %146 = llvm.mlir.constant(159744 : i32) : i32
  %147 = rocdl.make.buffer.rsrc %141, %145, %138, %146 : <1> to <7>
  %148 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
  %149 = llvm.insertvalue %147, %148[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %150 = llvm.insertvalue %147, %149[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %151 = llvm.insertvalue %142, %150[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %152 = llvm.insertvalue %143, %151[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %153 = llvm.insertvalue %144, %152[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %154 = llvm.trunc %11 : i64 to i32
  %155 = llvm.udiv %154, %4 : i32
  %156 = llvm.zext %155 : i32 to i64
  %157 = llvm.urem %154, %4 : i32
  %158 = llvm.zext %157 : i32 to i64
  %159 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %160 = llvm.mlir.constant(16 : index) : i64
  %161 = llvm.mul %156, %160 : i64
  %162 = llvm.add %161, %158 : i64
  %163 = llvm.getelementptr %159[%162] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
  llvm.store %6, %163 {alignment = 2 : i64} : vector<1xf16>, !llvm.ptr<7>
  llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  rocdl.s.barrier
  llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  %164 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %165 = llvm.mlir.constant(16 : index) : i64
  %166 = llvm.mul %156, %165 : i64
  %167 = llvm.add %166, %158 : i64
  %168 = llvm.getelementptr %164[%167] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
  llvm.store %5, %168 {alignment = 4 : i64} : vector<1xi32>, !llvm.ptr<7>
  llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  rocdl.s.barrier
  llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  llvm.br ^bb1(%3 : i32)
^bb1(%169: i32):  // 2 preds: ^bb0, ^bb5
  %170 = llvm.icmp "slt" %169, %2 : i32
  llvm.cond_br %170, ^bb2, ^bb6
^bb2:  // pred: ^bb1
  llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  rocdl.s.barrier
  llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
  llvm.br ^bb3(%3 : i32)
^bb3(%171: i32):  // 2 preds: ^bb2, ^bb4
  %172 = llvm.icmp "slt" %171, %1 : i32
  llvm.cond_br %172, ^bb4, ^bb5
^bb4:  // pred: ^bb3
  %173 = llvm.add %169, %171 : i32
  %174 = llvm.zext %173 : i32 to i64
  %175 = llvm.extractvalue %45[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %176 = llvm.mlir.constant(1536 : index) : i64
  %177 = llvm.mul %156, %176 overflow<nsw, nuw> : i64
  %178 = llvm.mlir.constant(96 : index) : i64
  %179 = llvm.mul %158, %178 overflow<nsw, nuw> : i64
  %180 = llvm.add %177, %179 overflow<nsw, nuw> : i64
  %181 = llvm.add %180, %174 overflow<nsw, nuw> : i64
  %182 = llvm.getelementptr inbounds|nuw %175[%181] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
  %183 = llvm.load %182 : !llvm.ptr<7> -> f16
  %184 = llvm.extractvalue %86[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
  %185 = llvm.mlir.constant(1536 : index) : i64
  %186 = llvm.mul %156, %185 overflow<nsw, nuw> : i64
  %187 = llvm.mlir.constant(96 : index) : i64
  %188 = llvm.mul %158, %187 overflow<nsw, nuw> : i64
  %189 = llvm.add %186, %188 overflow<nsw, nuw> : i64
  %190 = llvm.add %189, %174 overflow<nsw, nuw> : i64
  %191 = llvm.getelementptr inbounds|nuw %184[%190] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
  %192 = llvm.load %191 : !llvm.ptr<7> -> i32
  %193 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %194 = llvm.mlir.constant(16 : index) : i64
  %195 = llvm.mul %156, %194 overflow<nsw, nuw> : i64
  %196 = llvm.add %195, %158 overflow<nsw, nuw> : i64
  %197 = llvm.getelementptr inbounds|nuw %193[%196] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
  %198 = llvm.load %197 : !llvm.ptr<7> -> f16
  %199 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %200 = llvm.mlir.constant(16 : index) : i64
  %201 = llvm.mul %156, %200 overflow<nsw, nuw> : i64
  %202 = llvm.add %201, %158 overflow<nsw, nuw> : i64
  %203 = llvm.getelementptr inbounds|nuw %199[%202] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
  %204 = llvm.load %203 : !llvm.ptr<7> -> i32
  %205 = llvm.fcmp "ogt" %183, %198 : f16
  %206 = llvm.select %205, %183, %198 : i1, f16
  %207 = llvm.select %205, %192, %204 : i1, i32
  %208 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %209 = llvm.mlir.constant(16 : index) : i64
  %210 = llvm.mul %156, %209 overflow<nsw, nuw> : i64
  %211 = llvm.add %210, %158 overflow<nsw, nuw> : i64
  %212 = llvm.getelementptr inbounds|nuw %208[%211] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
  llvm.store %206, %212 : f16, !llvm.ptr<7>
  %213 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
  %214 = llvm.mlir.constant(16 : index) : i64
  %215 = llvm.mul %156, %214 overflow<nsw, nuw> : i64
  %216 = llvm.add %215, %158 overflow<nsw, nuw> : i64
  %217 = llvm.getelementptr inbounds|nuw %213[%216] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
  llvm.store %207, %217 : i32, !llvm.ptr<7>
  %218 = llvm.add %171, %0 : i32
  llvm.br ^bb3(%218 : i32)
^bb5:  // pred: ^bb3
  %219 = llvm.add %169, %1 : i32
  llvm.br ^bb1(%219 : i32)
^bb6:  // pred: ^bb1
  llvm.return
}

// -----// IR Dump After TranslateTargetExecutableVariantsPass (iree-hal-translate-target-executable-variants) //----- //
hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
  hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
    %c1 = arith.constant 1 : index
    %c1_0 = arith.constant 1 : index
    %c1_1 = arith.constant 1 : index
    hal.return %c1, %c1_0, %c1_1 : index, index, index
  } attributes {subgroup_size = 64 : index, workgroup_size = [64 : index, 1 : index, 1 : index]}
  builtin.module {
    llvm.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}) attributes {rocdl.flat_work_group_size = "64,64", rocdl.kernel, rocdl.reqd_work_group_size = array<i32: 64, 1, 1>} {
      %0 = llvm.mlir.constant(1 : i32) : i32
      %1 = llvm.mlir.constant(4 : i32) : i32
      %2 = llvm.mlir.constant(96 : i32) : i32
      %3 = llvm.mlir.constant(0 : i32) : i32
      %4 = llvm.mlir.constant(16 : i32) : i32
      %5 = llvm.mlir.constant(dense<0> : vector<1xi32>) : vector<1xi32>
      %6 = llvm.mlir.constant(dense<0xFC00> : vector<1xf16>) : vector<1xf16>
      %7 = llvm.mlir.constant(128 : index) : i64
      %8 = llvm.mlir.constant(12288 : index) : i64
      %9 = llvm.mlir.constant(0 : index) : i64
      %10 = rocdl.workitem.id.x range <i32, 0, 64> : i32
      %11 = llvm.sext %10 : i32 to i64
      %12 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
      %13 = llvm.insertvalue %arg0, %12[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %14 = llvm.insertvalue %arg0, %13[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %15 = llvm.mlir.constant(0 : index) : i64
      %16 = llvm.insertvalue %15, %14[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %17 = llvm.mlir.constant(4 : index) : i64
      %18 = llvm.insertvalue %17, %16[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %19 = llvm.mlir.constant(1536 : index) : i64
      %20 = llvm.insertvalue %19, %18[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %21 = llvm.mlir.constant(16 : index) : i64
      %22 = llvm.insertvalue %21, %20[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %23 = llvm.mlir.constant(96 : index) : i64
      %24 = llvm.insertvalue %23, %22[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %25 = llvm.mlir.constant(96 : index) : i64
      %26 = llvm.insertvalue %25, %24[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %27 = llvm.mlir.constant(1 : index) : i64
      %28 = llvm.insertvalue %27, %26[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %29 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %30 = llvm.mlir.constant(true) : i1
      %31 = llvm.mlir.constant(64 : index) : i64
      llvm.intr.assume %30 ["align"(%29, %31 : !llvm.ptr<1>, i64)] : i1
      %32 = llvm.mlir.constant(12288 : i64) : i64
      %33 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %34 = llvm.mlir.constant(0 : index) : i64
      %35 = llvm.extractvalue %28[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %36 = llvm.extractvalue %28[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %37 = llvm.mlir.constant(0 : i16) : i16
      %38 = llvm.mlir.constant(159744 : i32) : i32
      %39 = rocdl.make.buffer.rsrc %33, %37, %32, %38 : <1> to <7>
      %40 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
      %41 = llvm.insertvalue %39, %40[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %42 = llvm.insertvalue %39, %41[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %43 = llvm.insertvalue %34, %42[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %44 = llvm.insertvalue %35, %43[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %45 = llvm.insertvalue %36, %44[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %46 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
      %47 = llvm.insertvalue %arg0, %46[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %48 = llvm.insertvalue %arg0, %47[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %49 = llvm.mlir.constant(32 : i64) : i64
      %50 = llvm.mlir.constant(8 : i64) : i64
      %51 = llvm.mul %8, %50 : i64
      %52 = llvm.udiv %51, %49 : i64
      %53 = llvm.insertvalue %52, %48[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %54 = llvm.mlir.constant(4 : index) : i64
      %55 = llvm.insertvalue %54, %53[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %56 = llvm.mlir.constant(16 : index) : i64
      %57 = llvm.insertvalue %56, %55[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %58 = llvm.mlir.constant(96 : index) : i64
      %59 = llvm.insertvalue %58, %57[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %60 = llvm.mlir.constant(1 : index) : i64
      %61 = llvm.insertvalue %60, %59[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %62 = llvm.mlir.constant(96 : index) : i64
      %63 = llvm.insertvalue %62, %61[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %64 = llvm.mlir.constant(1536 : index) : i64
      %65 = llvm.insertvalue %64, %63[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %66 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %67 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %68 = llvm.getelementptr %66[%67] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
      %69 = llvm.mlir.constant(true) : i1
      %70 = llvm.mlir.constant(64 : index) : i64
      llvm.intr.assume %69 ["align"(%68, %70 : !llvm.ptr<1>, i64)] : i1
      %71 = llvm.mlir.constant(24576 : i64) : i64
      %72 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %73 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %74 = llvm.getelementptr %72[%73] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
      %75 = llvm.mlir.constant(0 : index) : i64
      %76 = llvm.extractvalue %65[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %77 = llvm.extractvalue %65[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
      %78 = llvm.mlir.constant(0 : i16) : i16
      %79 = llvm.mlir.constant(159744 : i32) : i32
      %80 = rocdl.make.buffer.rsrc %74, %78, %71, %79 : <1> to <7>
      %81 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
      %82 = llvm.insertvalue %80, %81[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %83 = llvm.insertvalue %80, %82[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %84 = llvm.insertvalue %75, %83[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %85 = llvm.insertvalue %76, %84[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %86 = llvm.insertvalue %77, %85[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %87 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
      %88 = llvm.insertvalue %arg1, %87[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %89 = llvm.insertvalue %arg1, %88[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %90 = llvm.mlir.constant(0 : index) : i64
      %91 = llvm.insertvalue %90, %89[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %92 = llvm.mlir.constant(4 : index) : i64
      %93 = llvm.insertvalue %92, %91[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %94 = llvm.mlir.constant(16 : index) : i64
      %95 = llvm.insertvalue %94, %93[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %96 = llvm.mlir.constant(16 : index) : i64
      %97 = llvm.insertvalue %96, %95[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %98 = llvm.mlir.constant(1 : index) : i64
      %99 = llvm.insertvalue %98, %97[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %100 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %101 = llvm.mlir.constant(true) : i1
      %102 = llvm.mlir.constant(64 : index) : i64
      llvm.intr.assume %101 ["align"(%100, %102 : !llvm.ptr<1>, i64)] : i1
      %103 = llvm.mlir.constant(128 : i64) : i64
      %104 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %105 = llvm.mlir.constant(0 : index) : i64
      %106 = llvm.extractvalue %99[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %107 = llvm.extractvalue %99[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %108 = llvm.mlir.constant(0 : i16) : i16
      %109 = llvm.mlir.constant(159744 : i32) : i32
      %110 = rocdl.make.buffer.rsrc %104, %108, %103, %109 : <1> to <7>
      %111 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
      %112 = llvm.insertvalue %110, %111[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %113 = llvm.insertvalue %110, %112[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %114 = llvm.insertvalue %105, %113[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %115 = llvm.insertvalue %106, %114[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %116 = llvm.insertvalue %107, %115[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %117 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
      %118 = llvm.insertvalue %arg2, %117[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %119 = llvm.insertvalue %arg2, %118[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %120 = llvm.mlir.constant(32 : i64) : i64
      %121 = llvm.mlir.constant(8 : i64) : i64
      %122 = llvm.mul %7, %121 : i64
      %123 = llvm.udiv %122, %120 : i64
      %124 = llvm.insertvalue %123, %119[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %125 = llvm.mlir.constant(4 : index) : i64
      %126 = llvm.insertvalue %125, %124[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %127 = llvm.mlir.constant(16 : index) : i64
      %128 = llvm.insertvalue %127, %126[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %129 = llvm.mlir.constant(1 : index) : i64
      %130 = llvm.insertvalue %129, %128[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %131 = llvm.mlir.constant(16 : index) : i64
      %132 = llvm.insertvalue %131, %130[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %133 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %134 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %135 = llvm.getelementptr %133[%134] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
      %136 = llvm.mlir.constant(true) : i1
      %137 = llvm.mlir.constant(64 : index) : i64
      llvm.intr.assume %136 ["align"(%135, %137 : !llvm.ptr<1>, i64)] : i1
      %138 = llvm.mlir.constant(256 : i64) : i64
      %139 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %141 = llvm.getelementptr %139[%140] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
      %142 = llvm.mlir.constant(0 : index) : i64
      %143 = llvm.extractvalue %132[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %144 = llvm.extractvalue %132[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
      %145 = llvm.mlir.constant(0 : i16) : i16
      %146 = llvm.mlir.constant(159744 : i32) : i32
      %147 = rocdl.make.buffer.rsrc %141, %145, %138, %146 : <1> to <7>
      %148 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
      %149 = llvm.insertvalue %147, %148[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %150 = llvm.insertvalue %147, %149[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %151 = llvm.insertvalue %142, %150[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %152 = llvm.insertvalue %143, %151[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %153 = llvm.insertvalue %144, %152[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %154 = llvm.trunc %11 : i64 to i32
      %155 = llvm.udiv %154, %4 : i32
      %156 = llvm.zext %155 : i32 to i64
      %157 = llvm.urem %154, %4 : i32
      %158 = llvm.zext %157 : i32 to i64
      %159 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %160 = llvm.mlir.constant(16 : index) : i64
      %161 = llvm.mul %156, %160 : i64
      %162 = llvm.add %161, %158 : i64
      %163 = llvm.getelementptr %159[%162] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
      llvm.store %6, %163 {alignment = 2 : i64} : vector<1xf16>, !llvm.ptr<7>
      llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      rocdl.s.barrier
      llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      %164 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %165 = llvm.mlir.constant(16 : index) : i64
      %166 = llvm.mul %156, %165 : i64
      %167 = llvm.add %166, %158 : i64
      %168 = llvm.getelementptr %164[%167] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
      llvm.store %5, %168 {alignment = 4 : i64} : vector<1xi32>, !llvm.ptr<7>
      llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      rocdl.s.barrier
      llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      llvm.br ^bb1(%3 : i32)
    ^bb1(%169: i32):  // 2 preds: ^bb0, ^bb5
      %170 = llvm.icmp "slt" %169, %2 : i32
      llvm.cond_br %170, ^bb2, ^bb6
    ^bb2:  // pred: ^bb1
      llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      rocdl.s.barrier
      llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
      llvm.br ^bb3(%3 : i32)
    ^bb3(%171: i32):  // 2 preds: ^bb2, ^bb4
      %172 = llvm.icmp "slt" %171, %1 : i32
      llvm.cond_br %172, ^bb4, ^bb5
    ^bb4:  // pred: ^bb3
      %173 = llvm.add %169, %171 : i32
      %174 = llvm.zext %173 : i32 to i64
      %175 = llvm.extractvalue %45[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %176 = llvm.mlir.constant(1536 : index) : i64
      %177 = llvm.mul %156, %176 overflow<nsw, nuw> : i64
      %178 = llvm.mlir.constant(96 : index) : i64
      %179 = llvm.mul %158, %178 overflow<nsw, nuw> : i64
      %180 = llvm.add %177, %179 overflow<nsw, nuw> : i64
      %181 = llvm.add %180, %174 overflow<nsw, nuw> : i64
      %182 = llvm.getelementptr inbounds|nuw %175[%181] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
      %183 = llvm.load %182 : !llvm.ptr<7> -> f16
      %184 = llvm.extractvalue %86[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
      %185 = llvm.mlir.constant(1536 : index) : i64
      %186 = llvm.mul %156, %185 overflow<nsw, nuw> : i64
      %187 = llvm.mlir.constant(96 : index) : i64
      %188 = llvm.mul %158, %187 overflow<nsw, nuw> : i64
      %189 = llvm.add %186, %188 overflow<nsw, nuw> : i64
      %190 = llvm.add %189, %174 overflow<nsw, nuw> : i64
      %191 = llvm.getelementptr inbounds|nuw %184[%190] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
      %192 = llvm.load %191 : !llvm.ptr<7> -> i32
      %193 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %194 = llvm.mlir.constant(16 : index) : i64
      %195 = llvm.mul %156, %194 overflow<nsw, nuw> : i64
      %196 = llvm.add %195, %158 overflow<nsw, nuw> : i64
      %197 = llvm.getelementptr inbounds|nuw %193[%196] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
      %198 = llvm.load %197 : !llvm.ptr<7> -> f16
      %199 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %200 = llvm.mlir.constant(16 : index) : i64
      %201 = llvm.mul %156, %200 overflow<nsw, nuw> : i64
      %202 = llvm.add %201, %158 overflow<nsw, nuw> : i64
      %203 = llvm.getelementptr inbounds|nuw %199[%202] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
      %204 = llvm.load %203 : !llvm.ptr<7> -> i32
      %205 = llvm.fcmp "ogt" %183, %198 : f16
      %206 = llvm.select %205, %183, %198 : i1, f16
      %207 = llvm.select %205, %192, %204 : i1, i32
      %208 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %209 = llvm.mlir.constant(16 : index) : i64
      %210 = llvm.mul %156, %209 overflow<nsw, nuw> : i64
      %211 = llvm.add %210, %158 overflow<nsw, nuw> : i64
      %212 = llvm.getelementptr inbounds|nuw %208[%211] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
      llvm.store %206, %212 : f16, !llvm.ptr<7>
      %213 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
      %214 = llvm.mlir.constant(16 : index) : i64
      %215 = llvm.mul %156, %214 overflow<nsw, nuw> : i64
      %216 = llvm.add %215, %158 overflow<nsw, nuw> : i64
      %217 = llvm.getelementptr inbounds|nuw %213[%216] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
      llvm.store %207, %217 : i32, !llvm.ptr<7>
      %218 = llvm.add %171, %0 : i32
      llvm.br ^bb3(%218 : i32)
    ^bb5:  // pred: ^bb3
      %219 = llvm.add %169, %1 : i32
      llvm.br ^bb1(%219 : i32)
    ^bb6:  // pred: ^bb1
      llvm.return
    }
  }
}

// -----// IR Dump After TranslateAllExecutablesPass (iree-hal-translate-all-executables) //----- //
hal.executable private @test_simple_argmax_4x16_dispatch_1 {
  hal.executable.variant public @rocm_hsaco_fb target(<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>) {
    hal.executable.export public @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32 ordinal(0) layout(#hal.pipeline.layout<bindings = [#hal.pipeline.binding<storage_buffer, "ReadOnly|Indirect">, #hal.pipeline.binding<storage_buffer, Indirect>, #hal.pipeline.binding<storage_buffer, Indirect>], flags = Indirect>) count(%arg0: !hal.device) -> (index, index, index) {
      %c1 = arith.constant 1 : index
      %c1_0 = arith.constant 1 : index
      %c1_1 = arith.constant 1 : index
      hal.return %c1, %c1_0, %c1_1 : index, index, index
    } attributes {subgroup_size = 64 : index, workgroup_size = [64 : index, 1 : index, 1 : index]}
    builtin.module {
      llvm.func @test_simple_argmax_4x16_dispatch_1_reduce_4x16x96_f16xi32xf16xi32(%arg0: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef, llvm.readonly}, %arg1: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}, %arg2: !llvm.ptr<1> {llvm.align = 16 : i32, llvm.inreg, llvm.noalias, llvm.nonnull, llvm.noundef}) attributes {rocdl.flat_work_group_size = "64,64", rocdl.kernel, rocdl.reqd_work_group_size = array<i32: 64, 1, 1>} {
        %0 = llvm.mlir.constant(1 : i32) : i32
        %1 = llvm.mlir.constant(4 : i32) : i32
        %2 = llvm.mlir.constant(96 : i32) : i32
        %3 = llvm.mlir.constant(0 : i32) : i32
        %4 = llvm.mlir.constant(16 : i32) : i32
        %5 = llvm.mlir.constant(dense<0> : vector<1xi32>) : vector<1xi32>
        %6 = llvm.mlir.constant(dense<0xFC00> : vector<1xf16>) : vector<1xf16>
        %7 = llvm.mlir.constant(128 : index) : i64
        %8 = llvm.mlir.constant(12288 : index) : i64
        %9 = llvm.mlir.constant(0 : index) : i64
        %10 = rocdl.workitem.id.x range <i32, 0, 64> : i32
        %11 = llvm.sext %10 : i32 to i64
        %12 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
        %13 = llvm.insertvalue %arg0, %12[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %14 = llvm.insertvalue %arg0, %13[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %15 = llvm.mlir.constant(0 : index) : i64
        %16 = llvm.insertvalue %15, %14[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %17 = llvm.mlir.constant(4 : index) : i64
        %18 = llvm.insertvalue %17, %16[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %19 = llvm.mlir.constant(1536 : index) : i64
        %20 = llvm.insertvalue %19, %18[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %21 = llvm.mlir.constant(16 : index) : i64
        %22 = llvm.insertvalue %21, %20[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %23 = llvm.mlir.constant(96 : index) : i64
        %24 = llvm.insertvalue %23, %22[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %25 = llvm.mlir.constant(96 : index) : i64
        %26 = llvm.insertvalue %25, %24[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %27 = llvm.mlir.constant(1 : index) : i64
        %28 = llvm.insertvalue %27, %26[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %29 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %30 = llvm.mlir.constant(true) : i1
        %31 = llvm.mlir.constant(64 : index) : i64
        llvm.intr.assume %30 ["align"(%29, %31 : !llvm.ptr<1>, i64)] : i1
        %32 = llvm.mlir.constant(12288 : i64) : i64
        %33 = llvm.extractvalue %28[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %34 = llvm.mlir.constant(0 : index) : i64
        %35 = llvm.extractvalue %28[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %36 = llvm.extractvalue %28[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %37 = llvm.mlir.constant(0 : i16) : i16
        %38 = llvm.mlir.constant(159744 : i32) : i32
        %39 = rocdl.make.buffer.rsrc %33, %37, %32, %38 : <1> to <7>
        %40 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
        %41 = llvm.insertvalue %39, %40[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %42 = llvm.insertvalue %39, %41[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %43 = llvm.insertvalue %34, %42[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %44 = llvm.insertvalue %35, %43[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %45 = llvm.insertvalue %36, %44[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %46 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)>
        %47 = llvm.insertvalue %arg0, %46[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %48 = llvm.insertvalue %arg0, %47[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %49 = llvm.mlir.constant(32 : i64) : i64
        %50 = llvm.mlir.constant(8 : i64) : i64
        %51 = llvm.mul %8, %50 : i64
        %52 = llvm.udiv %51, %49 : i64
        %53 = llvm.insertvalue %52, %48[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %54 = llvm.mlir.constant(4 : index) : i64
        %55 = llvm.insertvalue %54, %53[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %56 = llvm.mlir.constant(16 : index) : i64
        %57 = llvm.insertvalue %56, %55[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %58 = llvm.mlir.constant(96 : index) : i64
        %59 = llvm.insertvalue %58, %57[3, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %60 = llvm.mlir.constant(1 : index) : i64
        %61 = llvm.insertvalue %60, %59[4, 2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %62 = llvm.mlir.constant(96 : index) : i64
        %63 = llvm.insertvalue %62, %61[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %64 = llvm.mlir.constant(1536 : index) : i64
        %65 = llvm.insertvalue %64, %63[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %66 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %67 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %68 = llvm.getelementptr %66[%67] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
        %69 = llvm.mlir.constant(true) : i1
        %70 = llvm.mlir.constant(64 : index) : i64
        llvm.intr.assume %69 ["align"(%68, %70 : !llvm.ptr<1>, i64)] : i1
        %71 = llvm.mlir.constant(24576 : i64) : i64
        %72 = llvm.extractvalue %65[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %73 = llvm.extractvalue %65[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %74 = llvm.getelementptr %72[%73] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
        %75 = llvm.mlir.constant(0 : index) : i64
        %76 = llvm.extractvalue %65[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %77 = llvm.extractvalue %65[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<3 x i64>, array<3 x i64>)> 
        %78 = llvm.mlir.constant(0 : i16) : i16
        %79 = llvm.mlir.constant(159744 : i32) : i32
        %80 = rocdl.make.buffer.rsrc %74, %78, %71, %79 : <1> to <7>
        %81 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)>
        %82 = llvm.insertvalue %80, %81[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %83 = llvm.insertvalue %80, %82[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %84 = llvm.insertvalue %75, %83[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %85 = llvm.insertvalue %76, %84[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %86 = llvm.insertvalue %77, %85[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %87 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
        %88 = llvm.insertvalue %arg1, %87[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %89 = llvm.insertvalue %arg1, %88[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %90 = llvm.mlir.constant(0 : index) : i64
        %91 = llvm.insertvalue %90, %89[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %92 = llvm.mlir.constant(4 : index) : i64
        %93 = llvm.insertvalue %92, %91[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %94 = llvm.mlir.constant(16 : index) : i64
        %95 = llvm.insertvalue %94, %93[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %96 = llvm.mlir.constant(16 : index) : i64
        %97 = llvm.insertvalue %96, %95[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %98 = llvm.mlir.constant(1 : index) : i64
        %99 = llvm.insertvalue %98, %97[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %100 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %101 = llvm.mlir.constant(true) : i1
        %102 = llvm.mlir.constant(64 : index) : i64
        llvm.intr.assume %101 ["align"(%100, %102 : !llvm.ptr<1>, i64)] : i1
        %103 = llvm.mlir.constant(128 : i64) : i64
        %104 = llvm.extractvalue %99[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %105 = llvm.mlir.constant(0 : index) : i64
        %106 = llvm.extractvalue %99[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %107 = llvm.extractvalue %99[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %108 = llvm.mlir.constant(0 : i16) : i16
        %109 = llvm.mlir.constant(159744 : i32) : i32
        %110 = rocdl.make.buffer.rsrc %104, %108, %103, %109 : <1> to <7>
        %111 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
        %112 = llvm.insertvalue %110, %111[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %113 = llvm.insertvalue %110, %112[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %114 = llvm.insertvalue %105, %113[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %115 = llvm.insertvalue %106, %114[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %116 = llvm.insertvalue %107, %115[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %117 = llvm.mlir.poison : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)>
        %118 = llvm.insertvalue %arg2, %117[0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %119 = llvm.insertvalue %arg2, %118[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %120 = llvm.mlir.constant(32 : i64) : i64
        %121 = llvm.mlir.constant(8 : i64) : i64
        %122 = llvm.mul %7, %121 : i64
        %123 = llvm.udiv %122, %120 : i64
        %124 = llvm.insertvalue %123, %119[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %125 = llvm.mlir.constant(4 : index) : i64
        %126 = llvm.insertvalue %125, %124[3, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %127 = llvm.mlir.constant(16 : index) : i64
        %128 = llvm.insertvalue %127, %126[3, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %129 = llvm.mlir.constant(1 : index) : i64
        %130 = llvm.insertvalue %129, %128[4, 1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %131 = llvm.mlir.constant(16 : index) : i64
        %132 = llvm.insertvalue %131, %130[4, 0] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %133 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %134 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %135 = llvm.getelementptr %133[%134] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
        %136 = llvm.mlir.constant(true) : i1
        %137 = llvm.mlir.constant(64 : index) : i64
        llvm.intr.assume %136 ["align"(%135, %137 : !llvm.ptr<1>, i64)] : i1
        %138 = llvm.mlir.constant(256 : i64) : i64
        %139 = llvm.extractvalue %132[1] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %140 = llvm.extractvalue %132[2] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %141 = llvm.getelementptr %139[%140] : (!llvm.ptr<1>, i64) -> !llvm.ptr<1>, i32
        %142 = llvm.mlir.constant(0 : index) : i64
        %143 = llvm.extractvalue %132[3] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %144 = llvm.extractvalue %132[4] : !llvm.struct<(ptr<1>, ptr<1>, i64, array<2 x i64>, array<2 x i64>)> 
        %145 = llvm.mlir.constant(0 : i16) : i16
        %146 = llvm.mlir.constant(159744 : i32) : i32
        %147 = rocdl.make.buffer.rsrc %141, %145, %138, %146 : <1> to <7>
        %148 = llvm.mlir.poison : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)>
        %149 = llvm.insertvalue %147, %148[0] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %150 = llvm.insertvalue %147, %149[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %151 = llvm.insertvalue %142, %150[2] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %152 = llvm.insertvalue %143, %151[3] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %153 = llvm.insertvalue %144, %152[4] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %154 = llvm.trunc %11 : i64 to i32
        %155 = llvm.udiv %154, %4 : i32
        %156 = llvm.zext %155 : i32 to i64
        %157 = llvm.urem %154, %4 : i32
        %158 = llvm.zext %157 : i32 to i64
        %159 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %160 = llvm.mlir.constant(16 : index) : i64
        %161 = llvm.mul %156, %160 : i64
        %162 = llvm.add %161, %158 : i64
        %163 = llvm.getelementptr %159[%162] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
        llvm.store %6, %163 {alignment = 2 : i64} : vector<1xf16>, !llvm.ptr<7>
        llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        rocdl.s.barrier
        llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        %164 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %165 = llvm.mlir.constant(16 : index) : i64
        %166 = llvm.mul %156, %165 : i64
        %167 = llvm.add %166, %158 : i64
        %168 = llvm.getelementptr %164[%167] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
        llvm.store %5, %168 {alignment = 4 : i64} : vector<1xi32>, !llvm.ptr<7>
        llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        rocdl.s.barrier
        llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        llvm.br ^bb1(%3 : i32)
      ^bb1(%169: i32):  // 2 preds: ^bb0, ^bb5
        %170 = llvm.icmp "slt" %169, %2 : i32
        llvm.cond_br %170, ^bb2, ^bb6
      ^bb2:  // pred: ^bb1
        llvm.fence syncscope("workgroup") release {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        rocdl.s.barrier
        llvm.fence syncscope("workgroup") acquire {llvm.mmra = #llvm.mmra_tag<"amdgpu-synchronize-as":"local">}
        llvm.br ^bb3(%3 : i32)
      ^bb3(%171: i32):  // 2 preds: ^bb2, ^bb4
        %172 = llvm.icmp "slt" %171, %1 : i32
        llvm.cond_br %172, ^bb4, ^bb5
      ^bb4:  // pred: ^bb3
        %173 = llvm.add %169, %171 : i32
        %174 = llvm.zext %173 : i32 to i64
        %175 = llvm.extractvalue %45[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %176 = llvm.mlir.constant(1536 : index) : i64
        %177 = llvm.mul %156, %176 overflow<nsw, nuw> : i64
        %178 = llvm.mlir.constant(96 : index) : i64
        %179 = llvm.mul %158, %178 overflow<nsw, nuw> : i64
        %180 = llvm.add %177, %179 overflow<nsw, nuw> : i64
        %181 = llvm.add %180, %174 overflow<nsw, nuw> : i64
        %182 = llvm.getelementptr inbounds|nuw %175[%181] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
        %183 = llvm.load %182 : !llvm.ptr<7> -> f16
        %184 = llvm.extractvalue %86[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<3 x i64>, array<3 x i64>)> 
        %185 = llvm.mlir.constant(1536 : index) : i64
        %186 = llvm.mul %156, %185 overflow<nsw, nuw> : i64
        %187 = llvm.mlir.constant(96 : index) : i64
        %188 = llvm.mul %158, %187 overflow<nsw, nuw> : i64
        %189 = llvm.add %186, %188 overflow<nsw, nuw> : i64
        %190 = llvm.add %189, %174 overflow<nsw, nuw> : i64
        %191 = llvm.getelementptr inbounds|nuw %184[%190] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
        %192 = llvm.load %191 : !llvm.ptr<7> -> i32
        %193 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %194 = llvm.mlir.constant(16 : index) : i64
        %195 = llvm.mul %156, %194 overflow<nsw, nuw> : i64
        %196 = llvm.add %195, %158 overflow<nsw, nuw> : i64
        %197 = llvm.getelementptr inbounds|nuw %193[%196] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
        %198 = llvm.load %197 : !llvm.ptr<7> -> f16
        %199 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %200 = llvm.mlir.constant(16 : index) : i64
        %201 = llvm.mul %156, %200 overflow<nsw, nuw> : i64
        %202 = llvm.add %201, %158 overflow<nsw, nuw> : i64
        %203 = llvm.getelementptr inbounds|nuw %199[%202] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
        %204 = llvm.load %203 : !llvm.ptr<7> -> i32
        %205 = llvm.fcmp "ogt" %183, %198 : f16
        %206 = llvm.select %205, %183, %198 : i1, f16
        %207 = llvm.select %205, %192, %204 : i1, i32
        %208 = llvm.extractvalue %116[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %209 = llvm.mlir.constant(16 : index) : i64
        %210 = llvm.mul %156, %209 overflow<nsw, nuw> : i64
        %211 = llvm.add %210, %158 overflow<nsw, nuw> : i64
        %212 = llvm.getelementptr inbounds|nuw %208[%211] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, f16
        llvm.store %206, %212 : f16, !llvm.ptr<7>
        %213 = llvm.extractvalue %153[1] : !llvm.struct<(ptr<7>, ptr<7>, i64, array<2 x i64>, array<2 x i64>)> 
        %214 = llvm.mlir.constant(16 : index) : i64
        %215 = llvm.mul %156, %214 overflow<nsw, nuw> : i64
        %216 = llvm.add %215, %158 overflow<nsw, nuw> : i64
        %217 = llvm.getelementptr inbounds|nuw %213[%216] : (!llvm.ptr<7>, i64) -> !llvm.ptr<7>, i32
        llvm.store %207, %217 : i32, !llvm.ptr<7>
        %218 = llvm.add %171, %0 : i32
        llvm.br ^bb3(%218 : i32)
      ^bb5:  // pred: ^bb3
        %219 = llvm.add %169, %1 : i32
        llvm.br ^bb1(%219 : i32)
      ^bb6:  // pred: ^bb1
        llvm.return
      }
    }
  }
}

dispatch_4.mlir:16:26: error: 'iree_linalg_ext.arg_compare' op output shape must match input shape with reduction dimension removed. Expected: [1, 16], but got: [16, 1]
    %max_val, %max_idx = iree_linalg_ext.arg_compare dimension(2) 
                         ^
dispatch_4.mlir:2:3: note: called from
  func.func @test_simple_argmax_4x16(%input: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
  ^
dispatch_4.mlir:16:26: note: see current operation: 
%37:2 = "iree_linalg_ext.arg_compare"(%26, %31, %36, %24) <{dimension = 2 : i64, operandSegmentSizes = array<i32: 1, 2, 1>}> ({
^bb0(%arg7: f16, %arg8: f16):
  %40 = "arith.cmpf"(%arg7, %arg8) <{fastmath = #arith.fastmath<none>, predicate = 2 : i64}> : (f16, f16) -> i1
  "iree_linalg_ext.yield"(%40) : (i1) -> ()
}) {lowering_config = #iree_codegen.lowering_config<tile_sizes = [[1, 128]]>} : (tensor<1x16x1336xf16>, tensor<16x1xf16>, tensor<16x1xi32>, index) -> (tensor<16x1xf16>, tensor<16x1xi32>)
    %max_val, %max_idx = iree_linalg_ext.arg_compare dimension(2) 
                         ^
dispatch_4.mlir:16:26: error: failed to run translation of source executable to target executable for backend #hal.executable.target<"rocm", "rocm-hsaco-fb", {abi = "hip", iree.encoding.resolver = #iree_gpu.gpu_encoding_resolver<>, iree_codegen.default_tuning_spec = #rocm.builtin.tuning_module<"iree_default_tuning_spec_gfx942.mlir">, iree_codegen.target_info = #iree_gpu.target<arch = "gfx942", features = "", wgp = <compute =  fp64|fp32|fp16|int64|int32|int16|int8, storage =  b64|b32|b16|b8, subgroup =  shuffle|arithmetic, dot =  dp4xi8toi32, mma = [<MFMA_F32_16x16x16_BF16>, <MFMA_F32_32x32x8_BF16>, <MFMA_F32_16x16x32_F8E5M2FNUZ>, <MFMA_F32_16x16x32_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ>, <MFMA_F32_16x16x32_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ>, <MFMA_F32_32x32x16_F8E5M2FNUZ_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ>, <MFMA_F32_32x32x16_F8E4M3FNUZ_F8E5M2FNUZ>, <MFMA_I32_16x16x32_I8>, <MFMA_I32_32x32x16_I8>, <MFMA_F64_16x16x4_F64>, <MFMA_F32_16x16x4_F32>, <MFMA_F32_16x16x16_F16>, <MFMA_F32_32x32x8_F16>], subgroup_size_choices = [64], max_workgroup_sizes = [1024, 1024, 1024], max_thread_count_per_workgroup = 1024, max_workgroup_memory_bytes = 65536, max_workgroup_counts = [2147483647, 2147483647, 2147483647], max_load_instruction_bits = 128, simds_per_wgp = 4, vgpr_space_bits = 16384, dma_sizes = [32]>>, ukernels = "none"}>
    %max_val, %max_idx = iree_linalg_ext.arg_compare dimension(2) 
                         ^
dispatch_4.mlir:2:3: note: called from
  func.func @test_simple_argmax_4x16(%input: tensor<4x16x128256xf16>) -> (tensor<4x16xf16>, tensor<4x16xi32>) {
  ^
